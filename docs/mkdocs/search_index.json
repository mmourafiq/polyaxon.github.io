{
    "docs": [
        {
            "location": "/", 
            "text": "Polyaxon\n\n\nDeep Learning library for TensorFlow for building end to end models and experiments.\n\n\nDesign Goals\n\n\nPolyaxon was built with the following goals:\n\n\n\n\n\n\nModularity: The creation of a computation graph based on modular and understandable modules,\n    with the possibility to reuse and share the module in subsequent usage.\n\n\n\n\n\n\nUsability: Training a model should be easy enough, and should enable quick experimentations.\n\n\n\n\n\n\nConfigurable: Models and experiments could be created using a YAML/Json file, but also in python files.\n\n\n\n\n\n\nExtensibility: The modularity and the extensive documentation of the code makes it easy to build and extend the set of provided modules.\n\n\n\n\n\n\nPerformance: Polyaxon is based on internal \ntensorflow\n code base and leverage the builtin distributed learning.\n\n\n\n\n\n\nData Preprocessing: Polyaxon provides many pipelines and data processor to support different data inputs.\n\n\n\n\n\n\nQuick start\n\n\nA classification problem\n\n\n    \nX_train\n,\n \nY_train\n,\n \nX_test\n,\n \nY_test\n \n=\n \nload_mnist\n()\n\n\n    \nconfig\n \n=\n \n{\n\n        \nname\n:\n \nlenet_mnsit\n,\n\n        \noutput_dir\n:\n \noutput_dir\n,\n\n        \neval_every_n_steps\n:\n \n10\n,\n\n        \ntrain_steps_per_iteration\n:\n \n100\n,\n\n        \nrun_config\n:\n \n{\nsave_checkpoints_steps\n:\n \n100\n},\n\n        \ntrain_input_data_config\n:\n \n{\n\n            \ninput_type\n:\n \nplx\n.\nconfigs\n.\nInputDataConfig\n.\nNUMPY\n,\n\n            \npipeline_config\n:\n \n{\nname\n:\n \ntrain\n,\n \nbatch_size\n:\n \n64\n,\n \nnum_epochs\n:\n \nNone\n,\n\n                                \nshuffle\n:\n \nTrue\n},\n\n            \nx\n:\n \nX_train\n,\n\n            \ny\n:\n \nY_train\n\n        \n},\n\n        \neval_input_data_config\n:\n \n{\n\n            \ninput_type\n:\n \nplx\n.\nconfigs\n.\nInputDataConfig\n.\nNUMPY\n,\n\n            \npipeline_config\n:\n \n{\nname\n:\n \neval\n,\n \nbatch_size\n:\n \n32\n,\n \nnum_epochs\n:\n \nNone\n,\n\n                                \nshuffle\n:\n \nFalse\n},\n\n            \nx\n:\n \nX_test\n,\n\n            \ny\n:\n \nY_test\n\n        \n},\n\n        \nestimator_config\n:\n \n{\noutput_dir\n:\n \noutput_dir\n},\n\n        \nmodel_config\n:\n \n{\n\n            \nsummaries\n:\n \nall\n,\n\n            \nmodel_type\n:\n \nclassifier\n,\n\n            \nloss_config\n:\n \n{\nname\n:\n \nsoftmax_cross_entropy\n},\n\n            \neval_metrics_config\n:\n \n[{\nname\n:\n \nstreaming_accuracy\n},\n\n                                    \n{\nname\n:\n \nstreaming_precision\n}],\n\n            \noptimizer_config\n:\n \n{\nname\n:\n \nAdam\n,\n \nlearning_rate\n:\n \n0.002\n,\n\n                                 \ndecay_type\n:\n \nexponential_decay\n,\n \ndecay_rate\n:\n \n0.2\n},\n\n            \ngraph_config\n:\n \n{\n\n                \nname\n:\n \nlenet\n,\n\n                \ndefinition\n:\n \n[\n\n                    \n(\nplx\n.\nlayers\n.\nConv2d\n,\n \n{\nnum_filter\n:\n \n32\n,\n \nfilter_size\n:\n \n5\n,\n \nstrides\n:\n \n1\n,\n\n                                         \nregularizer\n:\n \nl2_regularizer\n}),\n\n                    \n(\nplx\n.\nlayers\n.\nMaxPool2d\n,\n \n{\nkernel_size\n:\n \n2\n}),\n\n                    \n(\nplx\n.\nlayers\n.\nConv2d\n,\n \n{\nnum_filter\n:\n \n64\n,\n \nfilter_size\n:\n \n5\n,\n\n                                         \nregularizer\n:\n \nl2_regularizer\n}),\n\n                    \n(\nplx\n.\nlayers\n.\nMaxPool2d\n,\n \n{\nkernel_size\n:\n \n2\n}),\n\n                    \n(\nplx\n.\nlayers\n.\nFullyConnected\n,\n \n{\nn_units\n:\n \n1024\n,\n \nactivation\n:\n \ntanh\n}),\n\n                    \n(\nplx\n.\nlayers\n.\nFullyConnected\n,\n \n{\nn_units\n:\n \n10\n}),\n\n                \n]\n\n            \n}\n\n        \n}\n\n    \n}\n\n    \nexperiment_config\n \n=\n \nplx\n.\nconfigs\n.\nExperimentConfig\n.\nread_configs\n(\nconfig\n)\n\n    \nxp\n \n=\n \nplx\n.\nexperiments\n.\ncreate_experiment\n(\nexperiment_config\n)\n\n    \nxp\n.\ncontinuous_train_and_evaluate\n()\n\n\n\n\n\n\nA regression problem\n\n\n    \nX\n,\n \ny\n \n=\n \ngenerate_data\n(\nnp\n.\nsin\n,\n \nnp\n.\nlinspace\n(\n0\n,\n \n100\n,\n \n10000\n,\n \ndtype\n=\nnp\n.\nfloat32\n),\n \ntime_steps\n=\n7\n)\n\n\n    \nconfig\n \n=\n \n{\n\n        \nname\n:\n \ntime_series\n,\n\n        \noutput_dir\n:\n \noutput_dir\n,\n\n        \neval_every_n_steps\n:\n \n5\n,\n\n        \nrun_config\n:\n \n{\nsave_checkpoints_steps\n:\n \n100\n},\n\n        \ntrain_input_data_config\n:\n \n{\n\n            \ninput_type\n:\n \nplx\n.\nconfigs\n.\nInputDataConfig\n.\nNUMPY\n,\n\n            \npipeline_config\n:\n \n{\nname\n:\n \ntrain\n,\n \nbatch_size\n:\n \n64\n,\n \nnum_epochs\n:\n \nNone\n,\n\n                                \nshuffle\n:\n \nFalse\n},\n\n            \nx\n:\n \nX\n[\ntrain\n],\n\n            \ny\n:\n \ny\n[\ntrain\n]\n\n        \n},\n\n        \neval_input_data_config\n:\n \n{\n\n            \ninput_type\n:\n \nplx\n.\nconfigs\n.\nInputDataConfig\n.\nNUMPY\n,\n\n            \npipeline_config\n:\n \n{\nname\n:\n \neval\n,\n \nbatch_size\n:\n \n32\n,\n \nnum_epochs\n:\n \nNone\n,\n\n                                \nshuffle\n:\n \nFalse\n},\n\n            \nx\n:\n \nX\n[\nval\n],\n\n            \ny\n:\n \ny\n[\nval\n]\n\n        \n},\n\n        \nestimator_config\n:\n \n{\noutput_dir\n:\n \noutput_dir\n},\n\n        \nmodel_config\n:\n \n{\n\n            \nmodel_type\n:\n \nregressor\n,\n\n            \nloss_config\n:\n \n{\nname\n:\n \nmean_squared_error\n},\n\n            \neval_metrics_config\n:\n \n[{\nname\n:\n \nstreaming_root_mean_squared_error\n},\n\n                                    \n{\nname\n:\n \nstreaming_mean_absolute_error\n}],\n\n            \noptimizer_config\n:\n \n{\nname\n:\n \nAdagrad\n,\n \nlearning_rate\n:\n \n0.1\n},\n\n            \ngraph_config\n:\n \n{\n\n                \nname\n:\n \nregressor\n,\n\n                \ndefinition\n:\n \n[\n\n                    \n(\nplx\n.\nlayers\n.\nLSTM\n,\n \n{\nnum_units\n:\n \n7\n,\n \nnum_layers\n:\n \n1\n}),\n\n                    \n# (Slice, {\nbegin\n: [0, 6], \nsize\n: [-1, 1]}),\n\n                    \n(\nplx\n.\nlayers\n.\nFullyConnected\n,\n \n{\nn_units\n:\n \n1\n}),\n\n                \n]\n\n            \n}\n\n        \n}\n\n    \n}\n\n    \nexperiment_config\n \n=\n \nplx\n.\nconfigs\n.\nExperimentConfig\n.\nread_configs\n(\nconfig\n)\n\n    \nxp\n \n=\n \nplx\n.\nexperiments\n.\ncreate_experiment\n(\nexperiment_config\n)\n\n    \nxp\n.\ncontinuous_train_and_evaluate\n()\n\n\n\n\n\n\nInstallation\n\n\nTo install the latest version of Polyaxon: \npip install polyaxon\n\n\nAlternatively, you can also install from source by running (from source folder): \npython setup.py install\n\n\nExamples\n\n\nSome example are provided \nhere\n, more examples and use case will pushed, a contribution with an example is also appreciated.\n\n\nProject status\n\n\nPolyaxon is in a pre-release \"alpha\" state. All interfaces, programming interfaces, and data structures may be changed without prior notice. \nWe'll do our best to communicate potentially disruptive changes.\n\n\nContributions\n\n\nPlease follow the contribution guide line: \nContribute to Polyaxon\n.\n\n\nLicense\n\n\nMIT License\n\n\nCredit\n\n\nThis work is based and was inspired from different projects, \ntensorflow.contrib.learn\n, \nkeras\n, \nsonnet\n, and \nseq2seq\n.\nThe idea behind creating this library is to provide a tool that allow engineers and researchers to develop and experiment with end to end solution.\nThis would allow us to have a complete control over the api and future design decision.", 
            "title": "Home"
        }, 
        {
            "location": "/#polyaxon", 
            "text": "Deep Learning library for TensorFlow for building end to end models and experiments.", 
            "title": "Polyaxon"
        }, 
        {
            "location": "/#design-goals", 
            "text": "Polyaxon was built with the following goals:    Modularity: The creation of a computation graph based on modular and understandable modules,\n    with the possibility to reuse and share the module in subsequent usage.    Usability: Training a model should be easy enough, and should enable quick experimentations.    Configurable: Models and experiments could be created using a YAML/Json file, but also in python files.    Extensibility: The modularity and the extensive documentation of the code makes it easy to build and extend the set of provided modules.    Performance: Polyaxon is based on internal  tensorflow  code base and leverage the builtin distributed learning.    Data Preprocessing: Polyaxon provides many pipelines and data processor to support different data inputs.", 
            "title": "Design Goals"
        }, 
        {
            "location": "/#quick-start", 
            "text": "", 
            "title": "Quick start"
        }, 
        {
            "location": "/#a-classification-problem", 
            "text": "X_train ,   Y_train ,   X_test ,   Y_test   =   load_mnist () \n\n     config   =   { \n         name :   lenet_mnsit , \n         output_dir :   output_dir , \n         eval_every_n_steps :   10 , \n         train_steps_per_iteration :   100 , \n         run_config :   { save_checkpoints_steps :   100 }, \n         train_input_data_config :   { \n             input_type :   plx . configs . InputDataConfig . NUMPY , \n             pipeline_config :   { name :   train ,   batch_size :   64 ,   num_epochs :   None , \n                                 shuffle :   True }, \n             x :   X_train , \n             y :   Y_train \n         }, \n         eval_input_data_config :   { \n             input_type :   plx . configs . InputDataConfig . NUMPY , \n             pipeline_config :   { name :   eval ,   batch_size :   32 ,   num_epochs :   None , \n                                 shuffle :   False }, \n             x :   X_test , \n             y :   Y_test \n         }, \n         estimator_config :   { output_dir :   output_dir }, \n         model_config :   { \n             summaries :   all , \n             model_type :   classifier , \n             loss_config :   { name :   softmax_cross_entropy }, \n             eval_metrics_config :   [{ name :   streaming_accuracy }, \n                                     { name :   streaming_precision }], \n             optimizer_config :   { name :   Adam ,   learning_rate :   0.002 , \n                                  decay_type :   exponential_decay ,   decay_rate :   0.2 }, \n             graph_config :   { \n                 name :   lenet , \n                 definition :   [ \n                     ( plx . layers . Conv2d ,   { num_filter :   32 ,   filter_size :   5 ,   strides :   1 , \n                                          regularizer :   l2_regularizer }), \n                     ( plx . layers . MaxPool2d ,   { kernel_size :   2 }), \n                     ( plx . layers . Conv2d ,   { num_filter :   64 ,   filter_size :   5 , \n                                          regularizer :   l2_regularizer }), \n                     ( plx . layers . MaxPool2d ,   { kernel_size :   2 }), \n                     ( plx . layers . FullyConnected ,   { n_units :   1024 ,   activation :   tanh }), \n                     ( plx . layers . FullyConnected ,   { n_units :   10 }), \n                 ] \n             } \n         } \n     } \n     experiment_config   =   plx . configs . ExperimentConfig . read_configs ( config ) \n     xp   =   plx . experiments . create_experiment ( experiment_config ) \n     xp . continuous_train_and_evaluate ()", 
            "title": "A classification problem"
        }, 
        {
            "location": "/#a-regression-problem", 
            "text": "X ,   y   =   generate_data ( np . sin ,   np . linspace ( 0 ,   100 ,   10000 ,   dtype = np . float32 ),   time_steps = 7 ) \n\n     config   =   { \n         name :   time_series , \n         output_dir :   output_dir , \n         eval_every_n_steps :   5 , \n         run_config :   { save_checkpoints_steps :   100 }, \n         train_input_data_config :   { \n             input_type :   plx . configs . InputDataConfig . NUMPY , \n             pipeline_config :   { name :   train ,   batch_size :   64 ,   num_epochs :   None , \n                                 shuffle :   False }, \n             x :   X [ train ], \n             y :   y [ train ] \n         }, \n         eval_input_data_config :   { \n             input_type :   plx . configs . InputDataConfig . NUMPY , \n             pipeline_config :   { name :   eval ,   batch_size :   32 ,   num_epochs :   None , \n                                 shuffle :   False }, \n             x :   X [ val ], \n             y :   y [ val ] \n         }, \n         estimator_config :   { output_dir :   output_dir }, \n         model_config :   { \n             model_type :   regressor , \n             loss_config :   { name :   mean_squared_error }, \n             eval_metrics_config :   [{ name :   streaming_root_mean_squared_error }, \n                                     { name :   streaming_mean_absolute_error }], \n             optimizer_config :   { name :   Adagrad ,   learning_rate :   0.1 }, \n             graph_config :   { \n                 name :   regressor , \n                 definition :   [ \n                     ( plx . layers . LSTM ,   { num_units :   7 ,   num_layers :   1 }), \n                     # (Slice, { begin : [0, 6],  size : [-1, 1]}), \n                     ( plx . layers . FullyConnected ,   { n_units :   1 }), \n                 ] \n             } \n         } \n     } \n     experiment_config   =   plx . configs . ExperimentConfig . read_configs ( config ) \n     xp   =   plx . experiments . create_experiment ( experiment_config ) \n     xp . continuous_train_and_evaluate ()", 
            "title": "A regression problem"
        }, 
        {
            "location": "/#installation", 
            "text": "To install the latest version of Polyaxon:  pip install polyaxon  Alternatively, you can also install from source by running (from source folder):  python setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/#examples", 
            "text": "Some example are provided  here , more examples and use case will pushed, a contribution with an example is also appreciated.", 
            "title": "Examples"
        }, 
        {
            "location": "/#project-status", 
            "text": "Polyaxon is in a pre-release \"alpha\" state. All interfaces, programming interfaces, and data structures may be changed without prior notice. \nWe'll do our best to communicate potentially disruptive changes.", 
            "title": "Project status"
        }, 
        {
            "location": "/#contributions", 
            "text": "Please follow the contribution guide line:  Contribute to Polyaxon .", 
            "title": "Contributions"
        }, 
        {
            "location": "/#license", 
            "text": "MIT License", 
            "title": "License"
        }, 
        {
            "location": "/#credit", 
            "text": "This work is based and was inspired from different projects,  tensorflow.contrib.learn ,  keras ,  sonnet , and  seq2seq .\nThe idea behind creating this library is to provide a tool that allow engineers and researchers to develop and experiment with end to end solution.\nThis would allow us to have a complete control over the api and future design decision.", 
            "title": "Credit"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Polyaxon with an introductory example\n\n\nimport\n \npolyaxon\n \nas\n \nplx\n\n\n\ndef\n \ngraph_fn\n(\nmode\n,\n \ninputs\n):\n\n    \ninference\n \n=\n \nplx\n.\nlayers\n.\nFullyConnected\n(\nmode\n=\nmode\n,\n \nn_units\n=\n64\n,\n \nactivation\n=\ntanh\n)(\ninputs\n)\n\n    \nreturn\n \nplx\n.\nlayers\n.\nFullyConnected\n(\nmode\n=\nmode\n,\n \nn_units\n=\n10\n)(\ninference\n)\n\n\n\nresults1\n \n=\n \ngraph_fn\n(\nplx\n.\nModeKeys\n.\nTRAIN\n,\n \ndataset1\n)\n\n\nresults2\n \n=\n \ngraph_fn\n(\nplx\n.\nModeKeys\n.\nEVAL\n,\n \ndataset2\n)\n\n\n\n\n\n\nSame thing can be achieved using \nSubgraph\n\n\nimport\n \npolyaxon\n \nas\n \nplx\n\n\n\ngraph\n \n=\n \nplx\n.\nexperiments\n.\nSubgraph\n(\nmode\n=\nplx\n.\nModeKeys\n.\nTRAIN\n,\n \nname\n=\ngraph\n,\n\n    \nmethods\n=\n[\nplx\n.\nlayers\n.\nFullyConnected\n,\n \nplx\n.\nlayers\n.\nFullyConnected\n],\n\n    \nkwargs\n=\n[{\nn_units\n:\n \n64\n,\n \nactivation\n:\n \ntanh\n},\n \n{\nn_units\n:\n \n10\n}])\n\n\n\nresults1\n \n=\n \ngraph\n(\ndataset1\n)\n\n\nresults2\n \n=\n \ngraph\n(\ndataset2\n)\n\n\n\n\n\n\nThe difference between the first approach and second is that the second creates a scope for the subgraph and only builds and connects the layers.\n\n\nImportant concepts\n\n\nGraphModule\n\n\nPolyaxon make use of tensorflow \ntf.make_template\n to easily share variables, and all of Polyaxon module inherits from \nGraphModule\n.\nEach Polyaxon module is python object that represent a part of the computation graph.\n\n\nInput data\n\n\nReading data from a file, set of files, a directory or numpy/pandas objects should be easy and reproducible.\n\n\n# an example of NUMPY data input configuration\n\n\ntrain_data\n \n=\n \ntrain_input_data_config\n:\n \n{\n\n    \ninput_type\n:\n \nplx\n.\nconfigs\n.\nInputDataConfig\n.\nNUMPY\n,\n\n    \npipeline_config\n:\n \n{\nname\n:\n \ntrain\n,\n \nbatch_size\n:\n \n64\n,\n \nnum_epochs\n:\n \n5\n,\n\n                        \nshuffle\n:\n \nTrue\n},\n\n    \nx\n:\n \nX_train\n,\n\n    \ny\n:\n \nY_train\n\n\n}\n\n\n\n\n\n\nVisualization\n\n\nVisualizing the graph is fully customizable and can be defined by providing the level and types of visualization:\n\n\nThe visualization levels: \nactivations\n, \nloss\n, \ngradients\n, \nvariables\n, and \nlearning_rate\n.\n\n\nThe visualization types currently supported: \nscalar\n, \nhistogram\n, and \nimage\n.", 
            "title": "Getting started"
        }, 
        {
            "location": "/getting_started/#polyaxon-with-an-introductory-example", 
            "text": "import   polyaxon   as   plx  def   graph_fn ( mode ,   inputs ): \n     inference   =   plx . layers . FullyConnected ( mode = mode ,   n_units = 64 ,   activation = tanh )( inputs ) \n     return   plx . layers . FullyConnected ( mode = mode ,   n_units = 10 )( inference )  results1   =   graph_fn ( plx . ModeKeys . TRAIN ,   dataset1 )  results2   =   graph_fn ( plx . ModeKeys . EVAL ,   dataset2 )   Same thing can be achieved using  Subgraph  import   polyaxon   as   plx  graph   =   plx . experiments . Subgraph ( mode = plx . ModeKeys . TRAIN ,   name = graph , \n     methods = [ plx . layers . FullyConnected ,   plx . layers . FullyConnected ], \n     kwargs = [{ n_units :   64 ,   activation :   tanh },   { n_units :   10 }])  results1   =   graph ( dataset1 )  results2   =   graph ( dataset2 )   The difference between the first approach and second is that the second creates a scope for the subgraph and only builds and connects the layers.", 
            "title": "Polyaxon with an introductory example"
        }, 
        {
            "location": "/getting_started/#important-concepts", 
            "text": "", 
            "title": "Important concepts"
        }, 
        {
            "location": "/getting_started/#graphmodule", 
            "text": "Polyaxon make use of tensorflow  tf.make_template  to easily share variables, and all of Polyaxon module inherits from  GraphModule .\nEach Polyaxon module is python object that represent a part of the computation graph.", 
            "title": "GraphModule"
        }, 
        {
            "location": "/getting_started/#input-data", 
            "text": "Reading data from a file, set of files, a directory or numpy/pandas objects should be easy and reproducible.  # an example of NUMPY data input configuration  train_data   =   train_input_data_config :   { \n     input_type :   plx . configs . InputDataConfig . NUMPY , \n     pipeline_config :   { name :   train ,   batch_size :   64 ,   num_epochs :   5 , \n                         shuffle :   True }, \n     x :   X_train , \n     y :   Y_train  }", 
            "title": "Input data"
        }, 
        {
            "location": "/getting_started/#visualization", 
            "text": "Visualizing the graph is fully customizable and can be defined by providing the level and types of visualization:  The visualization levels:  activations ,  loss ,  gradients ,  variables , and  learning_rate .  The visualization types currently supported:  scalar ,  histogram , and  image .", 
            "title": "Visualization"
        }, 
        {
            "location": "/experiments/introduction/", 
            "text": "Introduction\n\n\nPolyaxon encapsulate all configurations needed to train and evaluate a model in \nExperiments\n.\n\n\nAn \nExperiment\n expects a training (and evaluation) data pipeline, a model, and knows how to invoke the training from the estimator.\n\n\nYou can train your estimator pragmatically by invoking the training the experiment directly, or by using the the utils functions provided.\n\n\nAn \nExperiement\n can be created directly by instantiating the objects needed or by creating a \nExperiemntConfig\n.\n\n\nExperiment instance\n\n\nimport\n \npolyaxon\n \nas\n \nplx\n\n\nfrom\n \ntensorflow.python.estimator.inputs.inputs\n \nimport\n \nnumpy_input_fn\n\n\n\ntrain_input_fn\n \n=\n \nnumpy_input_fn\n(\nX\n[\ntrain\n],\n \ny\n[\ntrain\n],\n \nshuffle\n=\nTrue\n)\n\n\neval_input_fn\n \n=\n \nnumpy_input_fn\n(\nx\n[\neval\n],\n \ny\n[\neval\n],\n \nshuffle\n=\nFalse\n)\n\n\nestimator\n \n=\n \nplx\n.\nexperiments\n.\nEstimator\n(\n...\n)\n\n\nxp\n \n=\n \nplx\n.\nexperiments\n.\nExperiment\n(\nestimator\n=\nestimator\n,\n \n                                \ntrain_input_fn\n=\ntrain_input_fn\n,\n\n                                \neval_input_fn\n=\neval_input_fn\n)\n\n\nxp\n.\ntrain\n()\n\n\n\n\n\n\nExperiment from config\n\n\nimport\n \npolyaxon\n \nas\n \nplx\n\n\noutput_dir\n \n=\n \n\n\nconfig\n \n=\n \n{\n\n    \nname\n:\n \nlenet_mnsit\n,\n\n    \noutput_dir\n:\n \noutput_dir\n,\n\n    \neval_every_n_steps\n:\n \n10\n,\n\n    \ntrain_steps_per_iteration\n:\n \n100\n,\n\n    \nrun_config\n:\n \n{\nsave_checkpoints_steps\n:\n \n100\n},\n\n    \ntrain_input_data_config\n:\n \n{\n\n        \ninput_type\n:\n \nplx\n.\nconfigs\n.\nInputDataConfig\n.\nNUMPY\n,\n\n        \npipeline_config\n:\n \n{\nname\n:\n \ntrain\n,\n \nbatch_size\n:\n \n64\n,\n \nnum_epochs\n:\n \nNone\n,\n\n                            \nshuffle\n:\n \nTrue\n},\n\n        \nx\n:\n \nX_train\n,\n\n        \ny\n:\n \nY_train\n\n    \n},\n\n    \neval_input_data_config\n:\n \n{\n\n        \ninput_type\n:\n \nplx\n.\nconfigs\n.\nInputDataConfig\n.\nNUMPY\n,\n\n        \npipeline_config\n:\n \n{\nname\n:\n \neval\n,\n \nbatch_size\n:\n \n32\n,\n \nnum_epochs\n:\n \nNone\n,\n\n                            \nshuffle\n:\n \nFalse\n},\n\n        \nx\n:\n \nX_test\n,\n\n        \ny\n:\n \nY_test\n\n    \n},\n\n    \nestimator_config\n:\n \n{\noutput_dir\n:\n \noutput_dir\n},\n\n    \nmodel_config\n:\n \n{\n\n        \nsummaries\n:\n \nall\n,\n\n        \nmodel_type\n:\n \nclassifier\n,\n\n        \nloss_config\n:\n \n{\nname\n:\n \nsoftmax_cross_entropy\n},\n\n        \neval_metrics_config\n:\n \n[{\nname\n:\n \nstreaming_accuracy\n},\n\n                                \n{\nname\n:\n \nstreaming_precision\n}],\n\n        \noptimizer_config\n:\n \n{\nname\n:\n \nAdam\n,\n \nlearning_rate\n:\n \n0.002\n,\n\n                             \ndecay_type\n:\n \nexponential_decay\n,\n \ndecay_rate\n:\n \n0.2\n},\n\n        \ngraph_config\n:\n \n{\n\n            \nname\n:\n \nlenet\n,\n\n            \ndefinition\n:\n \n[\n\n                \n(\nplx\n.\nlayers\n.\nConv2d\n,\n \n{\nnum_filter\n:\n \n32\n,\n \nfilter_size\n:\n \n5\n,\n \nstrides\n:\n \n1\n,\n\n                                     \nregularizer\n:\n \nl2_regularizer\n}),\n\n                \n(\nplx\n.\nlayers\n.\nMaxPool2d\n,\n \n{\nkernel_size\n:\n \n2\n}),\n\n                \n(\nplx\n.\nlayers\n.\nConv2d\n,\n \n{\nnum_filter\n:\n \n64\n,\n \nfilter_size\n:\n \n5\n,\n\n                                     \nregularizer\n:\n \nl2_regularizer\n}),\n\n                \n(\nplx\n.\nlayers\n.\nMaxPool2d\n,\n \n{\nkernel_size\n:\n \n2\n}),\n\n                \n(\nplx\n.\nlayers\n.\nFullyConnected\n,\n \n{\nn_units\n:\n \n1024\n,\n \nactivation\n:\n \ntanh\n}),\n\n                \n(\nplx\n.\nlayers\n.\nFullyConnected\n,\n \n{\nn_units\n:\n \n10\n}),\n\n            \n]\n\n        \n}\n\n    \n}\n\n\n}\n\n\nxp_config\n \n=\n \nplx\n.\nconfigs\n.\nExperimentConfig\n.\nread_configs\n(\nconfig\n)\n\n\nxp\n \n=\n \nplx\n.\nexperiments\n.\ncreate_experiment\n(\nxp_config\n)\n\n\n\n\n\n\nPolyaxon Estimator\n\n\nThe \nEstimator\n is where the object responsible for training, evaluating and extracting predictions from the graph model.\n\n\nAn \nEstiamtor\n expects a function that creates a model graph.  \n\n\nimport\n \npolyaxon\n \nas\n \nplx\n\n\n\ndef\n \ndummy_model_fn\n(\nfeatures\n,\n \nlabels\n,\n \nmode\n):\n\n    \npass\n\n\n\nestimator\n \n=\n \nplx\n.\nexperiments\n.\nEstimator\n(\nmodel_fn\n=\ndummy_model_fn\n)\n\n\n\n\n\n\nPolyaxon Model\n\n\nA \nModel\n is \nGraphModule\n subclass where we define the computation graph.\n\n\nA \nModel\n expects all information about how to construct the \nGraph\n, the \nLoss\n, the evaluation \nMetrics\n, the \nOptimizer\n and the summaries level.\n\n\nimport\n \npolyaxon\n \nas\n \nplx\n\n\n\ndef\n \ngraph_fn\n(\nmode\n,\n \ninputs\n):\n\n    \nx\n \n=\n \nplx\n.\nlayers\n.\nFullyConnected\n(\nmode\n,\n \nn_units\n=\n128\n)(\ninputs\n)\n\n    \nx\n \n=\n \nplx\n.\nlayers\n.\nFullyConnected\n(\nmode\n,\n \nn_units\n=\n64\n)(\ninputs\n)\n\n    \nreturn\n \nplx\n.\nlayers\n.\nFullyConnected\n(\nmode\n,\n \nn_units\n=\n8\n)(\ninputs\n)\n\n\n\ndef\n \nmodel_fn\n(\nfeatures\n,\n \nlabels\n,\n \nmode\n):\n\n    \nloss\n \n=\n \nplx\n.\nconfigs\n.\nLossConfig\n(\nname\n=\nmean_squared_error\n)\n\n    \noptimizer\n \n=\n \nplx\n.\nconfigs\n.\nOptimizerConfig\n(\nname\n=\nSGD\n)\n\n    \nmodel\n \n=\n \nplx\n.\nexperiments\n.\nRegressorModel\n(\n\n    \nmode\n=\nmode\n,\n \nname\n=\nmy_regressor\n,\n \ngraph_fn\n=\ngraph_fn\n,\n \n    \nloss_config\n=\nloss\n,\n \noptimizer_config\n=\noptimizer\n)\n\n\n    \nreturn\n \nmodel\n(\nfeatures\n=\nfeatures\n,\n \nlabels\n=\nlabels\n)", 
            "title": "Introduction"
        }, 
        {
            "location": "/experiments/introduction/#introduction", 
            "text": "Polyaxon encapsulate all configurations needed to train and evaluate a model in  Experiments .  An  Experiment  expects a training (and evaluation) data pipeline, a model, and knows how to invoke the training from the estimator.  You can train your estimator pragmatically by invoking the training the experiment directly, or by using the the utils functions provided.  An  Experiement  can be created directly by instantiating the objects needed or by creating a  ExperiemntConfig .", 
            "title": "Introduction"
        }, 
        {
            "location": "/experiments/introduction/#experiment-instance", 
            "text": "import   polyaxon   as   plx  from   tensorflow.python.estimator.inputs.inputs   import   numpy_input_fn  train_input_fn   =   numpy_input_fn ( X [ train ],   y [ train ],   shuffle = True )  eval_input_fn   =   numpy_input_fn ( x [ eval ],   y [ eval ],   shuffle = False )  estimator   =   plx . experiments . Estimator ( ... )  xp   =   plx . experiments . Experiment ( estimator = estimator ,  \n                                 train_input_fn = train_input_fn , \n                                 eval_input_fn = eval_input_fn )  xp . train ()", 
            "title": "Experiment instance"
        }, 
        {
            "location": "/experiments/introduction/#experiment-from-config", 
            "text": "import   polyaxon   as   plx  output_dir   =    config   =   { \n     name :   lenet_mnsit , \n     output_dir :   output_dir , \n     eval_every_n_steps :   10 , \n     train_steps_per_iteration :   100 , \n     run_config :   { save_checkpoints_steps :   100 }, \n     train_input_data_config :   { \n         input_type :   plx . configs . InputDataConfig . NUMPY , \n         pipeline_config :   { name :   train ,   batch_size :   64 ,   num_epochs :   None , \n                             shuffle :   True }, \n         x :   X_train , \n         y :   Y_train \n     }, \n     eval_input_data_config :   { \n         input_type :   plx . configs . InputDataConfig . NUMPY , \n         pipeline_config :   { name :   eval ,   batch_size :   32 ,   num_epochs :   None , \n                             shuffle :   False }, \n         x :   X_test , \n         y :   Y_test \n     }, \n     estimator_config :   { output_dir :   output_dir }, \n     model_config :   { \n         summaries :   all , \n         model_type :   classifier , \n         loss_config :   { name :   softmax_cross_entropy }, \n         eval_metrics_config :   [{ name :   streaming_accuracy }, \n                                 { name :   streaming_precision }], \n         optimizer_config :   { name :   Adam ,   learning_rate :   0.002 , \n                              decay_type :   exponential_decay ,   decay_rate :   0.2 }, \n         graph_config :   { \n             name :   lenet , \n             definition :   [ \n                 ( plx . layers . Conv2d ,   { num_filter :   32 ,   filter_size :   5 ,   strides :   1 , \n                                      regularizer :   l2_regularizer }), \n                 ( plx . layers . MaxPool2d ,   { kernel_size :   2 }), \n                 ( plx . layers . Conv2d ,   { num_filter :   64 ,   filter_size :   5 , \n                                      regularizer :   l2_regularizer }), \n                 ( plx . layers . MaxPool2d ,   { kernel_size :   2 }), \n                 ( plx . layers . FullyConnected ,   { n_units :   1024 ,   activation :   tanh }), \n                 ( plx . layers . FullyConnected ,   { n_units :   10 }), \n             ] \n         } \n     }  }  xp_config   =   plx . configs . ExperimentConfig . read_configs ( config )  xp   =   plx . experiments . create_experiment ( xp_config )", 
            "title": "Experiment from config"
        }, 
        {
            "location": "/experiments/introduction/#polyaxon-estimator", 
            "text": "The  Estimator  is where the object responsible for training, evaluating and extracting predictions from the graph model.  An  Estiamtor  expects a function that creates a model graph.    import   polyaxon   as   plx  def   dummy_model_fn ( features ,   labels ,   mode ): \n     pass  estimator   =   plx . experiments . Estimator ( model_fn = dummy_model_fn )", 
            "title": "Polyaxon Estimator"
        }, 
        {
            "location": "/experiments/introduction/#polyaxon-model", 
            "text": "A  Model  is  GraphModule  subclass where we define the computation graph.  A  Model  expects all information about how to construct the  Graph , the  Loss , the evaluation  Metrics , the  Optimizer  and the summaries level.  import   polyaxon   as   plx  def   graph_fn ( mode ,   inputs ): \n     x   =   plx . layers . FullyConnected ( mode ,   n_units = 128 )( inputs ) \n     x   =   plx . layers . FullyConnected ( mode ,   n_units = 64 )( inputs ) \n     return   plx . layers . FullyConnected ( mode ,   n_units = 8 )( inputs )  def   model_fn ( features ,   labels ,   mode ): \n     loss   =   plx . configs . LossConfig ( name = mean_squared_error ) \n     optimizer   =   plx . configs . OptimizerConfig ( name = SGD ) \n     model   =   plx . experiments . RegressorModel ( \n     mode = mode ,   name = my_regressor ,   graph_fn = graph_fn ,  \n     loss_config = loss ,   optimizer_config = optimizer ) \n\n     return   model ( features = features ,   labels = labels )", 
            "title": "Polyaxon Model"
        }, 
        {
            "location": "/experiments/experiment/", 
            "text": "[source]\n\n\nExperiment\n\n\npolyaxon\n.\nexperiments\n.\nexperiment\n.\nExperiment\n(\nestimator\n,\n \ntrain_input_fn\n,\n \neval_input_fn\n,\n \ntrain_steps\n=\nNone\n,\n \neval_steps\n=\n10\n,\n \ntrain_hooks\n=\nNone\n,\n \neval_hooks\n=\nNone\n,\n \neval_delay_secs\n=\n0\n,\n \ncontinuous_eval_throttle_secs\n=\n60\n,\n \neval_every_n_steps\n=\n1\n,\n \ndelay_workers_by_global_step\n=\nFalse\n,\n \nexport_strategies\n=\nNone\n,\n \ntrain_steps_per_iteration\n=\n100\n)\n\n\n\n\n\n\nExperiment is a class containing all information needed to train a model.\n\n\nAfter an experiment is created (by passing an Estimator and inputs for\ntraining and evaluation), an Experiment instance knows how to invoke training\nand eval loops in a sensible fashion for distributed training.\n\n\nNone of the functions passed to this constructor are executed at construction time.\nThey are stored and used when a method is executed which requires it.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nestimator\n: Object implementing Estimator interface.\n\n\ntrain_input_fn\n: function, returns features and labels for training.\n\n\neval_input_fn\n: function, returns features and labels for evaluation. If\n    \neval_steps\n is \nNone\n, this should be configured only to produce for a\n    finite number of batches (generally, 1 epoch over the evaluation data).\n\n\ntrain_steps\n: Perform this many steps of training.  default: None, means train forever.\n\n\neval_steps\n: \nevaluate\n runs until input is exhausted (or another exception is raised),\n    or for \neval_steps\n steps, if specified.\n\n\ntrain_hooks\n: A list of monitors to pass to the \nEstimator\n's \nfit\n function.\n\n\neval_hooks\n: A list of \nSessionRunHook\n hooks to pass to\n    the \nEstimator\n's \nevaluate\n function.\n\n\neval_delay_secs\n: Start evaluating after waiting for this many seconds.\n\n\ncontinuous_eval_throttle_secs\n: Do not re-evaluate unless the last evaluation\n    was started at least this many seconds ago for continuous_eval().\n\n\neval_every_n_steps\n: (applies only to train_and_evaluate).\n    the minimum number of steps between evaluations. Of course, evaluation does not\n    occur if no new snapshot is available, hence, this is the minimum.\n\n\ndelay_workers_by_global_step\n: if \nTrue\n delays training workers based on global step\n    instead of time.\n\n\nexport_strategies\n: A list of \nExportStrategy\ns, or a single one, or None.\n\n\ntrain_steps_per_iteration\n: (applies only to continuous_train_and_evaluate).\n    Perform this many (integer) number of train steps for each training-evaluation\n    iteration. With a small value, the model will be evaluated more frequently\n    with more checkpoints saved. If \nNone\n, will use a default value\n    (which is smaller than \ntrain_steps\n if provided).\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if \nestimator\n does not implement Estimator interface,\n        or if export_strategies has the wrong type.\n\n\n\n\n\n\n\n\n\n\nreset_export_strategies\n\n\nreset_export_strategies\n(\nself\n,\n \nnew_export_strategies\n=\nNone\n)\n\n\n\n\n\n\nResets the export strategies with the \nnew_export_strategies\n.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nnew_export_strategies\n: A new list of \nExportStrategy\ns, or a single one,\nor None.\n\n\n\n\n\n\n\n\nReturns\n:\n    The old export strategies.\n\n\n\n\n\n\n\n\nextend_eval_hooks\n\n\nextend_eval_hooks\n(\nself\n,\n \nadditional_hooks\n)\n\n\n\n\n\n\nExtends the hooks for training.\n\n\n\n\nextend_eval_hooks\n\n\nextend_eval_hooks\n(\nself\n,\n \nadditional_hooks\n)\n\n\n\n\n\n\nExtends the hooks for training.\n\n\n\n\ntrain\n\n\ntrain\n(\nself\n,\n \ndelay_secs\n=\nNone\n)\n\n\n\n\n\n\nFit the estimator using the training data.\n\n\nTrain the estimator for \nself._train_steps\n steps, after waiting for \ndelay_secs\n seconds.\nIf \nself._train_steps\n is \nNone\n, train forever.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ndelay_secs\n: Start training after this many seconds.\n\n\n\n\n\n\n\n\nReturns\n:\n    The trained estimator.\n\n\n\n\n\n\n\n\nevaluate\n\n\nevaluate\n(\nself\n,\n \ndelay_secs\n=\nNone\n)\n\n\n\n\n\n\nEvaluate on the evaluation data.\n\n\nRuns evaluation on the evaluation data and returns the result. Runs for\n\nself._eval_steps\n steps, or if it's \nNone\n, then run until input is\nexhausted or another exception is raised. Start the evaluation after\n\ndelay_secs\n seconds, or if it's \nNone\n, defaults to using\n\nself._eval_delay_secs\n seconds.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ndelay_secs\n: Start evaluating after this many seconds. If \nNone\n, defaults to using\n\nself._eval_delays_secs\n.\n\n\n\n\n\n\n\n\nReturns\n:\n    The result of the \nevaluate\n call to the \nEstimator\n.\n\n\n\n\n\n\n\n\ncontinuous_eval\n\n\ncontinuous_eval\n(\nself\n,\n \ndelay_secs\n=\nNone\n,\n \nthrottle_delay_secs\n=\nNone\n,\n \nevaluate_checkpoint_only_once\n=\nTrue\n,\n \ncontinuous_eval_predicate_fn\n=\nNone\n)\n\n\n\n\n\n\n\n\ncontinuous_eval_on_train_data\n\n\ncontinuous_eval_on_train_data\n(\nself\n,\n \ndelay_secs\n=\nNone\n,\n \nthrottle_delay_secs\n=\nNone\n,\n \ncontinuous_eval_predicate_fn\n=\nNone\n)\n\n\n\n\n\n\n\n\ntrain_and_evaluate\n\n\ntrain_and_evaluate\n(\nself\n)\n\n\n\n\n\n\nInterleaves training and evaluation.\n\n\nThe frequency of evaluation is controlled by the constructor arg \neval_every_n_steps\n.\nWhen this parameter is None or 0, evaluation happens only after training has completed.\nNote that evaluation cannot happen more frequently than checkpoints are taken.\nIf no new snapshots are available when evaluation is supposed to occur,\nthen evaluation doesn't happen for another \neval_every_n_steps\n steps\n(assuming a checkpoint is available at that point).\nThus, settings \neval_every_n_steps\n to 1 means that the model will be evaluated\neverytime there is a new checkpoint.\n\n\nThis is particular useful for a \"Master\" task in the cloud, whose responsibility\nit is to take checkpoints, evaluate those checkpoints, and write out summaries.\nParticipating in training as the supervisor allows such a task to accomplish\nthe first and last items, while performing evaluation allows for the second.\n\n\n\n\nReturns\n:\n    The result of the \nevaluate\n call to the \nEstimator\n as well as the\n    export results using the specified \nExportStrategy\n.\n\n\n\n\n\n\ncontinuous_train_and_evaluate\n\n\ncontinuous_train_and_evaluate\n(\nself\n,\n \ncontinuous_eval_predicate_fn\n=\nNone\n)\n\n\n\n\n\n\nInterleaves training and evaluation.\n\n\nThe frequency of evaluation is controlled by the \ntrain_steps_per_iteration\n\n(via constructor). The model will be first trained for\n\ntrain_steps_per_iteration\n, and then be evaluated in turns.\n\n\nThis differs from \ntrain_and_evaluate\n as follows:\n    1. The procedure will have train and evaluation in turns. The model\n    will be trained for a number of steps (usuallly smaller than \ntrain_steps\n\n    if provided) and then be evaluated.  \ntrain_and_evaluate\n will train the\n    model for \ntrain_steps\n (no small training iteraions).\n\n\n2. Due to the different approach this schedule takes, it leads to two\ndifferences in resource control. First, the resources (e.g., memory) used\nby training will be released before evaluation (`train_and_evaluate` takes\ndouble resources). Second, more checkpoints will be saved as a checkpoint\nis generated at the end of each small trainning iteration.\n\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ncontinuous_eval_predicate_fn\n: A predicate function determining whether to\ncontinue after each iteration. \npredicate_fn\n takes the evaluation\nresults as its arguments. At the beginning of evaluation, the passed\neval results will be None so it's expected that the predicate function\nhandles that gracefully. When \npredicate_fn\n is not specified, this will\nrun in an infinite loop or exit when global_step reaches \ntrain_steps\n.\n\n\n\n\n\n\n\n\nReturns\n:\n   A tuple of the result of the \nevaluate\n call to the \nEstimator\n and the\n   export results using the specified \nExportStrategy\n.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if \ncontinuous_eval_predicate_fn\n is neither None norcallable.\n\n\n\n\n\n\n\n\n\n\nrun_std_server\n\n\nrun_std_server\n(\nself\n)\n\n\n\n\n\n\nStarts a TensorFlow server and joins the serving thread.\n\n\nTypically used for parameter servers.\n\n\n\n\nRaises\n:\n\n\nValueError\n: if not enough information is available in the estimator's\nconfig to create a server.\n\n\n\n\n\n\n\n\n\n\ntest\n\n\ntest\n(\nself\n)\n\n\n\n\n\n\nTests training, evaluating and exporting the estimator for a single step.\n\n\n\n\nReturns\n:\n    The result of the \nevaluate\n call to the \nEstimator\n.\n\n\n\n\n\n\ncreate_experiment\n\n\ncreate_experiment\n(\nexperiment_config\n)\n\n\n\n\n\n\nCreates a new \nExperiment\n instance.\n\n\n\n\nArgs\n:\n\n\nexperiment_config\n: the config to use for creating the experiment.\n\n\n\n\n\n\n\n\n\n\nrun_experiment\n\n\nrun_experiment\n(\nexperiment_fn\n,\n \noutput_dir\n,\n \nschedule\n=\nNone\n)\n\n\n\n\n\n\nMake and run an experiment.\n\n\nIt creates an Experiment by calling \nexperiment_fn\n. Then it calls the\nfunction named as \nschedule\n of the Experiment.\n\n\nIf schedule is not provided, then the default schedule for the current task\ntype is used. The defaults are as follows:\n\n\n\n\n'ps' maps to 'serve'\n\n\n'worker' maps to 'train'\n\n\n'master' maps to 'local_run'\n\n\n\n\nIf the experiment's config does not include a task type, then an exception\nis raised.\n\n\n\n\nExample\n:\n\n\n\n\n \ndef\n \n_create_my_experiment\n(\noutput_dir\n):\n\n\n \nreturn\n \ntf\n.\ncontrib\n.\nlearn\n.\nExperiment\n(\n\n\n  \nestimator\n=\nmy_estimator\n(\nmodel_dir\n=\noutput_dir\n),\n\n\n  \ntrain_input_fn\n=\nmy_train_input\n,\n\n\n  \neval_input_fn\n=\nmy_eval_input\n)\n\n\n\n \nrun\n(\nexperiment_fn\n=\n_create_my_experiment\n,\n\n\n \noutput_dir\n=\nsome/output/dir\n,\n\n\n \nschedule\n=\ntrain\n)\n\n\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nexperiment_fn\n: A function that creates an \nExperiment\n. It should accept an\n  argument \noutput_dir\n which should be used to create the \nEstimator\n\n  (passed as \nmodel_dir\n to its constructor). It must return an\n  \nExperiment\n.\n\n\noutput_dir\n: Base output directory.\n\n\nschedule\n: The name of the  method in the \nExperiment\n to run.\n\n\n\n\n\n\n\n\nReturns\n:\n    The return value of function \nschedule\n.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \noutput_dir\n is empty, \nschedule\n is None but no task\n  type is set in the built experiment's config, the task type has no\n  default, or \nschedule\n doesn't reference a member of \nExperiment\n.\n\n\nTypeError\n: \nschedule\n references non-callable member.", 
            "title": "Expermient"
        }, 
        {
            "location": "/experiments/experiment/#experiment", 
            "text": "polyaxon . experiments . experiment . Experiment ( estimator ,   train_input_fn ,   eval_input_fn ,   train_steps = None ,   eval_steps = 10 ,   train_hooks = None ,   eval_hooks = None ,   eval_delay_secs = 0 ,   continuous_eval_throttle_secs = 60 ,   eval_every_n_steps = 1 ,   delay_workers_by_global_step = False ,   export_strategies = None ,   train_steps_per_iteration = 100 )   Experiment is a class containing all information needed to train a model.  After an experiment is created (by passing an Estimator and inputs for\ntraining and evaluation), an Experiment instance knows how to invoke training\nand eval loops in a sensible fashion for distributed training.  None of the functions passed to this constructor are executed at construction time.\nThey are stored and used when a method is executed which requires it.    Args :   estimator : Object implementing Estimator interface.  train_input_fn : function, returns features and labels for training.  eval_input_fn : function, returns features and labels for evaluation. If\n     eval_steps  is  None , this should be configured only to produce for a\n    finite number of batches (generally, 1 epoch over the evaluation data).  train_steps : Perform this many steps of training.  default: None, means train forever.  eval_steps :  evaluate  runs until input is exhausted (or another exception is raised),\n    or for  eval_steps  steps, if specified.  train_hooks : A list of monitors to pass to the  Estimator 's  fit  function.  eval_hooks : A list of  SessionRunHook  hooks to pass to\n    the  Estimator 's  evaluate  function.  eval_delay_secs : Start evaluating after waiting for this many seconds.  continuous_eval_throttle_secs : Do not re-evaluate unless the last evaluation\n    was started at least this many seconds ago for continuous_eval().  eval_every_n_steps : (applies only to train_and_evaluate).\n    the minimum number of steps between evaluations. Of course, evaluation does not\n    occur if no new snapshot is available, hence, this is the minimum.  delay_workers_by_global_step : if  True  delays training workers based on global step\n    instead of time.  export_strategies : A list of  ExportStrategy s, or a single one, or None.  train_steps_per_iteration : (applies only to continuous_train_and_evaluate).\n    Perform this many (integer) number of train steps for each training-evaluation\n    iteration. With a small value, the model will be evaluated more frequently\n    with more checkpoints saved. If  None , will use a default value\n    (which is smaller than  train_steps  if provided).     Raises :   ValueError : if  estimator  does not implement Estimator interface,\n        or if export_strategies has the wrong type.", 
            "title": "Experiment"
        }, 
        {
            "location": "/experiments/experiment/#reset_export_strategies", 
            "text": "reset_export_strategies ( self ,   new_export_strategies = None )   Resets the export strategies with the  new_export_strategies .    Args :   new_export_strategies : A new list of  ExportStrategy s, or a single one,\nor None.     Returns :\n    The old export strategies.", 
            "title": "reset_export_strategies"
        }, 
        {
            "location": "/experiments/experiment/#extend_eval_hooks", 
            "text": "extend_eval_hooks ( self ,   additional_hooks )   Extends the hooks for training.", 
            "title": "extend_eval_hooks"
        }, 
        {
            "location": "/experiments/experiment/#extend_eval_hooks_1", 
            "text": "extend_eval_hooks ( self ,   additional_hooks )   Extends the hooks for training.", 
            "title": "extend_eval_hooks"
        }, 
        {
            "location": "/experiments/experiment/#train", 
            "text": "train ( self ,   delay_secs = None )   Fit the estimator using the training data.  Train the estimator for  self._train_steps  steps, after waiting for  delay_secs  seconds.\nIf  self._train_steps  is  None , train forever.    Args :   delay_secs : Start training after this many seconds.     Returns :\n    The trained estimator.", 
            "title": "train"
        }, 
        {
            "location": "/experiments/experiment/#evaluate", 
            "text": "evaluate ( self ,   delay_secs = None )   Evaluate on the evaluation data.  Runs evaluation on the evaluation data and returns the result. Runs for self._eval_steps  steps, or if it's  None , then run until input is\nexhausted or another exception is raised. Start the evaluation after delay_secs  seconds, or if it's  None , defaults to using self._eval_delay_secs  seconds.    Args :   delay_secs : Start evaluating after this many seconds. If  None , defaults to using self._eval_delays_secs .     Returns :\n    The result of the  evaluate  call to the  Estimator .", 
            "title": "evaluate"
        }, 
        {
            "location": "/experiments/experiment/#continuous_eval", 
            "text": "continuous_eval ( self ,   delay_secs = None ,   throttle_delay_secs = None ,   evaluate_checkpoint_only_once = True ,   continuous_eval_predicate_fn = None )", 
            "title": "continuous_eval"
        }, 
        {
            "location": "/experiments/experiment/#continuous_eval_on_train_data", 
            "text": "continuous_eval_on_train_data ( self ,   delay_secs = None ,   throttle_delay_secs = None ,   continuous_eval_predicate_fn = None )", 
            "title": "continuous_eval_on_train_data"
        }, 
        {
            "location": "/experiments/experiment/#train_and_evaluate", 
            "text": "train_and_evaluate ( self )   Interleaves training and evaluation.  The frequency of evaluation is controlled by the constructor arg  eval_every_n_steps .\nWhen this parameter is None or 0, evaluation happens only after training has completed.\nNote that evaluation cannot happen more frequently than checkpoints are taken.\nIf no new snapshots are available when evaluation is supposed to occur,\nthen evaluation doesn't happen for another  eval_every_n_steps  steps\n(assuming a checkpoint is available at that point).\nThus, settings  eval_every_n_steps  to 1 means that the model will be evaluated\neverytime there is a new checkpoint.  This is particular useful for a \"Master\" task in the cloud, whose responsibility\nit is to take checkpoints, evaluate those checkpoints, and write out summaries.\nParticipating in training as the supervisor allows such a task to accomplish\nthe first and last items, while performing evaluation allows for the second.   Returns :\n    The result of the  evaluate  call to the  Estimator  as well as the\n    export results using the specified  ExportStrategy .", 
            "title": "train_and_evaluate"
        }, 
        {
            "location": "/experiments/experiment/#continuous_train_and_evaluate", 
            "text": "continuous_train_and_evaluate ( self ,   continuous_eval_predicate_fn = None )   Interleaves training and evaluation.  The frequency of evaluation is controlled by the  train_steps_per_iteration \n(via constructor). The model will be first trained for train_steps_per_iteration , and then be evaluated in turns.  This differs from  train_and_evaluate  as follows:\n    1. The procedure will have train and evaluation in turns. The model\n    will be trained for a number of steps (usuallly smaller than  train_steps \n    if provided) and then be evaluated.   train_and_evaluate  will train the\n    model for  train_steps  (no small training iteraions).  2. Due to the different approach this schedule takes, it leads to two\ndifferences in resource control. First, the resources (e.g., memory) used\nby training will be released before evaluation (`train_and_evaluate` takes\ndouble resources). Second, more checkpoints will be saved as a checkpoint\nis generated at the end of each small trainning iteration.    Args :   continuous_eval_predicate_fn : A predicate function determining whether to\ncontinue after each iteration.  predicate_fn  takes the evaluation\nresults as its arguments. At the beginning of evaluation, the passed\neval results will be None so it's expected that the predicate function\nhandles that gracefully. When  predicate_fn  is not specified, this will\nrun in an infinite loop or exit when global_step reaches  train_steps .     Returns :\n   A tuple of the result of the  evaluate  call to the  Estimator  and the\n   export results using the specified  ExportStrategy .    Raises :   ValueError : if  continuous_eval_predicate_fn  is neither None norcallable.", 
            "title": "continuous_train_and_evaluate"
        }, 
        {
            "location": "/experiments/experiment/#run_std_server", 
            "text": "run_std_server ( self )   Starts a TensorFlow server and joins the serving thread.  Typically used for parameter servers.   Raises :  ValueError : if not enough information is available in the estimator's\nconfig to create a server.", 
            "title": "run_std_server"
        }, 
        {
            "location": "/experiments/experiment/#test", 
            "text": "test ( self )   Tests training, evaluating and exporting the estimator for a single step.   Returns :\n    The result of the  evaluate  call to the  Estimator .", 
            "title": "test"
        }, 
        {
            "location": "/experiments/experiment/#create_experiment", 
            "text": "create_experiment ( experiment_config )   Creates a new  Experiment  instance.   Args :  experiment_config : the config to use for creating the experiment.", 
            "title": "create_experiment"
        }, 
        {
            "location": "/experiments/experiment/#run_experiment", 
            "text": "run_experiment ( experiment_fn ,   output_dir ,   schedule = None )   Make and run an experiment.  It creates an Experiment by calling  experiment_fn . Then it calls the\nfunction named as  schedule  of the Experiment.  If schedule is not provided, then the default schedule for the current task\ntype is used. The defaults are as follows:   'ps' maps to 'serve'  'worker' maps to 'train'  'master' maps to 'local_run'   If the experiment's config does not include a task type, then an exception\nis raised.   Example :     def   _create_my_experiment ( output_dir ):    return   tf . contrib . learn . Experiment (     estimator = my_estimator ( model_dir = output_dir ),     train_input_fn = my_train_input ,     eval_input_fn = my_eval_input )    run ( experiment_fn = _create_my_experiment ,    output_dir = some/output/dir ,    schedule = train )     Args :   experiment_fn : A function that creates an  Experiment . It should accept an\n  argument  output_dir  which should be used to create the  Estimator \n  (passed as  model_dir  to its constructor). It must return an\n   Experiment .  output_dir : Base output directory.  schedule : The name of the  method in the  Experiment  to run.     Returns :\n    The return value of function  schedule .    Raises :   ValueError : If  output_dir  is empty,  schedule  is None but no task\n  type is set in the built experiment's config, the task type has no\n  default, or  schedule  doesn't reference a member of  Experiment .  TypeError :  schedule  references non-callable member.", 
            "title": "run_experiment"
        }, 
        {
            "location": "/experiments/estimator/", 
            "text": "[source]\n\n\nEstimator\n\n\npolyaxon\n.\nexperiments\n.\nestimator\n.\nEstimator\n(\nmodel_fn\n=\nNone\n,\n \nmodel_dir\n=\nNone\n,\n \nconfig\n=\nNone\n,\n \nparams\n=\nNone\n)\n\n\n\n\n\n\nEstimator class is the basic TensorFlow model trainer/evaluator.\n\n\nConstructs an \nEstimator\n instance.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\n\n\nmodel_fn\n: Model function. Follows the signature:\n\n\n\n\nArgs:\n\n\nfeatures\n: single \nTensor\n or \ndict\n of \nTensor\ns\n     (depending on data passed to \nfit\n),\n\n\nlabels\n: \nTensor\n or \ndict\n of \nTensor\ns (for multi-head models).\n    If mode is \nModeKeys.PREDICT\n, \nlabels=None\n will be passed.\n    If the \nmodel_fn\n's signature does not accept \nmode\n,\n    the \nmodel_fn\n must still be able to handle \nlabels=None\n.\n\n\nmode\n: Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nparams\n: Optional \ndict\n of hyperparameters.  Will receive what\n    is passed to Estimator in \nparams\n parameter. This allows\n    to configure Estimators from hyper parameter tuning.\n\n\nconfig\n: Optional configuration object. Will receive what is passed\n    to Estimator in \nconfig\n parameter, or the default \nconfig\n.\n    Allows updating things in your model_fn based on configuration\n    such as \nnum_ps_replicas\n.\n\n\n\n\nmodel_dir\n: Optional directory where model parameters, graph etc\n    are saved. Will receive what is passed to Estimator in\n    \nmodel_dir\n parameter, or the default \nmodel_dir\n. Allows\n    updating things in your model_fn that expect model_dir, such as\n    training hooks.\n\n\n\n\n\n\nReturns:\n   \nEstimatorSpec\n\n\n\n\n\n\nSupports next three signatures for the function:\n\n\n\n\n(features, labels, mode)\n\n\n(features, labels, mode, params)\n\n\n(features, labels, mode, params, config)\n\n\n(features, labels, mode, params, config, model_dir)\n\n\n\n\n\n\n\n\nmodel_dir\n: Directory to save model parameters, graph and etc. This can\n    also be used to load checkpoints from the directory into a estimator to\n    continue training a previously saved model.\n\n\n\n\nconfig\n: Configuration object.\n\n\nparams\n: \ndict\n of hyper parameters that will be passed into \nmodel_fn\n.\n      Keys are names of parameters, values are basic python types.\n\n\nRaises\n:\n\n\nValueError\n: parameters of \nmodel_fn\n don't match \nparams\n.\n\n\n\n\n\n\n\n\n\n\nexport_savedmodel\n\n\nexport_savedmodel\n(\nself\n,\n \nexport_dir_base\n,\n \nserving_input_receiver_fn\n,\n \nassets_extra\n=\nNone\n,\n \nas_text\n=\nFalse\n,\n \ncheckpoint_path\n=\nNone\n)\n\n\n\n\n\n\nExports inference graph as a SavedModel into given dir.\nThis method builds a new graph by first calling the\nserving_input_receiver_fn to obtain feature \nTensor\ns, and then calling\nthis \nEstimator\n's model_fn to generate the model graph based on those\nfeatures. It restores the given checkpoint (or, lacking that, the most\nrecent checkpoint) into this graph in a fresh session.  Finally it creates\na timestamped export directory below the given export_dir_base, and writes\na \nSavedModel\n into it containing a single \nMetaGraphDef\n saved from this\nsession.\nThe exported \nMetaGraphDef\n will provide one \nSignatureDef\n for each\nelement of the export_outputs dict returned from the model_fn, named using\nthe same keys.  One of these keys is always\nsignature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which\nsignature will be served when a serving request does not specify one.\nFor each signature, the outputs are provided by the corresponding\n\nExportOutput\ns, and the inputs are always the input receivers provided by\nthe serving_input_receiver_fn.\nExtra assets may be written into the SavedModel via the extra_assets\nargument.  This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory.  The\ncorresponding value gives the full path of the source file to be copied.\nFor example, the simple case of copying a single file without renaming it\nis specified as \n{'my_asset_file.txt': '/path/to/my_asset_file.txt'}\n.\n- \nArgs\n:\n    - \nexport_dir_base\n: A string containing a directory in which to create\n    timestamped subdirectories containing exported SavedModels.\n    - \nserving_input_receiver_fn\n: A function that takes no argument and\n    returns a \nServingInputReceiver\n.\n    - \nassets_extra\n: A dict specifying how to populate the assets.extra directory\n    within the exported SavedModel, or \nNone\n if no extra assets are needed.\n    - \nas_text\n: whether to write the SavedModel proto in text format.\n    - \ncheckpoint_path\n: The checkpoint path to export.  If \nNone\n (the default),\n    the most recent checkpoint found within the model directory is chosen.\n- \nReturns\n:\n    The string path to the exported directory.\n- \nRaises\n:\n    - \nValueError\n: if no serving_input_receiver_fn is provided, no export_outputs\n    are provided, or no checkpoint can be found.\n\n\n\n\ntrain\n\n\ntrain\n(\nself\n,\n \ninput_fn\n=\nNone\n,\n \nsteps\n=\nNone\n,\n \nhooks\n=\nNone\n,\n \nmax_steps\n=\nNone\n)\n\n\n\n\n\n\nTrains a model given training data \nx\n predictions and \ny\n labels.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ninput_fn\n: Input function returning a tuple of:\nfeatures - \nTensor\n or dictionary of string feature name to \nTensor\n.\nlabels - \nTensor\n or dictionary of \nTensor\n with labels.\n\n\nsteps\n: Number of steps for which to train model. If \nNone\n, train forever.\n'steps' works incrementally. If you call two times fit(steps=10) then\ntraining occurs in total 20 steps. If you don't want to have incremental\nbehaviour please set \nmax_steps\n instead. If set, \nmax_steps\n must be\n\nNone\n.\n\n\nhooks\n: List of \nBaseMonitor\n subclass instances.\nUsed for callbacks inside the training loop.\n\n\nmax_steps\n: Number of total steps for which to train model. If \nNone\n,\ntrain forever. If set, \nsteps\n must be \nNone\n.\n\n\n\n\nTwo calls to \nfit(steps=100)\n means 200 training iterations.\nOn the other hand, two calls to \nfit(max_steps=100)\n means\nthat the second call will not do any iteration since first call did all 100 steps.\n\n\n\n\n\n\nReturns\n:\n    \nself\n, for chaining.\n\n\n\n\n\n\n\n\nevaluate\n\n\nevaluate\n(\nself\n,\n \ninput_fn\n=\nNone\n,\n \nsteps\n=\nNone\n,\n \nhooks\n=\nNone\n,\n \ncheckpoint_path\n=\nNone\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nEvaluates given model with provided evaluation data.\n\n\nStop conditions - we evaluate on the given input data until one of the\n- \nfollowing\n:\n- If \nsteps\n is provided, and \nsteps\n batches of size \nbatch_size\n are\nprocessed.\n- If \ninput_fn\n is provided, and it raises an end-of-input\nexception (\nOutOfRangeError\n or \nStopIteration\n).\n- If \nx\n is provided, and all items in \nx\n have been processed.\n\n\nThe return value is a dict containing the metrics specified in \nmetrics\n, as\nwell as an entry \nglobal_step\n which contains the value of the global step\nfor which this evaluation was performed.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ninput_fn\n: Input function returning a tuple of:\nfeatures - Dictionary of string feature name to \nTensor\n or \nTensor\n.\nlabels - \nTensor\n or dictionary of \nTensor\n with labels.\nIf \nsteps\n is not provided, this should raise \nOutOfRangeError\n or\n\nStopIteration\n after the desired amount of data (e.g., one epoch) has\nbeen provided. See \"Stop conditions\" above for specifics.\n\n\nsteps\n: Number of steps for which to evaluate model. If \nNone\n, evaluate\nuntil \nx\n is consumed or \ninput_fn\n raises an end-of-input exception.\nSee \"Stop conditions\" above for specifics.\n\n\nname\n: Name of the evaluation if user needs to run multiple evaluations on\ndifferent data sets, such as on training data vs test data.\n\n\ncheckpoint_path\n: Path of a specific checkpoint to evaluate. If \nNone\n,\nthe latest checkpoint in \nmodel_dir\n is used.\n\n\nhooks\n: List of \nSessionRunHook\n subclass instances.\nUsed for callbacks inside the evaluation call.\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \nmetrics\n is not \nNone\n or \ndict\n.\n\n\n\n\n\n\n\n\nReturns\n:\n    Returns \ndict\n with evaluation results.\n\n\n\n\n\n\n\n\npredict\n\n\npredict\n(\nself\n,\n \ninput_fn\n=\nNone\n,\n \npredict_keys\n=\nNone\n,\n \nhooks\n=\nNone\n,\n \ncheckpoint_path\n=\nNone\n)\n\n\n\n\n\n\nReturns predictions for given features.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ninput_fn\n: Input function returning features which is a dictionary of\nstring feature name to \nTensor\n or \nSparseTensor\n. If it returns a\ntuple, first item is extracted as features. Prediction continues until\n\ninput_fn\n raises an end-of-input exception (\nOutOfRangeError\n or \nStopIteration\n).\n\n\npredict_keys\n: list of \nstr\n, name of the keys to predict. It is used if\nthe \nEstimatorSpec.predictions\n is a \ndict\n. If \npredict_keys\n is used then rest\nof the predictions will be filtered from the dictionary. If \nNone\n, returns all.\n\n\nhooks\n: List of \nSessionRunHook\n subclass instances. Used for callbacks\ninside the prediction call.\n\n\ncheckpoint_path\n: Path of a specific checkpoint to predict. If \nNone\n, the\nlatest checkpoint in \nmodel_dir\n is used.\n\n\n\n\n\n\n\n\nYields\n:\n    Evaluated values of \npredictions\n tensors.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: Could not find a trained model in model_dir.\n\n\nValueError\n: if batch length of predictions are not same.\n\n\nValueError\n: If there is a conflict between \npredict_keys\n and \npredictions\n.\nFor example if \npredict_keys\n is not \nNone\n\nbut \nEstimatorSpec.predictions\n is not a \ndict\n.\n\n\n\n\n\n\n\n\n\n\nget_variable_value\n\n\nget_variable_value\n(\nself\n,\n \nname\n)\n\n\n\n\n\n\nReturns value of the variable given by name.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nname\n: string, name of the tensor.\n\n\n\n\n\n\n\n\nReturns\n:\n    Numpy array - value of the tensor.\n\n\n\n\n\n\n\n\nget_variable_names\n\n\nget_variable_names\n(\nself\n)\n\n\n\n\n\n\nReturns list of all variable names in this model.\n\n\n\n\nReturns\n:\n    List of names.", 
            "title": "Estimator"
        }, 
        {
            "location": "/experiments/estimator/#estimator", 
            "text": "polyaxon . experiments . estimator . Estimator ( model_fn = None ,   model_dir = None ,   config = None ,   params = None )   Estimator class is the basic TensorFlow model trainer/evaluator.  Constructs an  Estimator  instance.    Args :    model_fn : Model function. Follows the signature:   Args:  features : single  Tensor  or  dict  of  Tensor s\n     (depending on data passed to  fit ),  labels :  Tensor  or  dict  of  Tensor s (for multi-head models).\n    If mode is  ModeKeys.PREDICT ,  labels=None  will be passed.\n    If the  model_fn 's signature does not accept  mode ,\n    the  model_fn  must still be able to handle  labels=None .  mode : Specifies if this training, evaluation or prediction. See  ModeKeys .  params : Optional  dict  of hyperparameters.  Will receive what\n    is passed to Estimator in  params  parameter. This allows\n    to configure Estimators from hyper parameter tuning.  config : Optional configuration object. Will receive what is passed\n    to Estimator in  config  parameter, or the default  config .\n    Allows updating things in your model_fn based on configuration\n    such as  num_ps_replicas .   model_dir : Optional directory where model parameters, graph etc\n    are saved. Will receive what is passed to Estimator in\n     model_dir  parameter, or the default  model_dir . Allows\n    updating things in your model_fn that expect model_dir, such as\n    training hooks.    Returns:\n    EstimatorSpec    Supports next three signatures for the function:   (features, labels, mode)  (features, labels, mode, params)  (features, labels, mode, params, config)  (features, labels, mode, params, config, model_dir)     model_dir : Directory to save model parameters, graph and etc. This can\n    also be used to load checkpoints from the directory into a estimator to\n    continue training a previously saved model.   config : Configuration object.  params :  dict  of hyper parameters that will be passed into  model_fn .\n      Keys are names of parameters, values are basic python types.  Raises :  ValueError : parameters of  model_fn  don't match  params .", 
            "title": "Estimator"
        }, 
        {
            "location": "/experiments/estimator/#export_savedmodel", 
            "text": "export_savedmodel ( self ,   export_dir_base ,   serving_input_receiver_fn ,   assets_extra = None ,   as_text = False ,   checkpoint_path = None )   Exports inference graph as a SavedModel into given dir.\nThis method builds a new graph by first calling the\nserving_input_receiver_fn to obtain feature  Tensor s, and then calling\nthis  Estimator 's model_fn to generate the model graph based on those\nfeatures. It restores the given checkpoint (or, lacking that, the most\nrecent checkpoint) into this graph in a fresh session.  Finally it creates\na timestamped export directory below the given export_dir_base, and writes\na  SavedModel  into it containing a single  MetaGraphDef  saved from this\nsession.\nThe exported  MetaGraphDef  will provide one  SignatureDef  for each\nelement of the export_outputs dict returned from the model_fn, named using\nthe same keys.  One of these keys is always\nsignature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which\nsignature will be served when a serving request does not specify one.\nFor each signature, the outputs are provided by the corresponding ExportOutput s, and the inputs are always the input receivers provided by\nthe serving_input_receiver_fn.\nExtra assets may be written into the SavedModel via the extra_assets\nargument.  This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory.  The\ncorresponding value gives the full path of the source file to be copied.\nFor example, the simple case of copying a single file without renaming it\nis specified as  {'my_asset_file.txt': '/path/to/my_asset_file.txt'} .\n-  Args :\n    -  export_dir_base : A string containing a directory in which to create\n    timestamped subdirectories containing exported SavedModels.\n    -  serving_input_receiver_fn : A function that takes no argument and\n    returns a  ServingInputReceiver .\n    -  assets_extra : A dict specifying how to populate the assets.extra directory\n    within the exported SavedModel, or  None  if no extra assets are needed.\n    -  as_text : whether to write the SavedModel proto in text format.\n    -  checkpoint_path : The checkpoint path to export.  If  None  (the default),\n    the most recent checkpoint found within the model directory is chosen.\n-  Returns :\n    The string path to the exported directory.\n-  Raises :\n    -  ValueError : if no serving_input_receiver_fn is provided, no export_outputs\n    are provided, or no checkpoint can be found.", 
            "title": "export_savedmodel"
        }, 
        {
            "location": "/experiments/estimator/#train", 
            "text": "train ( self ,   input_fn = None ,   steps = None ,   hooks = None ,   max_steps = None )   Trains a model given training data  x  predictions and  y  labels.    Args :   input_fn : Input function returning a tuple of:\nfeatures -  Tensor  or dictionary of string feature name to  Tensor .\nlabels -  Tensor  or dictionary of  Tensor  with labels.  steps : Number of steps for which to train model. If  None , train forever.\n'steps' works incrementally. If you call two times fit(steps=10) then\ntraining occurs in total 20 steps. If you don't want to have incremental\nbehaviour please set  max_steps  instead. If set,  max_steps  must be None .  hooks : List of  BaseMonitor  subclass instances.\nUsed for callbacks inside the training loop.  max_steps : Number of total steps for which to train model. If  None ,\ntrain forever. If set,  steps  must be  None .   Two calls to  fit(steps=100)  means 200 training iterations.\nOn the other hand, two calls to  fit(max_steps=100)  means\nthat the second call will not do any iteration since first call did all 100 steps.    Returns :\n     self , for chaining.", 
            "title": "train"
        }, 
        {
            "location": "/experiments/estimator/#evaluate", 
            "text": "evaluate ( self ,   input_fn = None ,   steps = None ,   hooks = None ,   checkpoint_path = None ,   name = None )   Evaluates given model with provided evaluation data.  Stop conditions - we evaluate on the given input data until one of the\n-  following :\n- If  steps  is provided, and  steps  batches of size  batch_size  are\nprocessed.\n- If  input_fn  is provided, and it raises an end-of-input\nexception ( OutOfRangeError  or  StopIteration ).\n- If  x  is provided, and all items in  x  have been processed.  The return value is a dict containing the metrics specified in  metrics , as\nwell as an entry  global_step  which contains the value of the global step\nfor which this evaluation was performed.    Args :   input_fn : Input function returning a tuple of:\nfeatures - Dictionary of string feature name to  Tensor  or  Tensor .\nlabels -  Tensor  or dictionary of  Tensor  with labels.\nIf  steps  is not provided, this should raise  OutOfRangeError  or StopIteration  after the desired amount of data (e.g., one epoch) has\nbeen provided. See \"Stop conditions\" above for specifics.  steps : Number of steps for which to evaluate model. If  None , evaluate\nuntil  x  is consumed or  input_fn  raises an end-of-input exception.\nSee \"Stop conditions\" above for specifics.  name : Name of the evaluation if user needs to run multiple evaluations on\ndifferent data sets, such as on training data vs test data.  checkpoint_path : Path of a specific checkpoint to evaluate. If  None ,\nthe latest checkpoint in  model_dir  is used.  hooks : List of  SessionRunHook  subclass instances.\nUsed for callbacks inside the evaluation call.     Raises :   ValueError : If  metrics  is not  None  or  dict .     Returns :\n    Returns  dict  with evaluation results.", 
            "title": "evaluate"
        }, 
        {
            "location": "/experiments/estimator/#predict", 
            "text": "predict ( self ,   input_fn = None ,   predict_keys = None ,   hooks = None ,   checkpoint_path = None )   Returns predictions for given features.    Args :   input_fn : Input function returning features which is a dictionary of\nstring feature name to  Tensor  or  SparseTensor . If it returns a\ntuple, first item is extracted as features. Prediction continues until input_fn  raises an end-of-input exception ( OutOfRangeError  or  StopIteration ).  predict_keys : list of  str , name of the keys to predict. It is used if\nthe  EstimatorSpec.predictions  is a  dict . If  predict_keys  is used then rest\nof the predictions will be filtered from the dictionary. If  None , returns all.  hooks : List of  SessionRunHook  subclass instances. Used for callbacks\ninside the prediction call.  checkpoint_path : Path of a specific checkpoint to predict. If  None , the\nlatest checkpoint in  model_dir  is used.     Yields :\n    Evaluated values of  predictions  tensors.    Raises :   ValueError : Could not find a trained model in model_dir.  ValueError : if batch length of predictions are not same.  ValueError : If there is a conflict between  predict_keys  and  predictions .\nFor example if  predict_keys  is not  None \nbut  EstimatorSpec.predictions  is not a  dict .", 
            "title": "predict"
        }, 
        {
            "location": "/experiments/estimator/#get_variable_value", 
            "text": "get_variable_value ( self ,   name )   Returns value of the variable given by name.    Args :   name : string, name of the tensor.     Returns :\n    Numpy array - value of the tensor.", 
            "title": "get_variable_value"
        }, 
        {
            "location": "/experiments/estimator/#get_variable_names", 
            "text": "get_variable_names ( self )   Returns list of all variable names in this model.   Returns :\n    List of names.", 
            "title": "get_variable_names"
        }, 
        {
            "location": "/experiments/base_model/", 
            "text": "[source]\n\n\nBaseModel\n\n\npolyaxon\n.\nmodels\n.\nbase\n.\nBaseModel\n(\nmode\n,\n \nmodel_type\n,\n \ngraph_fn\n,\n \nloss_config\n,\n \noptimizer_config\n=\nNone\n,\n \neval_metrics_config\n=\nNone\n,\n \nsummaries\n=\nall\n,\n \nclip_gradients\n=\n0.5\n,\n \nname\n=\nModel\n)\n\n\n\n\n\n\nBase class for models.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ngraph_fn\n: Graph function. Follows the signature:\n\n\nArgs:\n\n\nmode\n: Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ninputs\n: the feature inputs.\n\n\n\n\n\n\nloss_config\n: An instance of \nLossConfig\n.\n\n\noptimizer_config\n: An instance of \nOptimizerConfig\n. Default value \nAdam\n.\n\n\nmodel_type\n: \nstr\n, the type of this model.\n    Possible values: \nregressor\n, \nclassifier\n, \ngenerator\n\n\nsummaries\n: \nstr\n or \nlist\n. The verbosity of the tensorboard visualization.\n    Possible values: \nall\n, \nactivations\n, \nloss\n, \nlearning_rate\n, \nvariables\n, \ngradients\n\n\nname\n: \nstr\n, the name of this model, everything will be encapsulated inside this scope.\n\n\n\n\n\n\n\n\nReturns\n:\n    \nEstimatorSpec\n\n\n\n\n\n\n\n\n_clip_gradients_fn\n\n\n_clip_gradients_fn\n(\nself\n,\n \ngrads_and_vars\n)\n\n\n\n\n\n\nClips gradients by global norm.\n\n\n\n\n_build_optimizer\n\n\n_build_optimizer\n(\nself\n)\n\n\n\n\n\n\nCreates the optimizer\n\n\n\n\n_build_summary_op\n\n\n_build_summary_op\n(\nself\n,\n \nresults\n=\nNone\n,\n \ngenerated\n=\nNone\n,\n \nfeatures\n=\nNone\n,\n \nlabels\n=\nNone\n)\n\n\n\n\n\n\nBuilds summaries for this model.\n\n\nThe summaries are one value (or more) of:\n    * (\nACTIVATIONS\n, \nVARIABLES\n, \nGRADIENTS\n, \nLOSS\n, \nLEARNING_RATE\n)\n\n\n\n\n_build_loss\n\n\n_build_loss\n(\nself\n,\n \nresults\n,\n \nfeatures\n,\n \nlabels\n)\n\n\n\n\n\n\nCreates the loss operation\n\n\n\n\nReturns\n:\n     tuple \n(losses, loss)\n:\n    \nlosses\n are the per-batch losses.\n    \nloss\n is a single scalar tensor to minimize.\n\n\n\n\n\n\n_build_eval_metrics\n\n\n_build_eval_metrics\n(\nself\n,\n \nresults\n,\n \nfeatures\n,\n \nlabels\n)\n\n\n\n\n\n\nCreates the loss operation\n\n\nReturns a tuple \n(losses, loss)\n:\n    \nlosses\n are the per-batch losses.\n    \nloss\n is a single scalar tensor to minimize.\n\n\n\n\n_build_train_op\n\n\n_build_train_op\n(\nself\n,\n \nloss\n)\n\n\n\n\n\n\nCreates the training operation\n\n\n\n\n_preprocess\n\n\n_preprocess\n(\nself\n,\n \nmode\n,\n \nfeatures\n,\n \nlabels\n)\n\n\n\n\n\n\nModel specific preprocessing.\n\n\n\n\n_build_predictions\n\n\n_build_predictions\n(\nresults\n,\n \nfeatures\n,\n \nlabels\n,\n \nlosses\n=\nNone\n)\n\n\n\n\n\n\nCreates the dictionary of predictions that is returned by the model.\n\n\n\n\n_build\n\n\n_build\n(\nself\n,\n \nfeatures\n,\n \nlabels\n,\n \nparams\n=\nNone\n,\n \nconfig\n=\nNone\n)\n\n\n\n\n\n\nBuild the different operation of the model.\n\n\n\n\nbatch_size\n\n\nbatch_size\n(\nfeatures\n,\n \nlabels\n)\n\n\n\n\n\n\nReturns the batch size of the curren batch based on the passed features.\n\n\n\n\nArgs\n:\n\n\nfeatures\n: The features.\n\n\nlabels\n: The labels", 
            "title": "Base Model"
        }, 
        {
            "location": "/experiments/base_model/#basemodel", 
            "text": "polyaxon . models . base . BaseModel ( mode ,   model_type ,   graph_fn ,   loss_config ,   optimizer_config = None ,   eval_metrics_config = None ,   summaries = all ,   clip_gradients = 0.5 ,   name = Model )   Base class for models.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  graph_fn : Graph function. Follows the signature:  Args:  mode : Specifies if this training, evaluation or prediction. See  ModeKeys .  inputs : the feature inputs.    loss_config : An instance of  LossConfig .  optimizer_config : An instance of  OptimizerConfig . Default value  Adam .  model_type :  str , the type of this model.\n    Possible values:  regressor ,  classifier ,  generator  summaries :  str  or  list . The verbosity of the tensorboard visualization.\n    Possible values:  all ,  activations ,  loss ,  learning_rate ,  variables ,  gradients  name :  str , the name of this model, everything will be encapsulated inside this scope.     Returns :\n     EstimatorSpec", 
            "title": "BaseModel"
        }, 
        {
            "location": "/experiments/base_model/#_clip_gradients_fn", 
            "text": "_clip_gradients_fn ( self ,   grads_and_vars )   Clips gradients by global norm.", 
            "title": "_clip_gradients_fn"
        }, 
        {
            "location": "/experiments/base_model/#_build_optimizer", 
            "text": "_build_optimizer ( self )   Creates the optimizer", 
            "title": "_build_optimizer"
        }, 
        {
            "location": "/experiments/base_model/#_build_summary_op", 
            "text": "_build_summary_op ( self ,   results = None ,   generated = None ,   features = None ,   labels = None )   Builds summaries for this model.  The summaries are one value (or more) of:\n    * ( ACTIVATIONS ,  VARIABLES ,  GRADIENTS ,  LOSS ,  LEARNING_RATE )", 
            "title": "_build_summary_op"
        }, 
        {
            "location": "/experiments/base_model/#_build_loss", 
            "text": "_build_loss ( self ,   results ,   features ,   labels )   Creates the loss operation   Returns :\n     tuple  (losses, loss) :\n     losses  are the per-batch losses.\n     loss  is a single scalar tensor to minimize.", 
            "title": "_build_loss"
        }, 
        {
            "location": "/experiments/base_model/#_build_eval_metrics", 
            "text": "_build_eval_metrics ( self ,   results ,   features ,   labels )   Creates the loss operation  Returns a tuple  (losses, loss) :\n     losses  are the per-batch losses.\n     loss  is a single scalar tensor to minimize.", 
            "title": "_build_eval_metrics"
        }, 
        {
            "location": "/experiments/base_model/#_build_train_op", 
            "text": "_build_train_op ( self ,   loss )   Creates the training operation", 
            "title": "_build_train_op"
        }, 
        {
            "location": "/experiments/base_model/#_preprocess", 
            "text": "_preprocess ( self ,   mode ,   features ,   labels )   Model specific preprocessing.", 
            "title": "_preprocess"
        }, 
        {
            "location": "/experiments/base_model/#_build_predictions", 
            "text": "_build_predictions ( results ,   features ,   labels ,   losses = None )   Creates the dictionary of predictions that is returned by the model.", 
            "title": "_build_predictions"
        }, 
        {
            "location": "/experiments/base_model/#_build", 
            "text": "_build ( self ,   features ,   labels ,   params = None ,   config = None )   Build the different operation of the model.", 
            "title": "_build"
        }, 
        {
            "location": "/experiments/base_model/#batch_size", 
            "text": "batch_size ( features ,   labels )   Returns the batch size of the curren batch based on the passed features.   Args :  features : The features.  labels : The labels", 
            "title": "batch_size"
        }, 
        {
            "location": "/experiments/models/", 
            "text": "[source]\n\n\nRegressor\n\n\npolyaxon\n.\nmodels\n.\nregressors\n.\nRegressor\n(\nmode\n,\n \ngraph_fn\n,\n \nloss_config\n=\nNone\n,\n \noptimizer_config\n=\nNone\n,\n \neval_metrics_config\n=\nNone\n,\n \nsummaries\n=\nall\n,\n \nclip_gradients\n=\n0.5\n,\n \nname\n=\nRegressor\n)\n\n\n\n\n\n\nRegressor base model.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ngraph_fn\n: Graph function. Follows the signature:\n\n\nArgs:\n\n\nmode\n: Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ninputs\n: the feature inputs.\n\n\n\n\n\n\nloss_config\n: An instance of \nLossConfig\n. Default value \nmean_squared_error\n.\n\n\noptimizer_config\n: An instance of \nOptimizerConfig\n. Default value \nAdam\n.\n\n\nsummaries\n: \nstr\n or \nlist\n. The verbosity of the tensorboard visualization.\n    Possible values: \nall\n, \nactivations\n, \nloss\n, \nlearning_rate\n, \nvariables\n, \ngradients\n\n\nname\n: \nstr\n, the name of this model, everything will be encapsulated inside this scope.\n\n\n\n\n\n\n\n\nReturns\n:\n    \nEstimatorSpec\n\n\n\n\n\n\n\n\n[source]\n\n\nClassifier\n\n\npolyaxon\n.\nmodels\n.\nclassifiers\n.\nClassifier\n(\nmode\n,\n \ngraph_fn\n,\n \nloss_config\n=\nNone\n,\n \noptimizer_config\n=\nNone\n,\n \nsummaries\n=\nall\n,\n \neval_metrics_config\n=\nNone\n,\n \nclip_gradients\n=\n0.5\n,\n \none_hot_encode\n=\nNone\n,\n \nn_classes\n=\nNone\n,\n \nname\n=\nClassfier\n)\n\n\n\n\n\n\nRegressor base model.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ngraph_fn\n: Graph function. Follows the signature:\n\n\nArgs:\n\n\nmode\n: Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ninputs\n: the feature inputs.\n\n\n\n\n\n\nloss_config\n: An instance of \nLossConfig\n. Default value \nsigmoid_cross_entropy\n.\n\n\noptimizer_config\n: An instance of \nOptimizerConfig\n. Default value \nAdam\n.\n\n\nsummaries\n: \nstr\n or \nlist\n. The verbosity of the tensorboard visualization.\n    Possible values: \nall\n, \nactivations\n, \nloss\n, \nlearning_rate\n, \nvariables\n, \ngradients\n\n\nname\n: \nstr\n, the name of this model, everything will be encapsulated inside this scope.\n\n\none_hot_encode\n: \nbool\n. to one hot encode the outputs.\n\n\nn_classes\n: \nint\n. The number of classes used in the one hot encoding.\n\n\n\n\n\n\n\n\nReturns\n:\n    \nEstimatorSpec\n\n\n\n\n\n\n\n\n[source]\n\n\nGenerator\n\n\npolyaxon\n.\nmodels\n.\ngenerators\n.\nGenerator\n(\nmode\n,\n \nencoder_fn\n,\n \ndecoder_fn\n,\n \nbridge_fn\n,\n \nloss_config\n=\nNone\n,\n \noptimizer_config\n=\nNone\n,\n \nsummaries\n=\nall\n,\n \neval_metrics_config\n=\nNone\n,\n \nclip_gradients\n=\n0.5\n,\n \nname\n=\nGenerator\n)\n\n\n\n\n\n\nGenerator base model.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nencoder_fn\n: Encoder Graph function. Follows the signature:\n\n\nArgs:\n\n\nmode\n: Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ninputs\n: the feature inputs.\n\n\n\n\n\n\ndecoder_fn\n: Decoder Graph function. Follows the signature:\n\n\nArgs:\n\n\nmode\n: Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ninputs\n: the feature inputs.\n\n\n\n\n\n\nbridge_fn\n: The bridge to use. Follows the signature:\n\n\nArgs:\n\n\nmode\n: Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ninputs\n: the feature inputs.\n\n\nencoder_fn\n: the encoder function.\n\n\ndecoder_fn\n the decoder function.\n\n\n\n\n\n\nloss_config\n: An instance of \nLossConfig\n. Default value \nmean_squared_error\n.\n\n\noptimizer_config\n: An instance of \nOptimizerConfig\n. Default value \nAdadelta\n.\n\n\nsummaries\n: \nstr\n or \nlist\n. The verbosity of the tensorboard visualization.\n    Possible values: \nall\n, \nactivations\n, \nloss\n, \nlearning_rate\n, \nvariables\n, \ngradients\n\n\nname\n: \nstr\n, the name of this model, everything will be encapsulated inside this scope.\n\n\n\n\n\n\n\n\nReturns\n:\n    \nEstimatorSpec", 
            "title": "Models"
        }, 
        {
            "location": "/experiments/models/#regressor", 
            "text": "polyaxon . models . regressors . Regressor ( mode ,   graph_fn ,   loss_config = None ,   optimizer_config = None ,   eval_metrics_config = None ,   summaries = all ,   clip_gradients = 0.5 ,   name = Regressor )   Regressor base model.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  graph_fn : Graph function. Follows the signature:  Args:  mode : Specifies if this training, evaluation or prediction. See  ModeKeys .  inputs : the feature inputs.    loss_config : An instance of  LossConfig . Default value  mean_squared_error .  optimizer_config : An instance of  OptimizerConfig . Default value  Adam .  summaries :  str  or  list . The verbosity of the tensorboard visualization.\n    Possible values:  all ,  activations ,  loss ,  learning_rate ,  variables ,  gradients  name :  str , the name of this model, everything will be encapsulated inside this scope.     Returns :\n     EstimatorSpec     [source]", 
            "title": "Regressor"
        }, 
        {
            "location": "/experiments/models/#classifier", 
            "text": "polyaxon . models . classifiers . Classifier ( mode ,   graph_fn ,   loss_config = None ,   optimizer_config = None ,   summaries = all ,   eval_metrics_config = None ,   clip_gradients = 0.5 ,   one_hot_encode = None ,   n_classes = None ,   name = Classfier )   Regressor base model.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  graph_fn : Graph function. Follows the signature:  Args:  mode : Specifies if this training, evaluation or prediction. See  ModeKeys .  inputs : the feature inputs.    loss_config : An instance of  LossConfig . Default value  sigmoid_cross_entropy .  optimizer_config : An instance of  OptimizerConfig . Default value  Adam .  summaries :  str  or  list . The verbosity of the tensorboard visualization.\n    Possible values:  all ,  activations ,  loss ,  learning_rate ,  variables ,  gradients  name :  str , the name of this model, everything will be encapsulated inside this scope.  one_hot_encode :  bool . to one hot encode the outputs.  n_classes :  int . The number of classes used in the one hot encoding.     Returns :\n     EstimatorSpec     [source]", 
            "title": "Classifier"
        }, 
        {
            "location": "/experiments/models/#generator", 
            "text": "polyaxon . models . generators . Generator ( mode ,   encoder_fn ,   decoder_fn ,   bridge_fn ,   loss_config = None ,   optimizer_config = None ,   summaries = all ,   eval_metrics_config = None ,   clip_gradients = 0.5 ,   name = Generator )   Generator base model.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  encoder_fn : Encoder Graph function. Follows the signature:  Args:  mode : Specifies if this training, evaluation or prediction. See  ModeKeys .  inputs : the feature inputs.    decoder_fn : Decoder Graph function. Follows the signature:  Args:  mode : Specifies if this training, evaluation or prediction. See  ModeKeys .  inputs : the feature inputs.    bridge_fn : The bridge to use. Follows the signature:  Args:  mode : Specifies if this training, evaluation or prediction. See  ModeKeys .  inputs : the feature inputs.  encoder_fn : the encoder function.  decoder_fn  the decoder function.    loss_config : An instance of  LossConfig . Default value  mean_squared_error .  optimizer_config : An instance of  OptimizerConfig . Default value  Adadelta .  summaries :  str  or  list . The verbosity of the tensorboard visualization.\n    Possible values:  all ,  activations ,  loss ,  learning_rate ,  variables ,  gradients  name :  str , the name of this model, everything will be encapsulated inside this scope.     Returns :\n     EstimatorSpec", 
            "title": "Generator"
        }, 
        {
            "location": "/experiments/hooks/", 
            "text": "[source]\n\n\nGlobalStepWaiterHook\n\n\npolyaxon\n.\nexperiments\n.\nhooks\n.\nGlobalStepWaiterHook\n(\nwait_until_step\n)\n\n\n\n\n\n\nDelay execution until global step reaches to wait_until_step.\n(A mirror to tensorflow.python.training.basic_session_run_hooks GlobalStepWaiterHook.)\n\n\nThis hook delays execution until global step reaches to \nwait_until_step\n. It\nis used to gradually start workers in distributed settings. One example usage\nwould be setting \nwait_until_step=int(K*log(task_id+1))\n assuming that\ntask_id=0 is the chief.\n\n\n\n\nArgs\n:\n\n\nwait_until_step\n: an \nint\n shows until which global step should we wait.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nFinalOpsHook\n\n\npolyaxon\n.\nexperiments\n.\nhooks\n.\nFinalOpsHook\n(\nfinal_ops\n,\n \nfinal_ops_feed_dict\n=\nNone\n)\n\n\n\n\n\n\nA run hook which evaluates \nTensors\n at the end of a session.\n(A mirror to tensorflow.python.training.basic_session_run_hooks GlobalStepWaiterHook.)\n\n\n\n\nArgs\n:\n\n\nfinal_ops\n: A single \nTensor\n, a list of \nTensors\n or a dictionary of names to \nTensors\n.\n\n\nfinal_ops_feed_dict\n: A feed dictionary to use when running \nfinal_ops_dict\n.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nStepCounterHook\n\n\npolyaxon\n.\nexperiments\n.\nhooks\n.\nStepCounterHook\n(\nevery_n_steps\n=\n100\n,\n \nevery_n_secs\n=\nNone\n,\n \noutput_dir\n=\nNone\n,\n \nsummary_writer\n=\nNone\n)\n\n\n\n\n\n\nSteps per second monitor.\n(A mirror to tensorflow.python.training.basic_session_run_hooks CheckpointSaverHook.)\n\n\n\n\n[source]\n\n\nLoggingTensorHook\n\n\npolyaxon\n.\nexperiments\n.\nhooks\n.\nLoggingTensorHook\n(\ntensors\n,\n \nevery_n_iter\n=\nNone\n,\n \nevery_n_secs\n=\nNone\n,\n \nformatter\n=\nNone\n)\n\n\n\n\n\n\nPrints the given tensors once every N local steps or once every N seconds.\n(A mirror to tensorflow.python.training.basic_session_run_hooks LoggingTensorHook.)\n\n\nThe tensors will be printed to the log, with \nINFO\n severity.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ntensors\n: \ndict\n that maps string-valued tags to tensors/tensor names,\n    or \niterable\n of tensors/tensor names.\n\n\nevery_n_iter\n: \nint\n, print the values of \ntensors\n once every N local\n    steps taken on the current worker.\n\n\nevery_n_secs\n: \nint\n or \nfloat\n, print the values of \ntensors\n once every N\n    seconds. Exactly one of \nevery_n_iter\n and \nevery_n_secs\n should be\n    provided.\n\n\nformatter\n: function, takes dict of \ntag\n-\nTensor\n and returns a string.\n    If \nNone\n uses default printing all tensors.\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if \nevery_n_iter\n is non-positive.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nCheckpointSaverHook\n\n\npolyaxon\n.\nexperiments\n.\nhooks\n.\nCheckpointSaverHook\n(\ncheckpoint_dir\n,\n \nsave_secs\n=\nNone\n,\n \nsave_steps\n=\nNone\n,\n \nsaver\n=\nNone\n,\n \ncheckpoint_basename\n=\nmodel.ckpt\n,\n \nscaffold\n=\nNone\n,\n \nlisteners\n=\nNone\n)\n\n\n\n\n\n\nSaves checkpoints every N steps or seconds.\n(A mirror to tensorflow.python.training.basic_session_run_hooks CheckpointSaverHook.)\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ncheckpoint_dir\n: \nstr\n, base directory for the checkpoint files.\n\n\nsave_secs\n: \nint\n, save every N secs.\n\n\nsave_steps\n: \nint\n, save every N steps.\n\n\nsaver\n: \nSaver\n object, used for saving.\n\n\ncheckpoint_basename\n: \nstr\n, base name for the checkpoint files.\n\n\nscaffold\n: \nScaffold\n, use to get saver object.\n\n\nlisteners\n: List of \nCheckpointSaverListener\n subclass instances.\n    Used for callbacks that run immediately after the corresponding\n    CheckpointSaverHook callbacks, only in steps where the\n    CheckpointSaverHook was triggered.\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: One of \nsave_steps\n or \nsave_secs\n should be set.\n\n\nValueError\n: Exactly one of saver or scaffold should be set.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nStopAfterNEvalsHook\n\n\npolyaxon\n.\nexperiments\n.\nhooks\n.\nStopAfterNEvalsHook\n(\nnum_evals\n,\n \nlog_progress\n=\nTrue\n)\n\n\n\n\n\n\nRun hook used by the evaluation routines to run the \neval_ops\n N times.\n\n\n\n\n[source]\n\n\nStopAtStepHook\n\n\npolyaxon\n.\nexperiments\n.\nhooks\n.\nStopAtStepHook\n(\nnum_steps\n=\nNone\n,\n \nlast_step\n=\nNone\n)\n\n\n\n\n\n\nMonitor to request stop at a specified step.\n(A mirror to tensorflow.python.training.basic_session_run_hooks StopAtStepHook.)\n\n\nThis hook requests stop after either a number of steps have been\nexecuted or a last step has been reached. Only one of the two options can be\nspecified.\n\n\nif \nnum_steps\n is specified, it indicates the number of steps to execute\nafter \nbegin()\n is called. If instead \nlast_step\n is specified, it\nindicates the last step we want to execute, as passed to the \nafter_run()\n\ncall.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nnum_steps\n: Number of steps to execute.\n\n\nlast_step\n: Step after which to stop.\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If one of the arguments is invalid.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nNanTensorHook\n\n\npolyaxon\n.\nexperiments\n.\nhooks\n.\nNanTensorHook\n(\nloss_tensor\n,\n \nfail_on_nan_loss\n=\nTrue\n)\n\n\n\n\n\n\nNaN Loss monitor.\n(A mirror to tensorflow.python.training.basic_session_run_hooks NanTensorHook.)\n\n\nMonitors loss and stops training if loss is NaN.\nCan either fail with exception or just stop training.\n\n\n\n\nArgs\n:\n\n\nloss_tensor\n: \nTensor\n, the loss tensor.\n\n\nfail_on_nan_loss\n: \nbool\n, whether to raise exception when loss is NaN.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nSummarySaverHook\n\n\npolyaxon\n.\nexperiments\n.\nhooks\n.\nSummarySaverHook\n(\nsave_steps\n=\nNone\n,\n \nsave_secs\n=\nNone\n,\n \noutput_dir\n=\nNone\n,\n \nsummary_writer\n=\nNone\n,\n \nscaffold\n=\nNone\n,\n \nsummary_op\n=\nNone\n)\n\n\n\n\n\n\nSaves summaries every N steps.\n(A mirror to tensorflow.python.training.basic_session_run_hooks NanTensorHook.)\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nsave_steps\n: \nint\n, save summaries every N steps. Exactly one of\n    \nsave_secs\n and \nsave_steps\n should be set.\n\n\nsave_secs\n: \nint\n, save summaries every N seconds.\n\n\noutput_dir\n: \nstring\n, the directory to save the summaries to. Only used\n    if no \nsummary_writer\n is supplied.\n\n\nsummary_writer\n: \nSummaryWriter\n. If \nNone\n and an \noutput_dir\n was passed,\n    one will be created accordingly.\n\n\nscaffold\n: \nScaffold\n to get summary_op if it's not provided.\n\n\nsummary_op\n: \nTensor\n of type \nstring\n containing the serialized \nSummary\n\n    protocol buffer or a list of \nTensor\n. They are most likely an output\n    by TF summary methods like \ntf.summary.scalar\n or\n    \ntf.summary.merge_all\n. It can be passed in as one tensor; if more\n    than one, they must be passed in as a list.\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: Exactly one of scaffold or summary_op should be set.", 
            "title": "Hooks"
        }, 
        {
            "location": "/experiments/hooks/#globalstepwaiterhook", 
            "text": "polyaxon . experiments . hooks . GlobalStepWaiterHook ( wait_until_step )   Delay execution until global step reaches to wait_until_step.\n(A mirror to tensorflow.python.training.basic_session_run_hooks GlobalStepWaiterHook.)  This hook delays execution until global step reaches to  wait_until_step . It\nis used to gradually start workers in distributed settings. One example usage\nwould be setting  wait_until_step=int(K*log(task_id+1))  assuming that\ntask_id=0 is the chief.   Args :  wait_until_step : an  int  shows until which global step should we wait.      [source]", 
            "title": "GlobalStepWaiterHook"
        }, 
        {
            "location": "/experiments/hooks/#finalopshook", 
            "text": "polyaxon . experiments . hooks . FinalOpsHook ( final_ops ,   final_ops_feed_dict = None )   A run hook which evaluates  Tensors  at the end of a session.\n(A mirror to tensorflow.python.training.basic_session_run_hooks GlobalStepWaiterHook.)   Args :  final_ops : A single  Tensor , a list of  Tensors  or a dictionary of names to  Tensors .  final_ops_feed_dict : A feed dictionary to use when running  final_ops_dict .      [source]", 
            "title": "FinalOpsHook"
        }, 
        {
            "location": "/experiments/hooks/#stepcounterhook", 
            "text": "polyaxon . experiments . hooks . StepCounterHook ( every_n_steps = 100 ,   every_n_secs = None ,   output_dir = None ,   summary_writer = None )   Steps per second monitor.\n(A mirror to tensorflow.python.training.basic_session_run_hooks CheckpointSaverHook.)   [source]", 
            "title": "StepCounterHook"
        }, 
        {
            "location": "/experiments/hooks/#loggingtensorhook", 
            "text": "polyaxon . experiments . hooks . LoggingTensorHook ( tensors ,   every_n_iter = None ,   every_n_secs = None ,   formatter = None )   Prints the given tensors once every N local steps or once every N seconds.\n(A mirror to tensorflow.python.training.basic_session_run_hooks LoggingTensorHook.)  The tensors will be printed to the log, with  INFO  severity.    Args :   tensors :  dict  that maps string-valued tags to tensors/tensor names,\n    or  iterable  of tensors/tensor names.  every_n_iter :  int , print the values of  tensors  once every N local\n    steps taken on the current worker.  every_n_secs :  int  or  float , print the values of  tensors  once every N\n    seconds. Exactly one of  every_n_iter  and  every_n_secs  should be\n    provided.  formatter : function, takes dict of  tag - Tensor  and returns a string.\n    If  None  uses default printing all tensors.     Raises :   ValueError : if  every_n_iter  is non-positive.      [source]", 
            "title": "LoggingTensorHook"
        }, 
        {
            "location": "/experiments/hooks/#checkpointsaverhook", 
            "text": "polyaxon . experiments . hooks . CheckpointSaverHook ( checkpoint_dir ,   save_secs = None ,   save_steps = None ,   saver = None ,   checkpoint_basename = model.ckpt ,   scaffold = None ,   listeners = None )   Saves checkpoints every N steps or seconds.\n(A mirror to tensorflow.python.training.basic_session_run_hooks CheckpointSaverHook.)    Args :   checkpoint_dir :  str , base directory for the checkpoint files.  save_secs :  int , save every N secs.  save_steps :  int , save every N steps.  saver :  Saver  object, used for saving.  checkpoint_basename :  str , base name for the checkpoint files.  scaffold :  Scaffold , use to get saver object.  listeners : List of  CheckpointSaverListener  subclass instances.\n    Used for callbacks that run immediately after the corresponding\n    CheckpointSaverHook callbacks, only in steps where the\n    CheckpointSaverHook was triggered.     Raises :   ValueError : One of  save_steps  or  save_secs  should be set.  ValueError : Exactly one of saver or scaffold should be set.      [source]", 
            "title": "CheckpointSaverHook"
        }, 
        {
            "location": "/experiments/hooks/#stopafternevalshook", 
            "text": "polyaxon . experiments . hooks . StopAfterNEvalsHook ( num_evals ,   log_progress = True )   Run hook used by the evaluation routines to run the  eval_ops  N times.   [source]", 
            "title": "StopAfterNEvalsHook"
        }, 
        {
            "location": "/experiments/hooks/#stopatstephook", 
            "text": "polyaxon . experiments . hooks . StopAtStepHook ( num_steps = None ,   last_step = None )   Monitor to request stop at a specified step.\n(A mirror to tensorflow.python.training.basic_session_run_hooks StopAtStepHook.)  This hook requests stop after either a number of steps have been\nexecuted or a last step has been reached. Only one of the two options can be\nspecified.  if  num_steps  is specified, it indicates the number of steps to execute\nafter  begin()  is called. If instead  last_step  is specified, it\nindicates the last step we want to execute, as passed to the  after_run() \ncall.    Args :   num_steps : Number of steps to execute.  last_step : Step after which to stop.     Raises :   ValueError : If one of the arguments is invalid.      [source]", 
            "title": "StopAtStepHook"
        }, 
        {
            "location": "/experiments/hooks/#nantensorhook", 
            "text": "polyaxon . experiments . hooks . NanTensorHook ( loss_tensor ,   fail_on_nan_loss = True )   NaN Loss monitor.\n(A mirror to tensorflow.python.training.basic_session_run_hooks NanTensorHook.)  Monitors loss and stops training if loss is NaN.\nCan either fail with exception or just stop training.   Args :  loss_tensor :  Tensor , the loss tensor.  fail_on_nan_loss :  bool , whether to raise exception when loss is NaN.      [source]", 
            "title": "NanTensorHook"
        }, 
        {
            "location": "/experiments/hooks/#summarysaverhook", 
            "text": "polyaxon . experiments . hooks . SummarySaverHook ( save_steps = None ,   save_secs = None ,   output_dir = None ,   summary_writer = None ,   scaffold = None ,   summary_op = None )   Saves summaries every N steps.\n(A mirror to tensorflow.python.training.basic_session_run_hooks NanTensorHook.)    Args :   save_steps :  int , save summaries every N steps. Exactly one of\n     save_secs  and  save_steps  should be set.  save_secs :  int , save summaries every N seconds.  output_dir :  string , the directory to save the summaries to. Only used\n    if no  summary_writer  is supplied.  summary_writer :  SummaryWriter . If  None  and an  output_dir  was passed,\n    one will be created accordingly.  scaffold :  Scaffold  to get summary_op if it's not provided.  summary_op :  Tensor  of type  string  containing the serialized  Summary \n    protocol buffer or a list of  Tensor . They are most likely an output\n    by TF summary methods like  tf.summary.scalar  or\n     tf.summary.merge_all . It can be passed in as one tensor; if more\n    than one, they must be passed in as a list.     Raises :   ValueError : Exactly one of scaffold or summary_op should be set.", 
            "title": "SummarySaverHook"
        }, 
        {
            "location": "/experiments/subgraph/", 
            "text": "[source]\n\n\nSubGraph\n\n\npolyaxon\n.\nexperiments\n.\nsubgraph\n.\nSubGraph\n(\nmode\n,\n \nmodules\n,\n \nname\n=\nSubgraph\n,\n \nfeatures\n=\nNone\n)\n\n\n\n\n\n\nThe \nSubGraph\n is a class that represents the flow of layers.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n. Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nname\n: \nstr\n. The name of this subgraph, used for creating the scope.\n\n\nmodules\n: \nlist\n.  The modules to connect inside this subgraph, e.g. layers\n\n\nfeatures\n: \nlist\n. The list of features keys to extract and use in this subgraph.\n    If \nNone\n, all features will be used.", 
            "title": "Subgraph"
        }, 
        {
            "location": "/experiments/subgraph/#subgraph", 
            "text": "polyaxon . experiments . subgraph . SubGraph ( mode ,   modules ,   name = Subgraph ,   features = None )   The  SubGraph  is a class that represents the flow of layers.   Args :  mode :  str . Specifies if this training, evaluation or prediction. See  ModeKeys .  name :  str . The name of this subgraph, used for creating the scope.  modules :  list .  The modules to connect inside this subgraph, e.g. layers  features :  list . The list of features keys to extract and use in this subgraph.\n    If  None , all features will be used.", 
            "title": "SubGraph"
        }, 
        {
            "location": "/experiments/summarizer/", 
            "text": "[source]\n\n\nSummaryOptions\n\n\npolyaxon\n.\nexperiments\n.\nsummarizer\n.\nSummaryOptions\n()\n\n\n\n\n\n\n\n\n[source]\n\n\nSummaryTypes\n\n\npolyaxon\n.\nexperiments\n.\nsummarizer\n.\nSummaryTypes\n()\n\n\n\n\n\n\n\n\nadd_learning_rate_summaries\n\n\nadd_learning_rate_summaries\n()\n\n\n\n\n\n\n\n\nadd_loss_summaries\n\n\nadd_loss_summaries\n(\ntotal_loss\n,\n \nloss\n)\n\n\n\n\n\n\nAdds loss scalar summaries.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ntotal_loss\n: \nTensor\n. The total loss (Regression loss + regularization losses).\n\n\nloss\n: \nTensor\n. Regression loss.\n\n\n\n\n\n\n\n\nReturns\n:\n    The list of created loss summaries.\n\n\n\n\n\n\n\n\nadd_activations_summary\n\n\nadd_activations_summary\n(\nactivation_ops\n)\n\n\n\n\n\n\nAdds histogram and scalar summary for given activations.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nactivation_ops\n: A list of \nTensor\n. The activations to summarize.\n\n\n\n\n\n\n\n\nReturns\n:\n    The list of created activation summaries.\n\n\n\n\n\n\n\n\nadd_gradients_summary\n\n\nadd_gradients_summary\n(\ngrads\n)\n\n\n\n\n\n\nAdd histogram summary for given gradients and scalar summary for clipped gradients.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ngrads\n: A list of \nTensor\n. The gradients to summarize.\n\n\n\n\n\n\n\n\nReturns\n:\n    The list of created gradient summaries.\n\n\n\n\n\n\n\n\nadd_trainable_vars_summary\n\n\nadd_trainable_vars_summary\n(\nvariables\n)\n\n\n\n\n\n\nAdds histogram summary for given variables weights.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nvariables\n: A list of \nVariable\n. The variables to summarize.\n\n\n\n\n\n\n\n\nReturns\n:\n    The list of created weights summaries.", 
            "title": "Summerizer"
        }, 
        {
            "location": "/experiments/summarizer/#summaryoptions", 
            "text": "polyaxon . experiments . summarizer . SummaryOptions ()    [source]", 
            "title": "SummaryOptions"
        }, 
        {
            "location": "/experiments/summarizer/#summarytypes", 
            "text": "polyaxon . experiments . summarizer . SummaryTypes ()", 
            "title": "SummaryTypes"
        }, 
        {
            "location": "/experiments/summarizer/#add_learning_rate_summaries", 
            "text": "add_learning_rate_summaries ()", 
            "title": "add_learning_rate_summaries"
        }, 
        {
            "location": "/experiments/summarizer/#add_loss_summaries", 
            "text": "add_loss_summaries ( total_loss ,   loss )   Adds loss scalar summaries.    Args :   total_loss :  Tensor . The total loss (Regression loss + regularization losses).  loss :  Tensor . Regression loss.     Returns :\n    The list of created loss summaries.", 
            "title": "add_loss_summaries"
        }, 
        {
            "location": "/experiments/summarizer/#add_activations_summary", 
            "text": "add_activations_summary ( activation_ops )   Adds histogram and scalar summary for given activations.    Args :   activation_ops : A list of  Tensor . The activations to summarize.     Returns :\n    The list of created activation summaries.", 
            "title": "add_activations_summary"
        }, 
        {
            "location": "/experiments/summarizer/#add_gradients_summary", 
            "text": "add_gradients_summary ( grads )   Add histogram summary for given gradients and scalar summary for clipped gradients.    Args :   grads : A list of  Tensor . The gradients to summarize.     Returns :\n    The list of created gradient summaries.", 
            "title": "add_gradients_summary"
        }, 
        {
            "location": "/experiments/summarizer/#add_trainable_vars_summary", 
            "text": "add_trainable_vars_summary ( variables )   Adds histogram summary for given variables weights.    Args :   variables : A list of  Variable . The variables to summarize.     Returns :\n    The list of created weights summaries.", 
            "title": "add_trainable_vars_summary"
        }, 
        {
            "location": "/layers/introduction/", 
            "text": "Introduction\n\n\nAll \nLayers\n in Polyaxon inherits from the \nBaseLayer\n (which is \nGraphModule\n child).\n\n\nThis property allows the layers to share the common behaviors.\n\n\nAll layers must defined there behavior in the \n_build\n function. \nAnd they should take into account the current mode of the estimator. (\nTRAIN\n, \nEVAL\n, and \nPREDICT\n).\n\n\nimport\n \npolyaxon\n \nas\n \nplx\n\n\n\nlayer\n \n=\n \nplx\n.\nlayers\n.\nFullyConnected\n(\nmode\n=\nplx\n.\nModeKeys\n.\nTRAIN\n,\n \nn_units\n=\n64\n,\n \nactivation\n=\ntanh\n)\n\n\n\n\n\n\nOnce the layer is created, it can be called as many time as needed with different inputs.\n\n\nThe layer itself knows how to validate the incoming values.\n\n\nA layer can build other layers inside its \n_build\n function.\n\n\nExample\n\n\nimport\n \ntensorflow\n \nas\n \ntf\n\n\nimport\n \npolyaxon\n \nas\n \nplx\n\n\n\nrnn_lstm\n \n=\n \nplx\n.\nlayers\n.\nLSTM\n(\nplx\n.\nModeKeys\n.\nTRAIN\n,\n \nnum_units\n=\n3\n,\n \nactivation\n=\ntanh\n,\n \n                           \ninner_activation\n=\ntf\n.\nnn\n.\nsigmoid\n,\n \ndropout\n=\n0.3\n,\n \nnum_layers\n=\n4\n)\n\n\n\n\n\n\nNote that for activations, initializers, and regularizers you can use a string (should be one of the supported values),\nyou can provide a function, or a tensor/value.", 
            "title": "Introduction"
        }, 
        {
            "location": "/layers/introduction/#introduction", 
            "text": "All  Layers  in Polyaxon inherits from the  BaseLayer  (which is  GraphModule  child).  This property allows the layers to share the common behaviors.  All layers must defined there behavior in the  _build  function. \nAnd they should take into account the current mode of the estimator. ( TRAIN ,  EVAL , and  PREDICT ).  import   polyaxon   as   plx  layer   =   plx . layers . FullyConnected ( mode = plx . ModeKeys . TRAIN ,   n_units = 64 ,   activation = tanh )   Once the layer is created, it can be called as many time as needed with different inputs.  The layer itself knows how to validate the incoming values.  A layer can build other layers inside its  _build  function.", 
            "title": "Introduction"
        }, 
        {
            "location": "/layers/introduction/#example", 
            "text": "import   tensorflow   as   tf  import   polyaxon   as   plx  rnn_lstm   =   plx . layers . LSTM ( plx . ModeKeys . TRAIN ,   num_units = 3 ,   activation = tanh ,  \n                            inner_activation = tf . nn . sigmoid ,   dropout = 0.3 ,   num_layers = 4 )   Note that for activations, initializers, and regularizers you can use a string (should be one of the supported values),\nyou can provide a function, or a tensor/value.", 
            "title": "Example"
        }, 
        {
            "location": "/layers/core/", 
            "text": "[source]\n\n\nFullyConnected\n\n\npolyaxon\n.\nlayers\n.\ncore\n.\nFullyConnected\n(\nmode\n,\n \nnum_units\n,\n \nactivation\n=\nlinear\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\ntruncated_normal\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nNone\n,\n \nscale\n=\n0.001\n,\n \ndropout\n=\nNone\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nFullyConnected\n)\n\n\n\n\n\n\nAdds a fully connected layer.\n\n\nfully_connected\n creates a variable called \nw\n, representing a fully\nconnected weight matrix, which is multiplied by the \nincoming\n to produce a\n\nTensor\n of hidden units.\n\n\n\n\n\n\nNote\n: that if \ninputs\n have a rank greater than 2, then \ninputs\n is flattened\nprior to the initial matrix multiply by \nweights\n.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'truncated_normal'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ndropout\n: \nfloat\n. Adds a dropout with \nkeep_prob\n as \n1 - dropout\n.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'FullyConnected'.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nw\n: \nTensor\n. Variable representing units weights.\n\n\nb\n: \nTensor\n. Variable representing biases.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nDropout\n\n\npolyaxon\n.\nlayers\n.\ncore\n.\nDropout\n(\nmode\n,\n \nkeep_prob\n,\n \nnoise_shape\n=\nNone\n,\n \nseed\n=\nNone\n,\n \nname\n=\nDropout\n)\n\n\n\n\n\n\nAdds a Dropout op to the input.\n\n\nOutputs the input element scaled up by \n1 / keep_prob\n. The scaling is so\nthat the expected sum is unchanged.\n\n\nBy default, each element is kept or dropped independently. If noise_shape\nis specified, it must be broadcastable to the shape of x, and only dimensions\nwith noise_shape[i] == shape(x)[i] will make independent decisions. For\nexample, if shape(x) = [k, l, m, n] and noise_shape = [k, 1, 1, n], each\nbatch and channel component will be kept independently and each row and column\nwill be kept or not kept together.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\nkeep_prob : A float representing the probability that each element\n    is kept.\nnoise_shape : A 1-D Tensor of type int32, representing the shape for\n    randomly generated keep/drop flags.\nname : A name for this layer (optional).\n\n\n\n\n\n\n\n\nReferences\n:\n\n\n\n\nDropout\n: A Simple Way to Prevent Neural Networks from Overfitting.\nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever \n R. Salakhutdinov,\n(2014), Journal of Machine Learning Research, 5(Jun)(2), 1929-1958.\n\n\n\n\n\n\n\n\nLinks\n:\n\n\n\n\n[https\n://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf]\n    (https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n\n\n\n\n\n\n[source]\n\n\nReshape\n\n\npolyaxon\n.\nlayers\n.\ncore\n.\nReshape\n(\nmode\n,\n \nnew_shape\n,\n \nname\n=\nReshape\n)\n\n\n\n\n\n\nReshape.\n\n\nA layer that reshape the incoming layer tensor output to the desired shape.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnew_shape\n: A list of \nint\n. The desired shape.\n\n\nname\n: A name for this layer (optional).\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nFlatten\n\n\npolyaxon\n.\nlayers\n.\ncore\n.\nFlatten\n(\nmode\n,\n \nname\n=\nFlatten\n)\n\n\n\n\n\n\nFlatten the incoming Tensor.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nname\n: A name for this layer (optional).\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nSingleUnit\n\n\npolyaxon\n.\nlayers\n.\ncore\n.\nSingleUnit\n(\nmode\n,\n \nactivation\n=\nlinear\n,\n \nbias\n=\nTrue\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nLinear\n)\n\n\n\n\n\n\nAdds a Single Unit Layer.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n. Activation applied to this layer. Default: 'linear'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'Linear'.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nW\n: \nTensor\n. Variable representing weight.\n\n\nb\n: \nTensor\n. Variable representing bias.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nHighway\n\n\npolyaxon\n.\nlayers\n.\ncore\n.\nHighway\n(\nmode\n,\n \nnum_units\n,\n \nactivation\n=\nlinear\n,\n \ntransform_dropout\n=\nNone\n,\n \nweights_init\n=\ntruncated_normal\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nNone\n,\n \nscale\n=\n0.001\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nFullyConnectedHighway\n)\n\n\n\n\n\n\nAdds Fully Connected Highway.\n\n\nA fully connected highway network layer, with some inspiration from\n- __\nhttps__://github.com/fomorians/highway-fcn\n.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\ntransform_dropout\n: \nfloat\n: Keep probability on the highway transform gate.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'truncated_normal'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'FullyConnectedHighway'.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nW\n: \nTensor\n. Variable representing units weights.\n\n\nW_t\n: \nTensor\n. Variable representing units weights for transform gate.\n\n\nb\n: \nTensor\n. Variable representing biases.\n\n\nb_t\n: \nTensor\n. Variable representing biases for transform gate.\n\n\n\n\n\n\n\n\nLinks\n:\n\n\n\n\n__\nhttps__://arxiv.org/abs/1505.00387\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nOneHotEncoding\n\n\npolyaxon\n.\nlayers\n.\ncore\n.\nOneHotEncoding\n(\nmode\n,\n \nn_classes\n,\n \non_value\n=\n1.0\n,\n \noff_value\n=\n0.0\n,\n \nname\n=\nOneHotEncoding\n)\n\n\n\n\n\n\nTransform numeric labels into one hot labels using \ntf.one_hot\n.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nn_classes\n: \nint\n. Total number of classes.\n\n\non_value\n: \nscalar\n. A scalar defining the on-value.\n\n\noff_value\n: \nscalar\n. A scalar defining the off-value.\n\n\nname\n: A name for this layer (optional). Default: 'OneHotEncoding'.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nGaussianNoise\n\n\npolyaxon\n.\nlayers\n.\ncore\n.\nGaussianNoise\n(\nmode\n,\n \nscale\n=\n1\n,\n \nmean\n=\n0.0\n,\n \nstddev\n=\n1.0\n,\n \nseed\n=\nNone\n,\n \nname\n=\nGaussianNoise\n)\n\n\n\n\n\n\nAdditive zero-centered Gaussian noise.\n\n\nThis is useful to mitigate overfitting, could be used as a form of random data augmentation.\nGaussian Noise (GS) is a natural choice as corruption process for real valued inputs.\n\n\nAs it is a regularization layer, it is only active at training time.\n\n\n\n\nArgs\n:\n\n\nscale\n: A 0-D Tensor or Python \nfloat\n. The scale at which to apply the the noise.\n\n\nmean\n: A 0-D Tensor or Python \nfloat\n. The mean of the noise distribution.\n\n\nstddev\n: A 0-D Tensor or Python \nfloat\n. The standard deviation of the noise distribution.\n\n\nseed\n: A Python integer. Used to create a random seed. See @{tf.set_random_seed}.\n\n\nname\n: A name for this operation (optional).\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nMerge\n\n\npolyaxon\n.\nlayers\n.\ncore\n.\nMerge\n(\nmode\n,\n \nmodules\n,\n \nmerge_mode\n,\n \naxis\n=\n1\n,\n \nname\n=\nMerge\n)\n\n\n\n\n\n\n\n\n[source]\n\n\nSlice\n\n\npolyaxon\n.\nlayers\n.\ncore\n.\nSlice\n(\nmode\n,\n \nbegin\n,\n \nsize\n,\n \nname\n=\nSlice\n)\n\n\n\n\n\n\nExtracts a slice from a tensor.\n\n\nThis operation extracts a slice of size size from a tensor input starting at\nthe location specified by begin.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nname\n: \nstr\n. A name for this layer (optional).", 
            "title": "Core Layers"
        }, 
        {
            "location": "/layers/core/#fullyconnected", 
            "text": "polyaxon . layers . core . FullyConnected ( mode ,   num_units ,   activation = linear ,   bias = True ,   weights_init = truncated_normal ,   bias_init = zeros ,   regularizer = None ,   scale = 0.001 ,   dropout = None ,   trainable = True ,   restore = True ,   name = FullyConnected )   Adds a fully connected layer.  fully_connected  creates a variable called  w , representing a fully\nconnected weight matrix, which is multiplied by the  incoming  to produce a Tensor  of hidden units.    Note : that if  inputs  have a rank greater than 2, then  inputs  is flattened\nprior to the initial matrix multiply by  weights .    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_units :  int , number of units for this layer.  activation :  str  (name) or  function  (returning a  Tensor ).  Default : 'linear'.    bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'truncated_normal'.    bias_init :  str  (name) or  Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  dropout :  float . Adds a dropout with  keep_prob  as  1 - dropout .  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'FullyConnected'.     Attributes :   w :  Tensor . Variable representing units weights.  b :  Tensor . Variable representing biases.      [source]", 
            "title": "FullyConnected"
        }, 
        {
            "location": "/layers/core/#dropout", 
            "text": "polyaxon . layers . core . Dropout ( mode ,   keep_prob ,   noise_shape = None ,   seed = None ,   name = Dropout )   Adds a Dropout op to the input.  Outputs the input element scaled up by  1 / keep_prob . The scaling is so\nthat the expected sum is unchanged.  By default, each element is kept or dropped independently. If noise_shape\nis specified, it must be broadcastable to the shape of x, and only dimensions\nwith noise_shape[i] == shape(x)[i] will make independent decisions. For\nexample, if shape(x) = [k, l, m, n] and noise_shape = [k, 1, 1, n], each\nbatch and channel component will be kept independently and each row and column\nwill be kept or not kept together.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .\nkeep_prob : A float representing the probability that each element\n    is kept.\nnoise_shape : A 1-D Tensor of type int32, representing the shape for\n    randomly generated keep/drop flags.\nname : A name for this layer (optional).     References :   Dropout : A Simple Way to Prevent Neural Networks from Overfitting.\nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever   R. Salakhutdinov,\n(2014), Journal of Machine Learning Research, 5(Jun)(2), 1929-1958.     Links :   [https ://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf]\n    (https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)    [source]", 
            "title": "Dropout"
        }, 
        {
            "location": "/layers/core/#reshape", 
            "text": "polyaxon . layers . core . Reshape ( mode ,   new_shape ,   name = Reshape )   Reshape.  A layer that reshape the incoming layer tensor output to the desired shape.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  new_shape : A list of  int . The desired shape.  name : A name for this layer (optional).      [source]", 
            "title": "Reshape"
        }, 
        {
            "location": "/layers/core/#flatten", 
            "text": "polyaxon . layers . core . Flatten ( mode ,   name = Flatten )   Flatten the incoming Tensor.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  name : A name for this layer (optional).      [source]", 
            "title": "Flatten"
        }, 
        {
            "location": "/layers/core/#singleunit", 
            "text": "polyaxon . layers . core . SingleUnit ( mode ,   activation = linear ,   bias = True ,   trainable = True ,   restore = True ,   name = Linear )   Adds a Single Unit Layer.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  activation :  str  (name) or  function . Activation applied to this layer. Default: 'linear'.  bias :  bool . If True, a bias is used.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'Linear'.     Attributes :   W :  Tensor . Variable representing weight.  b :  Tensor . Variable representing bias.      [source]", 
            "title": "SingleUnit"
        }, 
        {
            "location": "/layers/core/#highway", 
            "text": "polyaxon . layers . core . Highway ( mode ,   num_units ,   activation = linear ,   transform_dropout = None ,   weights_init = truncated_normal ,   bias_init = zeros ,   regularizer = None ,   scale = 0.001 ,   trainable = True ,   restore = True ,   name = FullyConnectedHighway )   Adds Fully Connected Highway.  A fully connected highway network layer, with some inspiration from\n- __ https__://github.com/fomorians/highway-fcn .    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_units :  int , number of units for this layer.  activation :  str  (name) or  function  (returning a  Tensor ).  Default : 'linear'.    transform_dropout :  float : Keep probability on the highway transform gate.  weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'truncated_normal'.    bias_init :  str  (name) or  Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'FullyConnectedHighway'.     Attributes :   W :  Tensor . Variable representing units weights.  W_t :  Tensor . Variable representing units weights for transform gate.  b :  Tensor . Variable representing biases.  b_t :  Tensor . Variable representing biases for transform gate.     Links :   __ https__://arxiv.org/abs/1505.00387      [source]", 
            "title": "Highway"
        }, 
        {
            "location": "/layers/core/#onehotencoding", 
            "text": "polyaxon . layers . core . OneHotEncoding ( mode ,   n_classes ,   on_value = 1.0 ,   off_value = 0.0 ,   name = OneHotEncoding )   Transform numeric labels into one hot labels using  tf.one_hot .   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  n_classes :  int . Total number of classes.  on_value :  scalar . A scalar defining the on-value.  off_value :  scalar . A scalar defining the off-value.  name : A name for this layer (optional). Default: 'OneHotEncoding'.      [source]", 
            "title": "OneHotEncoding"
        }, 
        {
            "location": "/layers/core/#gaussiannoise", 
            "text": "polyaxon . layers . core . GaussianNoise ( mode ,   scale = 1 ,   mean = 0.0 ,   stddev = 1.0 ,   seed = None ,   name = GaussianNoise )   Additive zero-centered Gaussian noise.  This is useful to mitigate overfitting, could be used as a form of random data augmentation.\nGaussian Noise (GS) is a natural choice as corruption process for real valued inputs.  As it is a regularization layer, it is only active at training time.   Args :  scale : A 0-D Tensor or Python  float . The scale at which to apply the the noise.  mean : A 0-D Tensor or Python  float . The mean of the noise distribution.  stddev : A 0-D Tensor or Python  float . The standard deviation of the noise distribution.  seed : A Python integer. Used to create a random seed. See @{tf.set_random_seed}.  name : A name for this operation (optional).      [source]", 
            "title": "GaussianNoise"
        }, 
        {
            "location": "/layers/core/#merge", 
            "text": "polyaxon . layers . core . Merge ( mode ,   modules ,   merge_mode ,   axis = 1 ,   name = Merge )    [source]", 
            "title": "Merge"
        }, 
        {
            "location": "/layers/core/#slice", 
            "text": "polyaxon . layers . core . Slice ( mode ,   begin ,   size ,   name = Slice )   Extracts a slice from a tensor.  This operation extracts a slice of size size from a tensor input starting at\nthe location specified by begin.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  name :  str . A name for this layer (optional).", 
            "title": "Slice"
        }, 
        {
            "location": "/layers/convolutional/", 
            "text": "[source]\n\n\nConv2d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nConv2d\n(\nmode\n,\n \nnum_filter\n,\n \nfilter_size\n,\n \nstrides\n=\n1\n,\n \npadding\n=\nSAME\n,\n \nactivation\n=\nlinear\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nuniform_scaling\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nNone\n,\n \nscale\n=\n0.001\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nConv2D\n)\n\n\n\n\n\n\nAdds a 2D convolution layer.\n\n\nThis operation creates a variable called 'w', representing the convolutional kernel,\nthat is convolved with the input. A second variable called 'b' is added to the result of\nthe convolution operation.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_filter\n: \nint\n. The number of convolutional filters.\n\n\nfilter_size\n: \nint\n or \nlist of int\n. Size of filters.\n\n\nstrides\n: 'int\nor list of\nint`. Strides of conv operation.\n\n\nDefault\n: [1 1 1 1].\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n) or None.\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'truncated_normal'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'Conv2D'.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nw\n: \nVariable\n. Variable representing filter weights.\n\n\nb\n: \nVariable\n. Variable representing biases.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nConv2dTranspose\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nConv2dTranspose\n(\nmode\n,\n \nnum_filter\n,\n \nfilter_size\n,\n \noutput_shape\n,\n \nstrides\n=\n1\n,\n \npadding\n=\nSAME\n,\n \nactivation\n=\nlinear\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nuniform_scaling\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nNone\n,\n \nscale\n=\n0.001\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nConv2DTranspose\n)\n\n\n\n\n\n\nAdds a Convolution 2D Transpose.\n\n\nThis operation is sometimes called \"deconvolution\" after (Deconvolutional\n- \nNetworks)[http\n://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf], but is\nactually the transpose (gradient) of \nconv2d\n rather than an actual\ndeconvolution.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_filter\n: \nint\n. The number of convolutional filters.\n\n\nfilter_size\n: \nint\n or \nlist of int\n. Size of filters.\n\n\noutput_shape\n: \nlist of int\n. Dimensions of the output tensor.\n    Can optionally include the number of conv filters.\n    [new height, new width, num_filter] or [new height, new width].\n\n\nstrides\n: \nint\n or list of \nint\n. Strides of conv operation.\n\n\nDefault\n: [1 1 1 1].\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'truncated_normal'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'Conv2DTranspose'.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nw\n: \nVariable\n. Variable representing filter weights.\n\n\nb\n: \nVariable\n. Variable representing biases.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nMaxPool2d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nMaxPool2d\n(\nmode\n,\n \nkernel_size\n,\n \nstrides\n=\nNone\n,\n \npadding\n=\nSAME\n,\n \nname\n=\nMaxPool2D\n)\n\n\n\n\n\n\nAdds Max Pooling 2D.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nkernel_size\n: 'int\nor\nlist of int`. Pooling kernel size.\n\n\nstrides\n: 'int\nor\nlist of int`. Strides of conv operation.\n\n\nDefault\n: SAME as kernel_size.\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nname\n: A name for this layer (optional). Default: 'MaxPool2D'.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nAvgPool2d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nAvgPool2d\n(\nmode\n,\n \nkernel_size\n,\n \nstrides\n=\nNone\n,\n \npadding\n=\nSAME\n,\n \nname\n=\nAvgPool2D\n)\n\n\n\n\n\n\nAdds Average Pooling 2D.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nkernel_size\n: 'int\nor\nlist of int`. Pooling kernel size.\n\n\nstrides\n: 'int\nor\nlist of int`. Strides of conv operation.\n\n\nDefault\n: SAME as kernel_size.\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nname\n: A name for this layer (optional). Default: 'AvgPool2D'.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nUpsample2d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nUpsample2d\n(\nmode\n,\n \nkernel_size\n,\n \nname\n=\nUpSample2D\n)\n\n\n\n\n\n\nAdds UpSample 2D operation.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nkernel_size\n: 'int\nor\nlist of int`. Upsampling kernel size.\n\n\nname\n: A name for this layer (optional). Default: 'UpSample2D'.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nHighwayConv2d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nHighwayConv2d\n(\nmode\n,\n \nnum_filter\n,\n \nfilter_size\n,\n \nstrides\n=\n1\n,\n \npadding\n=\nSAME\n,\n \nactivation\n=\nlinear\n,\n \nweights_init\n=\nuniform_scaling\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nNone\n,\n \nscale\n=\n0.001\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nHighwayConv2D\n)\n\n\n\n\n\n\nAdds a Highway Convolution 2D.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_filter\n: \nint\n. The number of convolutional filters.\n\n\nfilter_size\n: 'int\nor\nlist of int`. Size of filters.\n\n\nstrides\n: 'int\nor\nlist of int`. Strides of conv operation.\n\n\nDefault\n: [1 1 1 1].\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'truncated_normal'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'Conv2D'.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nw\n: \nVariable\n. Variable representing filter weights.\n\n\nw_t\n: \nVariable\n. Variable representing gate weights.\n\n\nb\n: \nVariable\n. Variable representing biases.\n\n\nb_t\n: \nVariable\n. Variable representing gate biases.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nUpscore\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nUpscore\n(\nmode\n,\n \nnum_classes\n,\n \nshape\n=\nNone\n,\n \nkernel_size\n=\n4\n,\n \nstrides\n=\n2\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nUpscore\n)\n\n\n\n\n\n\nAdds an Upscore layer.\n\n\nThis implements the upscore layer as used in\n(Fully Convolutional Networks)[http://arxiv.org/abs/1411.4038].\nThe upscore layer is initialized as bilinear upsampling filter.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_classes\n: \nint\n. Number of output feature maps.\n\n\nshape\n: \nlist of int\n. Dimension of the output map\n    [batch_size, new height, new width]. For convinience four values\n     are allows [batch_size, new height, new width, X], where X\n     is ignored.\n\n\nkernel_size\n: 'int\nor\nlist of int`. Upsampling kernel size.\n\n\nstrides\n: 'int\nor\nlist of int`. Strides of conv operation.\n\n\nDefault\n: [1 2 2 1].\n\n\n\n\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'Upscore'.\n\n\n\n\n\n\n\n\nLinks\n:\n    (Fully Convolutional Networks)[http://arxiv.org/abs/1411.4038]\n\n\n\n\n\n\n\n\n[source]\n\n\nConv1d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nConv1d\n(\nmode\n,\n \nnum_filter\n,\n \nfilter_size\n,\n \nstrides\n=\n1\n,\n \npadding\n=\nSAME\n,\n \nactivation\n=\nlinear\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nuniform_scaling\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nNone\n,\n \nscale\n=\n0.001\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nConv1D\n)\n\n\n\n\n\n\nAdds a Convolution 1D.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_filter\n: \nint\n. The number of convolutional filters.\n\n\nfilter_size\n: 'int\nor\nlist of int`. Size of filters.\n\n\nstrides\n: 'int\nor\nlist of int`. Strides of conv operation.\n\n\nDefault\n: [1 1 1 1].\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'truncated_normal'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'Conv1D'.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nw\n: \nVariable\n. Variable representing filter weights.\n\n\nb\n: \nVariable\n. Variable representing biases.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nMaxPool1d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nMaxPool1d\n(\nmode\n,\n \nkernel_size\n,\n \nstrides\n=\nNone\n,\n \npadding\n=\nSAME\n,\n \nname\n=\nMaxPool1D\n)\n\n\n\n\n\n\nAdds Max Pooling 1D.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nkernel_size\n: \nint\n or \nlist of int\n. Pooling kernel size.\n\n\nstrides\n: \nint\n or \nlist of int\n. Strides of conv operation.\n\n\nDefault\n: SAME as kernel_size.\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nname\n: A name for this layer (optional). Default: 'MaxPool1D'.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nAvgPool1d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nAvgPool1d\n(\nmode\n,\n \nkernel_size\n,\n \nstrides\n=\nNone\n,\n \npadding\n=\nSAME\n,\n \nname\n=\nAvgPool1D\n)\n\n\n\n\n\n\nAverage Pooling 1D.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nkernel_size\n: \nint\n or \nlist of int\n. Pooling kernel size.\n\n\nstrides\n: \nint\n or \nlist of int\n. Strides of conv operation.\n\n\nDefault\n: SAME as kernel_size.\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nname\n: A name for this layer (optional). Default: 'AvgPool1D'.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nHighwayConv1d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nHighwayConv1d\n(\nmode\n,\n \nnum_filter\n,\n \nfilter_size\n,\n \nstrides\n=\n1\n,\n \npadding\n=\nSAME\n,\n \nactivation\n=\nlinear\n,\n \nweights_init\n=\nuniform_scaling\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nNone\n,\n \nscale\n=\n0.001\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nHighwayConv1D\n)\n\n\n\n\n\n\nAdds a Highway Convolution 1D.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_filter\n: \nint\n. The number of convolutional filters.\n\n\nfilter_size\n: 'int\nor\nlist of int`. Size of filters.\n\n\nstrides\n: 'int\nor\nlist of int`. Strides of conv operation.\n\n\nDefault\n: [1 1 1 1].\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'truncated_normal'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'HighwayConv1D'.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nw\n: \nVariable\n. Variable representing filter weights.\n\n\nw_t\n: \nVariable\n. Variable representing gate weights.\n\n\nb\n: \nVariable\n. Variable representing biases.\n\n\nb_t\n: \nVariable\n. Variable representing gate biases.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nConv3d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nConv3d\n(\nmode\n,\n \nnum_filter\n,\n \nfilter_size\n,\n \nstrides\n=\n1\n,\n \npadding\n=\nSAME\n,\n \nactivation\n=\nlinear\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nuniform_scaling\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nNone\n,\n \nscale\n=\n0.001\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nConv3D\n)\n\n\n\n\n\n\nAdds Convolution 3D.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_filter\n: \nint\n. The number of convolutional filters.\n\n\nfilter_size\n: \nint\n or \nlist of int\n. Size of filters.\n\n\nstrides\n: 'int\nor list of\nint`. Strides of conv operation.\n\n\nDefault\n: [1 1 1 1 1]. Must have strides[0] = strides[4] = 1.\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'truncated_normal'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'Conv3D'.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nw\n: \nVariable\n. Variable representing filter weights.\n\n\nb\n: \nVariable\n. Variable representing biases.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nConv3dTranspose\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nConv3dTranspose\n(\nmode\n,\n \nnum_filter\n,\n \nfilter_size\n,\n \noutput_shape\n,\n \nstrides\n=\n1\n,\n \npadding\n=\nSAME\n,\n \nactivation\n=\nlinear\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nuniform_scaling\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nNone\n,\n \nscale\n=\n0.001\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nConv3DTranspose\n)\n\n\n\n\n\n\nAdds Convolution 3D Transpose.\n\n\nThis operation is sometimes called \"deconvolution\" after (Deconvolutional\n- \nNetworks)[http\n://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf], but is\nactually the transpose (gradient) of \nconv3d\n rather than an actual\ndeconvolution.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_filter\n: \nint\n. The number of convolutional filters.\n\n\nfilter_size\n: \nint\n or \nlist of int\n. Size of filters.\n\n\noutput_shape\n: \nlist of int\n. Dimensions of the output tensor.\n    Can optionally include the number of conv filters.\n    [new depth, new height, new width, num_filter]\n    or [new depth, new height, new width].\n\n\nstrides\n: \nint\n or list of \nint\n. Strides of conv operation.\n\n\nDefault\n: [1 1 1 1 1].\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'truncated_normal'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'Conv2DTranspose'.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nw\n: \nVariable\n. Variable representing filter weights.\n\n\nb\n: \nVariable\n. Variable representing biases.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nMaxPool3d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nMaxPool3d\n(\nmode\n,\n \nkernel_size\n,\n \nstrides\n=\n1\n,\n \npadding\n=\nSAME\n,\n \nname\n=\nMaxPool3D\n)\n\n\n\n\n\n\nMax Pooling 3D.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nkernel_size\n: 'int\nor\nlist of int`. Pooling kernel size.\n    Must have kernel_size[0] = kernel_size[1] = 1\n\n\nstrides\n: 'int\nor\nlist of int`. Strides of conv operation.\n    Must have strides[0] = strides[4] = 1.\n\n\nDefault\n: [1 1 1 1 1]\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nname\n: A name for this layer (optional). Default: 'MaxPool3D'.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nAvgPool3d\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nAvgPool3d\n(\nmode\n,\n \nkernel_size\n,\n \nstrides\n=\nNone\n,\n \npadding\n=\nSAME\n,\n \nname\n=\nAvgPool3D\n)\n\n\n\n\n\n\nAverage Pooling 3D.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nkernel_size\n: 'int\nor\nlist of int`. Pooling kernel size.\n    Must have kernel_size[0] = kernel_size[1] = 1\n\n\nstrides\n: 'int\nor\nlist of int`. Strides of conv operation.\n    Must have strides[0] = strides[4] = 1.\n\n\nDefault\n: [1 1 1 1 1]\n\n\n\n\n\n\npadding\n: \nstr\n from \n\"SAME\", \"VALID\"\n. Padding algo to use.\n\n\nDefault\n: 'SAME'.\n\n\n\n\n\n\nname\n: A name for this layer (optional). Default: 'AvgPool3D'.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nGlobalMaxPool\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nGlobalMaxPool\n(\nmode\n,\n \nname\n=\nGlobalMaxPool\n)\n\n\n\n\n\n\nAdds a Global Max Pooling.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nname\n: A name for this layer (optional). Default: 'GlobalMaxPool'.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nGlobalAvgPool\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nGlobalAvgPool\n(\nmode\n,\n \nname\n=\nGlobalAvgPool\n)\n\n\n\n\n\n\nAdds a Global Average Pooling.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nname\n: A name for this layer (optional). Default: 'GlobalAvgPool'.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nResidualBlock\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nResidualBlock\n(\nmode\n,\n \nnum_blocks\n,\n \nout_channels\n,\n \ndownsample\n=\nFalse\n,\n \ndownsample_strides\n=\n2\n,\n \nactivation\n=\nrelu\n,\n \nbatch_norm\n=\nTrue\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nvariance_scaling\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nl2_regularizer\n,\n \nscale\n=\n0.0001\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nResidualBlock\n)\n\n\n\n\n\n\nAdds a Residual Block.\n\n\nA residual block as described in MSRA's Deep Residual Network paper.\nFull pre-activation architecture is used here.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_blocks\n: \nint\n. Number of layer blocks.\n\n\nout_channels\n: \nint\n. The number of convolutional filters of the\n    convolution layers.\n\n\ndownsample\n: \nbool\n. If True, apply downsampling using\n    'downsample_strides' for strides.\n\n\ndownsample_strides\n: \nint\n. The strides to use when downsampling.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\nbatch_norm\n: \nbool\n. If True, apply batch normalization.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'uniform_scaling'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \ntf.Tensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'ShallowBottleneck'.\n\n\n\n\n\n\n\n\nReferences\n:\n\n\n\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu\n    Zhang, Shaoqing Ren, Jian Sun. 2015.\n\n\nIdentity Mappings in Deep Residual Networks. Kaiming He, Xiangyu\n    Zhang, Shaoqing Ren, Jian Sun. 2015.\n\n\n\n\n\n\n\n\nLinks\n:\n\n\n\n\n[http://arxiv.org/pdf/1512.03385v1.pdf]\n    (http://arxiv.org/pdf/1512.03385v1.pdf)\n\n\n[Identity Mappings in Deep Residual Networks]\n    (https://arxiv.org/pdf/1603.05027v2.pdf)\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nResidualBottleneck\n\n\npolyaxon\n.\nlayers\n.\nconvolutional\n.\nResidualBottleneck\n(\nmode\n,\n \nnum_blocks\n,\n \nbottleneck_size\n,\n \nout_channels\n,\n \ndownsample\n=\nFalse\n,\n \ndownsample_strides\n=\n2\n,\n \nactivation\n=\nrelu\n,\n \nbatch_norm\n=\nTrue\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nvariance_scaling\n,\n \nbias_init\n=\nzeros\n,\n \nregularizer\n=\nl2_regularizer\n,\n \nscale\n=\n0.0001\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nResidualBottleneck\n)\n\n\n\n\n\n\nAdds a Residual Bottleneck.\n\n\nA residual bottleneck block as described in MSRA's Deep Residual Network\npaper. Full pre-activation architecture is used here.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_blocks\n: \nint\n. Number of layer blocks.\n\n\nbottleneck_size\n: \nint\n. The number of convolutional filter of the\n    bottleneck convolutional layer.\n\n\nout_channels\n: \nint\n. The number of convolutional filters of the\n    layers surrounding the bottleneck layer.\n\n\ndownsample\n: \nbool\n. If True, apply downsampling using\n    'downsample_strides' for strides.\n\n\ndownsample_strides\n: \nint\n. The strides to use when downsampling.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n\n\nDefault\n: 'linear'.\n\n\n\n\n\n\nbatch_norm\n: \nbool\n. If True, apply batch normalization.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'uniform_scaling'.\n\n\n\n\n\n\nbias_init\n: \nstr\n (name) or \ntf.Tensor\n. Bias initialization.\n\n\nDefault\n: 'zeros'.\n\n\n\n\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this layer weights.\n\n\nDefault\n: None.\n\n\n\n\n\n\nscale\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'DeepBottleneck'.\n\n\n\n\n\n\n\n\nReferences\n:\n\n\n\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu\n    Zhang, Shaoqing Ren, Jian Sun. 2015.\n\n\nIdentity Mappings in Deep Residual Networks. Kaiming He, Xiangyu\n    Zhang, Shaoqing Ren, Jian Sun. 2015.\n\n\n\n\n\n\n\n\nLinks\n:\n\n\n\n\n[http://arxiv.org/pdf/1512.03385v1.pdf]\n    (http://arxiv.org/pdf/1512.03385v1.pdf)\n\n\n[Identity Mappings in Deep Residual Networks]\n    (https://arxiv.org/pdf/1603.05027v2.pdf)", 
            "title": "Convolutional Layers"
        }, 
        {
            "location": "/layers/convolutional/#conv2d", 
            "text": "polyaxon . layers . convolutional . Conv2d ( mode ,   num_filter ,   filter_size ,   strides = 1 ,   padding = SAME ,   activation = linear ,   bias = True ,   weights_init = uniform_scaling ,   bias_init = zeros ,   regularizer = None ,   scale = 0.001 ,   trainable = True ,   restore = True ,   name = Conv2D )   Adds a 2D convolution layer.  This operation creates a variable called 'w', representing the convolutional kernel,\nthat is convolved with the input. A second variable called 'b' is added to the result of\nthe convolution operation.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_filter :  int . The number of convolutional filters.  filter_size :  int  or  list of int . Size of filters.  strides : 'int or list of int`. Strides of conv operation.  Default : [1 1 1 1].    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    activation :  str  (name) or  function  (returning a  Tensor ) or None.  Default : 'linear'.    bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'truncated_normal'.    bias_init :  str  (name) or  Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'Conv2D'.     Attributes :   w :  Variable . Variable representing filter weights.  b :  Variable . Variable representing biases.      [source]", 
            "title": "Conv2d"
        }, 
        {
            "location": "/layers/convolutional/#conv2dtranspose", 
            "text": "polyaxon . layers . convolutional . Conv2dTranspose ( mode ,   num_filter ,   filter_size ,   output_shape ,   strides = 1 ,   padding = SAME ,   activation = linear ,   bias = True ,   weights_init = uniform_scaling ,   bias_init = zeros ,   regularizer = None ,   scale = 0.001 ,   trainable = True ,   restore = True ,   name = Conv2DTranspose )   Adds a Convolution 2D Transpose.  This operation is sometimes called \"deconvolution\" after (Deconvolutional\n-  Networks)[http ://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf], but is\nactually the transpose (gradient) of  conv2d  rather than an actual\ndeconvolution.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_filter :  int . The number of convolutional filters.  filter_size :  int  or  list of int . Size of filters.  output_shape :  list of int . Dimensions of the output tensor.\n    Can optionally include the number of conv filters.\n    [new height, new width, num_filter] or [new height, new width].  strides :  int  or list of  int . Strides of conv operation.  Default : [1 1 1 1].    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    activation :  str  (name) or  function  (returning a  Tensor ).  Default : 'linear'.    bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'truncated_normal'.    bias_init :  str  (name) or  Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'Conv2DTranspose'.     Attributes :   w :  Variable . Variable representing filter weights.  b :  Variable . Variable representing biases.      [source]", 
            "title": "Conv2dTranspose"
        }, 
        {
            "location": "/layers/convolutional/#maxpool2d", 
            "text": "polyaxon . layers . convolutional . MaxPool2d ( mode ,   kernel_size ,   strides = None ,   padding = SAME ,   name = MaxPool2D )   Adds Max Pooling 2D.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  kernel_size : 'int or list of int`. Pooling kernel size.  strides : 'int or list of int`. Strides of conv operation.  Default : SAME as kernel_size.    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    name : A name for this layer (optional). Default: 'MaxPool2D'.      [source]", 
            "title": "MaxPool2d"
        }, 
        {
            "location": "/layers/convolutional/#avgpool2d", 
            "text": "polyaxon . layers . convolutional . AvgPool2d ( mode ,   kernel_size ,   strides = None ,   padding = SAME ,   name = AvgPool2D )   Adds Average Pooling 2D.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  kernel_size : 'int or list of int`. Pooling kernel size.  strides : 'int or list of int`. Strides of conv operation.  Default : SAME as kernel_size.    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    name : A name for this layer (optional). Default: 'AvgPool2D'.      [source]", 
            "title": "AvgPool2d"
        }, 
        {
            "location": "/layers/convolutional/#upsample2d", 
            "text": "polyaxon . layers . convolutional . Upsample2d ( mode ,   kernel_size ,   name = UpSample2D )   Adds UpSample 2D operation.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  kernel_size : 'int or list of int`. Upsampling kernel size.  name : A name for this layer (optional). Default: 'UpSample2D'.      [source]", 
            "title": "Upsample2d"
        }, 
        {
            "location": "/layers/convolutional/#highwayconv2d", 
            "text": "polyaxon . layers . convolutional . HighwayConv2d ( mode ,   num_filter ,   filter_size ,   strides = 1 ,   padding = SAME ,   activation = linear ,   weights_init = uniform_scaling ,   bias_init = zeros ,   regularizer = None ,   scale = 0.001 ,   trainable = True ,   restore = True ,   name = HighwayConv2D )   Adds a Highway Convolution 2D.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_filter :  int . The number of convolutional filters.  filter_size : 'int or list of int`. Size of filters.  strides : 'int or list of int`. Strides of conv operation.  Default : [1 1 1 1].    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    activation :  str  (name) or  function  (returning a  Tensor ).  Default : 'linear'.    weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'truncated_normal'.    bias_init :  str  (name) or  Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'Conv2D'.     Attributes :   w :  Variable . Variable representing filter weights.  w_t :  Variable . Variable representing gate weights.  b :  Variable . Variable representing biases.  b_t :  Variable . Variable representing gate biases.      [source]", 
            "title": "HighwayConv2d"
        }, 
        {
            "location": "/layers/convolutional/#upscore", 
            "text": "polyaxon . layers . convolutional . Upscore ( mode ,   num_classes ,   shape = None ,   kernel_size = 4 ,   strides = 2 ,   trainable = True ,   restore = True ,   name = Upscore )   Adds an Upscore layer.  This implements the upscore layer as used in\n(Fully Convolutional Networks)[http://arxiv.org/abs/1411.4038].\nThe upscore layer is initialized as bilinear upsampling filter.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_classes :  int . Number of output feature maps.  shape :  list of int . Dimension of the output map\n    [batch_size, new height, new width]. For convinience four values\n     are allows [batch_size, new height, new width, X], where X\n     is ignored.  kernel_size : 'int or list of int`. Upsampling kernel size.  strides : 'int or list of int`. Strides of conv operation.  Default : [1 2 2 1].    trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'Upscore'.     Links :\n    (Fully Convolutional Networks)[http://arxiv.org/abs/1411.4038]     [source]", 
            "title": "Upscore"
        }, 
        {
            "location": "/layers/convolutional/#conv1d", 
            "text": "polyaxon . layers . convolutional . Conv1d ( mode ,   num_filter ,   filter_size ,   strides = 1 ,   padding = SAME ,   activation = linear ,   bias = True ,   weights_init = uniform_scaling ,   bias_init = zeros ,   regularizer = None ,   scale = 0.001 ,   trainable = True ,   restore = True ,   name = Conv1D )   Adds a Convolution 1D.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_filter :  int . The number of convolutional filters.  filter_size : 'int or list of int`. Size of filters.  strides : 'int or list of int`. Strides of conv operation.  Default : [1 1 1 1].    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    activation :  str  (name) or  function  (returning a  Tensor ).  Default : 'linear'.    bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'truncated_normal'.    bias_init :  str  (name) or  Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'Conv1D'.     Attributes :   w :  Variable . Variable representing filter weights.  b :  Variable . Variable representing biases.      [source]", 
            "title": "Conv1d"
        }, 
        {
            "location": "/layers/convolutional/#maxpool1d", 
            "text": "polyaxon . layers . convolutional . MaxPool1d ( mode ,   kernel_size ,   strides = None ,   padding = SAME ,   name = MaxPool1D )   Adds Max Pooling 1D.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  kernel_size :  int  or  list of int . Pooling kernel size.  strides :  int  or  list of int . Strides of conv operation.  Default : SAME as kernel_size.    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    name : A name for this layer (optional). Default: 'MaxPool1D'.      [source]", 
            "title": "MaxPool1d"
        }, 
        {
            "location": "/layers/convolutional/#avgpool1d", 
            "text": "polyaxon . layers . convolutional . AvgPool1d ( mode ,   kernel_size ,   strides = None ,   padding = SAME ,   name = AvgPool1D )   Average Pooling 1D.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  kernel_size :  int  or  list of int . Pooling kernel size.  strides :  int  or  list of int . Strides of conv operation.  Default : SAME as kernel_size.    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    name : A name for this layer (optional). Default: 'AvgPool1D'.      [source]", 
            "title": "AvgPool1d"
        }, 
        {
            "location": "/layers/convolutional/#highwayconv1d", 
            "text": "polyaxon . layers . convolutional . HighwayConv1d ( mode ,   num_filter ,   filter_size ,   strides = 1 ,   padding = SAME ,   activation = linear ,   weights_init = uniform_scaling ,   bias_init = zeros ,   regularizer = None ,   scale = 0.001 ,   trainable = True ,   restore = True ,   name = HighwayConv1D )   Adds a Highway Convolution 1D.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_filter :  int . The number of convolutional filters.  filter_size : 'int or list of int`. Size of filters.  strides : 'int or list of int`. Strides of conv operation.  Default : [1 1 1 1].    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    activation :  str  (name) or  function  (returning a  Tensor ).  Default : 'linear'.    weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'truncated_normal'.    bias_init :  str  (name) or  Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'HighwayConv1D'.     Attributes :   w :  Variable . Variable representing filter weights.  w_t :  Variable . Variable representing gate weights.  b :  Variable . Variable representing biases.  b_t :  Variable . Variable representing gate biases.      [source]", 
            "title": "HighwayConv1d"
        }, 
        {
            "location": "/layers/convolutional/#conv3d", 
            "text": "polyaxon . layers . convolutional . Conv3d ( mode ,   num_filter ,   filter_size ,   strides = 1 ,   padding = SAME ,   activation = linear ,   bias = True ,   weights_init = uniform_scaling ,   bias_init = zeros ,   regularizer = None ,   scale = 0.001 ,   trainable = True ,   restore = True ,   name = Conv3D )   Adds Convolution 3D.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_filter :  int . The number of convolutional filters.  filter_size :  int  or  list of int . Size of filters.  strides : 'int or list of int`. Strides of conv operation.  Default : [1 1 1 1 1]. Must have strides[0] = strides[4] = 1.    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    activation :  str  (name) or  function  (returning a  Tensor ).  Default : 'linear'.    bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'truncated_normal'.    bias_init :  str  (name) or  Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'Conv3D'.     Attributes :   w :  Variable . Variable representing filter weights.  b :  Variable . Variable representing biases.      [source]", 
            "title": "Conv3d"
        }, 
        {
            "location": "/layers/convolutional/#conv3dtranspose", 
            "text": "polyaxon . layers . convolutional . Conv3dTranspose ( mode ,   num_filter ,   filter_size ,   output_shape ,   strides = 1 ,   padding = SAME ,   activation = linear ,   bias = True ,   weights_init = uniform_scaling ,   bias_init = zeros ,   regularizer = None ,   scale = 0.001 ,   trainable = True ,   restore = True ,   name = Conv3DTranspose )   Adds Convolution 3D Transpose.  This operation is sometimes called \"deconvolution\" after (Deconvolutional\n-  Networks)[http ://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf], but is\nactually the transpose (gradient) of  conv3d  rather than an actual\ndeconvolution.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_filter :  int . The number of convolutional filters.  filter_size :  int  or  list of int . Size of filters.  output_shape :  list of int . Dimensions of the output tensor.\n    Can optionally include the number of conv filters.\n    [new depth, new height, new width, num_filter]\n    or [new depth, new height, new width].  strides :  int  or list of  int . Strides of conv operation.  Default : [1 1 1 1 1].    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    activation :  str  (name) or  function  (returning a  Tensor ).  Default : 'linear'.    bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'truncated_normal'.    bias_init :  str  (name) or  Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'Conv2DTranspose'.     Attributes :   w :  Variable . Variable representing filter weights.  b :  Variable . Variable representing biases.      [source]", 
            "title": "Conv3dTranspose"
        }, 
        {
            "location": "/layers/convolutional/#maxpool3d", 
            "text": "polyaxon . layers . convolutional . MaxPool3d ( mode ,   kernel_size ,   strides = 1 ,   padding = SAME ,   name = MaxPool3D )   Max Pooling 3D.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  kernel_size : 'int or list of int`. Pooling kernel size.\n    Must have kernel_size[0] = kernel_size[1] = 1  strides : 'int or list of int`. Strides of conv operation.\n    Must have strides[0] = strides[4] = 1.  Default : [1 1 1 1 1]    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    name : A name for this layer (optional). Default: 'MaxPool3D'.      [source]", 
            "title": "MaxPool3d"
        }, 
        {
            "location": "/layers/convolutional/#avgpool3d", 
            "text": "polyaxon . layers . convolutional . AvgPool3d ( mode ,   kernel_size ,   strides = None ,   padding = SAME ,   name = AvgPool3D )   Average Pooling 3D.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  kernel_size : 'int or list of int`. Pooling kernel size.\n    Must have kernel_size[0] = kernel_size[1] = 1  strides : 'int or list of int`. Strides of conv operation.\n    Must have strides[0] = strides[4] = 1.  Default : [1 1 1 1 1]    padding :  str  from  \"SAME\", \"VALID\" . Padding algo to use.  Default : 'SAME'.    name : A name for this layer (optional). Default: 'AvgPool3D'.      [source]", 
            "title": "AvgPool3d"
        }, 
        {
            "location": "/layers/convolutional/#globalmaxpool", 
            "text": "polyaxon . layers . convolutional . GlobalMaxPool ( mode ,   name = GlobalMaxPool )   Adds a Global Max Pooling.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  name : A name for this layer (optional). Default: 'GlobalMaxPool'.      [source]", 
            "title": "GlobalMaxPool"
        }, 
        {
            "location": "/layers/convolutional/#globalavgpool", 
            "text": "polyaxon . layers . convolutional . GlobalAvgPool ( mode ,   name = GlobalAvgPool )   Adds a Global Average Pooling.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  name : A name for this layer (optional). Default: 'GlobalAvgPool'.      [source]", 
            "title": "GlobalAvgPool"
        }, 
        {
            "location": "/layers/convolutional/#residualblock", 
            "text": "polyaxon . layers . convolutional . ResidualBlock ( mode ,   num_blocks ,   out_channels ,   downsample = False ,   downsample_strides = 2 ,   activation = relu ,   batch_norm = True ,   bias = True ,   weights_init = variance_scaling ,   bias_init = zeros ,   regularizer = l2_regularizer ,   scale = 0.0001 ,   trainable = True ,   restore = True ,   name = ResidualBlock )   Adds a Residual Block.  A residual block as described in MSRA's Deep Residual Network paper.\nFull pre-activation architecture is used here.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_blocks :  int . Number of layer blocks.  out_channels :  int . The number of convolutional filters of the\n    convolution layers.  downsample :  bool . If True, apply downsampling using\n    'downsample_strides' for strides.  downsample_strides :  int . The strides to use when downsampling.  activation :  str  (name) or  function  (returning a  Tensor ).  Default : 'linear'.    batch_norm :  bool . If True, apply batch normalization.  bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'uniform_scaling'.    bias_init :  str  (name) or  tf.Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'ShallowBottleneck'.     References :   Deep Residual Learning for Image Recognition. Kaiming He, Xiangyu\n    Zhang, Shaoqing Ren, Jian Sun. 2015.  Identity Mappings in Deep Residual Networks. Kaiming He, Xiangyu\n    Zhang, Shaoqing Ren, Jian Sun. 2015.     Links :   [http://arxiv.org/pdf/1512.03385v1.pdf]\n    (http://arxiv.org/pdf/1512.03385v1.pdf)  [Identity Mappings in Deep Residual Networks]\n    (https://arxiv.org/pdf/1603.05027v2.pdf)      [source]", 
            "title": "ResidualBlock"
        }, 
        {
            "location": "/layers/convolutional/#residualbottleneck", 
            "text": "polyaxon . layers . convolutional . ResidualBottleneck ( mode ,   num_blocks ,   bottleneck_size ,   out_channels ,   downsample = False ,   downsample_strides = 2 ,   activation = relu ,   batch_norm = True ,   bias = True ,   weights_init = variance_scaling ,   bias_init = zeros ,   regularizer = l2_regularizer ,   scale = 0.0001 ,   trainable = True ,   restore = True ,   name = ResidualBottleneck )   Adds a Residual Bottleneck.  A residual bottleneck block as described in MSRA's Deep Residual Network\npaper. Full pre-activation architecture is used here.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_blocks :  int . Number of layer blocks.  bottleneck_size :  int . The number of convolutional filter of the\n    bottleneck convolutional layer.  out_channels :  int . The number of convolutional filters of the\n    layers surrounding the bottleneck layer.  downsample :  bool . If True, apply downsampling using\n    'downsample_strides' for strides.  downsample_strides :  int . The strides to use when downsampling.  activation :  str  (name) or  function  (returning a  Tensor ).  Default : 'linear'.    batch_norm :  bool . If True, apply batch normalization.  bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'uniform_scaling'.    bias_init :  str  (name) or  tf.Tensor . Bias initialization.  Default : 'zeros'.    regularizer :  str  (name) or  Tensor . Add a regularizer to this layer weights.  Default : None.    scale :  float . Regularizer decay parameter. Default: 0.001.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'DeepBottleneck'.     References :   Deep Residual Learning for Image Recognition. Kaiming He, Xiangyu\n    Zhang, Shaoqing Ren, Jian Sun. 2015.  Identity Mappings in Deep Residual Networks. Kaiming He, Xiangyu\n    Zhang, Shaoqing Ren, Jian Sun. 2015.     Links :   [http://arxiv.org/pdf/1512.03385v1.pdf]\n    (http://arxiv.org/pdf/1512.03385v1.pdf)  [Identity Mappings in Deep Residual Networks]\n    (https://arxiv.org/pdf/1603.05027v2.pdf)", 
            "title": "ResidualBottleneck"
        }, 
        {
            "location": "/layers/recurrent/", 
            "text": "[source]\n\n\nSimpleRNN\n\n\npolyaxon\n.\nlayers\n.\nrecurrent\n.\nSimpleRNN\n(\nmode\n,\n \nnum_units\n,\n \nactivation\n=\nsigmoid\n,\n \ndropout\n=\nNone\n,\n \nnum_layers\n=\n1\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nNone\n,\n \nreturn_seq\n=\nFalse\n,\n \nreturn_state\n=\nFalse\n,\n \ninitial_state\n=\nNone\n,\n \ndynamic\n=\nFalse\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nSimpleRNN\n)\n\n\n\n\n\n\nSimple RNN (Simple Recurrent Layer.)\n\n\n\n\n\n\nOutput\n:\n    if \nreturn_seq\n: 3-D Tensor [samples, timesteps, output dim].\n\n\n\n\nelse\n: 2-D Tensor [samples, output dim].\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n). Default: 'sigmoid'.\n\n\ndropout\n: \ntuple\n of \nfloat\n: (1 - input_keep_prob, 1 - output_keep_prob). The\n    input and output keep probability.\n\n\nnum_layers\n: \nint\n how many times to stack the cell.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nreturn_seq\n: \nbool\n. If True, returns the full sequence instead of\n    last sequence output only.\n\n\nreturn_state\n: \nbool\n. If True, returns a tuple with output and\n\n\nstates\n: (output, states).\n\n\n\n\n\n\ninitial_state\n: \nTensor\n. An initial state for the RNN.  This must be\n    a tensor of appropriate type and shape [batch_size x cell.state_size].\n\n\ndynamic\n: \nbool\n. If True, dynamic computation is performed. It will not\n    compute RNN steps above the sequence length. Note that because TF\n    requires to feed sequences of same length, 0 is used as a mask.\n    So a sequence padded with 0 at the end must be provided. When\n    computation is performed, it will stop when it meets a step with\n    a value of 0.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when loading a model.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nLSTM\n\n\npolyaxon\n.\nlayers\n.\nrecurrent\n.\nLSTM\n(\nmode\n,\n \nnum_units\n,\n \nactivation\n=\ntanh\n,\n \ninner_activation\n=\nsigmoid\n,\n \ndropout\n=\nNone\n,\n \nnum_layers\n=\n1\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nNone\n,\n \nforget_bias\n=\n1.0\n,\n \nreturn_seq\n=\nFalse\n,\n \nreturn_state\n=\nFalse\n,\n \ninitial_state\n=\nNone\n,\n \ndynamic\n=\nFalse\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nLSTM\n)\n\n\n\n\n\n\nLSTM (Long Short Term Memory Recurrent Layer).\n\n\n\n\n\n\nOutput\n:\n    if \nreturn_seq\n: 3-D Tensor [samples, timesteps, output dim].\n\n\n\n\nelse\n: 2-D Tensor [samples, output dim].\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n). Default: 'tanh'.\n\n\ninner_activation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n    LSTM inner activation. Default: 'sigmoid'.\n\n\ndropout\n: \ntuple\n of \nfloat\n: (1 - input_keep_prob, 1 - output_keep_prob). The\n    input and output keep probability.\n\n\nnum_layers\n: \nint\n how many times to stack the cell.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nforget_bias\n: \nfloat\n. Bias of the forget gate. Default: 1.0.\n\n\nreturn_seq\n: \nbool\n. If True, returns the full sequence instead of\n    last sequence output only.\n\n\nreturn_state\n: \nbool\n. If True, returns a tuple with output and\n\n\nstates\n: (output, states).\n\n\n\n\n\n\ninitial_state\n: \nTensor\n. An initial state for the RNN.  This must be\n    a tensor of appropriate type and shape [batch_size x cell.state_size].\n\n\ndynamic\n: \nbool\n. If True, dynamic computation is performed. It will not\n    compute RNN steps above the sequence length. Note that because TF\n    requires to feed sequences of same length, 0 is used as a mask.\n    So a sequence padded with 0 at the end must be provided. When\n    computation is performed, it will stop when it meets a step with\n    a value of 0.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when loading a model.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\n\n\n\n\nReferences\n:\n    Long Short Term Memory, Sepp Hochreiter \n Jurgen Schmidhuber,\n    Neural Computation 9(8): 1735-1780, 1997.\n\n\n\n\n\n\nLinks\n:\n\n\n\n\n[http\n://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf]\n(http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nGRU\n\n\npolyaxon\n.\nlayers\n.\nrecurrent\n.\nGRU\n(\nmode\n,\n \nnum_units\n,\n \nactivation\n=\ntanh\n,\n \ninner_activation\n=\nsigmoid\n,\n \ndropout\n=\nNone\n,\n \nnum_layers\n=\n1\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nNone\n,\n \nreturn_seq\n=\nFalse\n,\n \nreturn_state\n=\nFalse\n,\n \ninitial_state\n=\nNone\n,\n \ndynamic\n=\nFalse\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nGRU\n)\n\n\n\n\n\n\nGRU (Gated Recurrent Unit Layer).\n\n\n\n\n\n\nOutput\n:\n    if \nreturn_seq\n: 3-D Tensor [samples, timesteps, output dim].\n\n\n\n\nelse\n: 2-D Tensor [samples, output dim].\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n). Default: 'tanh'.\n\n\ninner_activation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n    GRU inner activation. Default: 'sigmoid'.\n\n\ndropout\n: \ntuple\n of \nfloat\n: (1 - input_keep_prob, 1 - output_keep_prob). The\n    input and output keep probability.\n\n\nnum_layers\n: \nint\n how many times to stack the cell.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nreturn_seq\n: \nbool\n. If True, returns the full sequence instead of\n    last sequence output only.\n\n\nreturn_state\n: \nbool\n. If True, returns a tuple with output and\n\n\nstates\n: (output, states).\n\n\n\n\n\n\ninitial_state\n: \nTensor\n. An initial state for the RNN.  This must be\n    a tensor of appropriate type and shape [batch_size x cell.state_size].\n\n\ndynamic\n: \nbool\n. If True, dynamic computation is performed. It will not\n    compute RNN steps above the sequence length. Note that because TF\n    requires to feed sequences of same length, 0 is used as a mask.\n    So a sequence padded with 0 at the end must be provided. When\n    computation is performed, it will stop when it meets a step with\n    a value of 0.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when loading a model.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\n\n\n\n\nReferences\n:\n    Learning Phrase Representations using RNN Encoder\u2013Decoder for\n    Statistical Machine Translation, K. Cho et al., 2014.\n\n\n\n\n\n\nLinks\n:\n\n\n\n\n__\nhttp__://arxiv.org/abs/1406.1078\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nBidirectionalRNN\n\n\npolyaxon\n.\nlayers\n.\nrecurrent\n.\nBidirectionalRNN\n(\nmode\n,\n \nrnncell_fw\n,\n \nrnncell_bw\n,\n \nreturn_seq\n=\nFalse\n,\n \nreturn_states\n=\nFalse\n,\n \ninitial_state_fw\n=\nNone\n,\n \ninitial_state_bw\n=\nNone\n,\n \ndynamic\n=\nFalse\n,\n \nname\n=\nBiRNN\n)\n\n\n\n\n\n\nBidirectional RNN.\n\n\nBuild a bidirectional recurrent neural network, it requires 2 RNN Cells\nto process sequence in forward and backward order. Any RNN Cell can be\nused i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two\ncells number of units must match.\n\n\n\n\n\n\nOutput\n:\n    if \nreturn_seq\n: 3-D Tensor [samples, timesteps, output dim].\n\n\n\n\nelse\n: 2-D Tensor Layer [samples, output dim].\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nrnncell_fw\n: \nRNNCell\n. The RNN Cell to use for foward computation.\n\n\nrnncell_bw\n: \nRNNCell\n. The RNN Cell to use for backward computation.\n\n\nreturn_seq\n: \nbool\n. If True, returns the full sequence instead of\n    last sequence output only.\n\n\nreturn_states\n: \nbool\n. If True, returns a tuple with output and\n\n\nstates\n: (output, states).\n\n\n\n\n\n\ninitial_state_fw\n: \nTensor\n. An initial state for the forward RNN.\n    This must be a tensor of appropriate type and shape [batch_size\n    x cell.state_size].\n\n\ninitial_state_bw\n: \nTensor\n. An initial state for the backward RNN.\n    This must be a tensor of appropriate type and shape [batch_size\n    x cell.state_size].\n\n\ndynamic\n: \nbool\n. If True, dynamic computation is performed. It will not\n    compute RNN steps above the sequence length. Note that because TF\n    requires to feed sequences of same length, 0 is used as a mask.\n    So a sequence padded with 0 at the end must be provided. When\n    computation is performed, it will stop when it meets a step with\n    a value of 0.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nBasicRNNCell\n\n\npolyaxon\n.\nlayers\n.\nrecurrent\n.\nBasicRNNCell\n(\nmode\n,\n \nnum_units\n,\n \nactivation\n=\ntanh\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nNone\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nBasicRNNCell\n)\n\n\n\n\n\n\nThe most basic RNN cell with custom params.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n). Default: 'tanh'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when loading a model.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nGRUCell\n\n\npolyaxon\n.\nlayers\n.\nrecurrent\n.\nGRUCell\n(\nmode\n,\n \nnum_units\n,\n \nactivation\n=\ntanh\n,\n \ninner_activation\n=\nsigmoid\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nNone\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nGRUCell\n)\n\n\n\n\n\n\nGated Recurrent Unit cell with custom params.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n). Default: 'tanh'.\n\n\ninner_activation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n    GRU inner activation. Default: 'sigmoid'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when loading a model.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nBasicLSTMCell\n\n\npolyaxon\n.\nlayers\n.\nrecurrent\n.\nBasicLSTMCell\n(\nmode\n,\n \nnum_units\n,\n \nforget_bias\n=\n1.0\n,\n \nstate_is_tuple\n=\nTrue\n,\n \nactivation\n=\ntanh\n,\n \ninner_activation\n=\nsigmoid\n,\n \nbias\n=\nTrue\n,\n \nweights_init\n=\nNone\n,\n \nbatch_norm\n=\nFalse\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nBasicLSTMCell\n)\n\n\n\n\n\n\nBasic LSTM recurrent network cell with custo\\m params.\n\n\nThe implementation is based on: http://arxiv.org/abs/1409.2329.\n\n\nWe add forget_bias (default: 1) to the biases of the forget gate in order to\nreduce the scale of forgetting in the beginning of the training.\n\n\nIt does not allow cell clipping, a projection layer, and does not\nuse peep-hole connections: it is the basic baseline.\n\n\nFor advanced models, please use the full LSTMCell that follows.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nnum_units\n: \nint\n, number of units for this layer.\n\n\nforget_bias\n: \nfloat\n. Bias of the forget gate. Default: 1.0.\n\n\nstate_is_tuple\n: If True, accepted and returned states are n-tuples, where\n    \nn = len(cells)\n.  If False, the states are all\n    concatenated along the column axis.  This latter behavior will soon be\n    deprecated.\n\n\nactivation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n). Default: 'tanh'.\n\n\ninner_activation\n: \nstr\n (name) or \nfunction\n (returning a \nTensor\n).\n    GRU inner activation. Default: 'sigmoid'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nbatch_norm\n: \nbool\n. If True, use batch normalization for this cell.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when loading a model.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nDropoutWrapper\n\n\npolyaxon\n.\nlayers\n.\nrecurrent\n.\nDropoutWrapper\n(\nmode\n,\n \ncell\n,\n \ninput_keep_prob\n=\n1.0\n,\n \noutput_keep_prob\n=\n1.0\n,\n \nseed\n=\nNone\n,\n \nname\n=\nDropoutWrapper\n)\n\n\n\n\n\n\nOperator adding dropout to inputs and outputs of the given cell.\n\n\nCreates a cell with added input and/or output dropout.\n\n\nDropout is never used on the state.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ncell\n: an RNNCell, a projection to output_size is added to it.\n\n\ninput_keep_prob\n: unit Tensor or float between 0 and 1, input keep probability;\n    if it is float and 1, no input dropout will be added.\n\n\noutput_keep_prob\n: unit Tensor or float between 0 and 1, output keep\nprobability; if it is float and 1, no output dropout will be added.\n\n\nseed\n: (optional) integer, the randomness seed.\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nTypeError\n: if cell is not an RNNCell.\n\n\nValueError\n: if keep_prob is not between 0 and 1.\n\n\n\n\n\n\n[source]\n\n\nMultiRNNCell\n\n\npolyaxon\n.\nlayers\n.\nrecurrent\n.\nMultiRNNCell\n(\nmode\n,\n \ncells\n,\n \nstate_is_tuple\n=\nTrue\n,\n \nname\n=\nMultiRNNCell\n)\n\n\n\n\n\n\nRNN cell composed sequentially of multiple simple cells.\n\n\nCreate a RNN cell composed sequentially of a number of RNNCells.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ncells\n: list of RNNCells that will be composed in this order.\n\n\nstate_is_tuple\n: If True, accepted and returned states are n-tuples, where\n    \nn = len(cells)\n.  If False, the states are all\n    concatenated along the column axis.  This latter behavior will soon be\n    deprecated.\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if cells is empty (not allowed), or at least one of the cells\n    returns a state tuple but the flag \nstate_is_tuple\n is \nFalse\n.", 
            "title": "Recurrent Layers"
        }, 
        {
            "location": "/layers/recurrent/#simplernn", 
            "text": "polyaxon . layers . recurrent . SimpleRNN ( mode ,   num_units ,   activation = sigmoid ,   dropout = None ,   num_layers = 1 ,   bias = True ,   weights_init = None ,   return_seq = False ,   return_state = False ,   initial_state = None ,   dynamic = False ,   trainable = True ,   restore = True ,   name = SimpleRNN )   Simple RNN (Simple Recurrent Layer.)    Output :\n    if  return_seq : 3-D Tensor [samples, timesteps, output dim].   else : 2-D Tensor [samples, output dim].     Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_units :  int , number of units for this layer.  activation :  str  (name) or  function  (returning a  Tensor ). Default: 'sigmoid'.  dropout :  tuple  of  float : (1 - input_keep_prob, 1 - output_keep_prob). The\n    input and output keep probability.  num_layers :  int  how many times to stack the cell.  bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  return_seq :  bool . If True, returns the full sequence instead of\n    last sequence output only.  return_state :  bool . If True, returns a tuple with output and  states : (output, states).    initial_state :  Tensor . An initial state for the RNN.  This must be\n    a tensor of appropriate type and shape [batch_size x cell.state_size].  dynamic :  bool . If True, dynamic computation is performed. It will not\n    compute RNN steps above the sequence length. Note that because TF\n    requires to feed sequences of same length, 0 is used as a mask.\n    So a sequence padded with 0 at the end must be provided. When\n    computation is performed, it will stop when it meets a step with\n    a value of 0.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when loading a model.  name :  str . A name for this layer (optional).      [source]", 
            "title": "SimpleRNN"
        }, 
        {
            "location": "/layers/recurrent/#lstm", 
            "text": "polyaxon . layers . recurrent . LSTM ( mode ,   num_units ,   activation = tanh ,   inner_activation = sigmoid ,   dropout = None ,   num_layers = 1 ,   bias = True ,   weights_init = None ,   forget_bias = 1.0 ,   return_seq = False ,   return_state = False ,   initial_state = None ,   dynamic = False ,   trainable = True ,   restore = True ,   name = LSTM )   LSTM (Long Short Term Memory Recurrent Layer).    Output :\n    if  return_seq : 3-D Tensor [samples, timesteps, output dim].   else : 2-D Tensor [samples, output dim].     Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_units :  int , number of units for this layer.  activation :  str  (name) or  function  (returning a  Tensor ). Default: 'tanh'.  inner_activation :  str  (name) or  function  (returning a  Tensor ).\n    LSTM inner activation. Default: 'sigmoid'.  dropout :  tuple  of  float : (1 - input_keep_prob, 1 - output_keep_prob). The\n    input and output keep probability.  num_layers :  int  how many times to stack the cell.  bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  forget_bias :  float . Bias of the forget gate. Default: 1.0.  return_seq :  bool . If True, returns the full sequence instead of\n    last sequence output only.  return_state :  bool . If True, returns a tuple with output and  states : (output, states).    initial_state :  Tensor . An initial state for the RNN.  This must be\n    a tensor of appropriate type and shape [batch_size x cell.state_size].  dynamic :  bool . If True, dynamic computation is performed. It will not\n    compute RNN steps above the sequence length. Note that because TF\n    requires to feed sequences of same length, 0 is used as a mask.\n    So a sequence padded with 0 at the end must be provided. When\n    computation is performed, it will stop when it meets a step with\n    a value of 0.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when loading a model.  name :  str . A name for this layer (optional).     References :\n    Long Short Term Memory, Sepp Hochreiter   Jurgen Schmidhuber,\n    Neural Computation 9(8): 1735-1780, 1997.    Links :   [http ://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf]\n(http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)      [source]", 
            "title": "LSTM"
        }, 
        {
            "location": "/layers/recurrent/#gru", 
            "text": "polyaxon . layers . recurrent . GRU ( mode ,   num_units ,   activation = tanh ,   inner_activation = sigmoid ,   dropout = None ,   num_layers = 1 ,   bias = True ,   weights_init = None ,   return_seq = False ,   return_state = False ,   initial_state = None ,   dynamic = False ,   trainable = True ,   restore = True ,   name = GRU )   GRU (Gated Recurrent Unit Layer).    Output :\n    if  return_seq : 3-D Tensor [samples, timesteps, output dim].   else : 2-D Tensor [samples, output dim].     Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_units :  int , number of units for this layer.  activation :  str  (name) or  function  (returning a  Tensor ). Default: 'tanh'.  inner_activation :  str  (name) or  function  (returning a  Tensor ).\n    GRU inner activation. Default: 'sigmoid'.  dropout :  tuple  of  float : (1 - input_keep_prob, 1 - output_keep_prob). The\n    input and output keep probability.  num_layers :  int  how many times to stack the cell.  bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  return_seq :  bool . If True, returns the full sequence instead of\n    last sequence output only.  return_state :  bool . If True, returns a tuple with output and  states : (output, states).    initial_state :  Tensor . An initial state for the RNN.  This must be\n    a tensor of appropriate type and shape [batch_size x cell.state_size].  dynamic :  bool . If True, dynamic computation is performed. It will not\n    compute RNN steps above the sequence length. Note that because TF\n    requires to feed sequences of same length, 0 is used as a mask.\n    So a sequence padded with 0 at the end must be provided. When\n    computation is performed, it will stop when it meets a step with\n    a value of 0.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when loading a model.  name :  str . A name for this layer (optional).     References :\n    Learning Phrase Representations using RNN Encoder\u2013Decoder for\n    Statistical Machine Translation, K. Cho et al., 2014.    Links :   __ http__://arxiv.org/abs/1406.1078      [source]", 
            "title": "GRU"
        }, 
        {
            "location": "/layers/recurrent/#bidirectionalrnn", 
            "text": "polyaxon . layers . recurrent . BidirectionalRNN ( mode ,   rnncell_fw ,   rnncell_bw ,   return_seq = False ,   return_states = False ,   initial_state_fw = None ,   initial_state_bw = None ,   dynamic = False ,   name = BiRNN )   Bidirectional RNN.  Build a bidirectional recurrent neural network, it requires 2 RNN Cells\nto process sequence in forward and backward order. Any RNN Cell can be\nused i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two\ncells number of units must match.    Output :\n    if  return_seq : 3-D Tensor [samples, timesteps, output dim].   else : 2-D Tensor Layer [samples, output dim].     Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  rnncell_fw :  RNNCell . The RNN Cell to use for foward computation.  rnncell_bw :  RNNCell . The RNN Cell to use for backward computation.  return_seq :  bool . If True, returns the full sequence instead of\n    last sequence output only.  return_states :  bool . If True, returns a tuple with output and  states : (output, states).    initial_state_fw :  Tensor . An initial state for the forward RNN.\n    This must be a tensor of appropriate type and shape [batch_size\n    x cell.state_size].  initial_state_bw :  Tensor . An initial state for the backward RNN.\n    This must be a tensor of appropriate type and shape [batch_size\n    x cell.state_size].  dynamic :  bool . If True, dynamic computation is performed. It will not\n    compute RNN steps above the sequence length. Note that because TF\n    requires to feed sequences of same length, 0 is used as a mask.\n    So a sequence padded with 0 at the end must be provided. When\n    computation is performed, it will stop when it meets a step with\n    a value of 0.  name :  str . A name for this layer (optional).      [source]", 
            "title": "BidirectionalRNN"
        }, 
        {
            "location": "/layers/recurrent/#basicrnncell", 
            "text": "polyaxon . layers . recurrent . BasicRNNCell ( mode ,   num_units ,   activation = tanh ,   bias = True ,   weights_init = None ,   trainable = True ,   restore = True ,   name = BasicRNNCell )   The most basic RNN cell with custom params.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_units :  int , number of units for this layer.  activation :  str  (name) or  function  (returning a  Tensor ). Default: 'tanh'.  bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when loading a model.  name :  str . A name for this layer (optional).      [source]", 
            "title": "BasicRNNCell"
        }, 
        {
            "location": "/layers/recurrent/#grucell", 
            "text": "polyaxon . layers . recurrent . GRUCell ( mode ,   num_units ,   activation = tanh ,   inner_activation = sigmoid ,   bias = True ,   weights_init = None ,   trainable = True ,   restore = True ,   name = GRUCell )   Gated Recurrent Unit cell with custom params.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_units :  int , number of units for this layer.  activation :  str  (name) or  function  (returning a  Tensor ). Default: 'tanh'.  inner_activation :  str  (name) or  function  (returning a  Tensor ).\n    GRU inner activation. Default: 'sigmoid'.  bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when loading a model.  name :  str . A name for this layer (optional).      [source]", 
            "title": "GRUCell"
        }, 
        {
            "location": "/layers/recurrent/#basiclstmcell", 
            "text": "polyaxon . layers . recurrent . BasicLSTMCell ( mode ,   num_units ,   forget_bias = 1.0 ,   state_is_tuple = True ,   activation = tanh ,   inner_activation = sigmoid ,   bias = True ,   weights_init = None ,   batch_norm = False ,   trainable = True ,   restore = True ,   name = BasicLSTMCell )   Basic LSTM recurrent network cell with custo\\m params.  The implementation is based on: http://arxiv.org/abs/1409.2329.  We add forget_bias (default: 1) to the biases of the forget gate in order to\nreduce the scale of forgetting in the beginning of the training.  It does not allow cell clipping, a projection layer, and does not\nuse peep-hole connections: it is the basic baseline.  For advanced models, please use the full LSTMCell that follows.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  num_units :  int , number of units for this layer.  forget_bias :  float . Bias of the forget gate. Default: 1.0.  state_is_tuple : If True, accepted and returned states are n-tuples, where\n     n = len(cells) .  If False, the states are all\n    concatenated along the column axis.  This latter behavior will soon be\n    deprecated.  activation :  str  (name) or  function  (returning a  Tensor ). Default: 'tanh'.  inner_activation :  str  (name) or  function  (returning a  Tensor ).\n    GRU inner activation. Default: 'sigmoid'.  bias :  bool . If True, a bias is used.  weights_init :  str  (name) or  Tensor . Weights initialization.  batch_norm :  bool . If True, use batch normalization for this cell.  trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when loading a model.  name :  str . A name for this layer (optional).      [source]", 
            "title": "BasicLSTMCell"
        }, 
        {
            "location": "/layers/recurrent/#dropoutwrapper", 
            "text": "polyaxon . layers . recurrent . DropoutWrapper ( mode ,   cell ,   input_keep_prob = 1.0 ,   output_keep_prob = 1.0 ,   seed = None ,   name = DropoutWrapper )   Operator adding dropout to inputs and outputs of the given cell.  Creates a cell with added input and/or output dropout.  Dropout is never used on the state.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  cell : an RNNCell, a projection to output_size is added to it.  input_keep_prob : unit Tensor or float between 0 and 1, input keep probability;\n    if it is float and 1, no input dropout will be added.  output_keep_prob : unit Tensor or float between 0 and 1, output keep\nprobability; if it is float and 1, no output dropout will be added.  seed : (optional) integer, the randomness seed.     Raises :   TypeError : if cell is not an RNNCell.  ValueError : if keep_prob is not between 0 and 1.    [source]", 
            "title": "DropoutWrapper"
        }, 
        {
            "location": "/layers/recurrent/#multirnncell", 
            "text": "polyaxon . layers . recurrent . MultiRNNCell ( mode ,   cells ,   state_is_tuple = True ,   name = MultiRNNCell )   RNN cell composed sequentially of multiple simple cells.  Create a RNN cell composed sequentially of a number of RNNCells.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  cells : list of RNNCells that will be composed in this order.  state_is_tuple : If True, accepted and returned states are n-tuples, where\n     n = len(cells) .  If False, the states are all\n    concatenated along the column axis.  This latter behavior will soon be\n    deprecated.     Raises :   ValueError : if cells is empty (not allowed), or at least one of the cells\n    returns a state tuple but the flag  state_is_tuple  is  False .", 
            "title": "MultiRNNCell"
        }, 
        {
            "location": "/layers/embeddings/", 
            "text": "[source]\n\n\nEmbedding\n\n\npolyaxon\n.\nlayers\n.\nembedding\n.\nEmbedding\n(\nmode\n,\n \ninput_dim\n,\n \noutput_dim\n,\n \nvalidate_indices\n=\nFalse\n,\n \nweights_init\n=\ntruncated_normal\n,\n \ntrainable\n=\nTrue\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nEmbedding\n)\n\n\n\n\n\n\nEmbedding layer for a sequence of integer ids or floats.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\ninput_dim\n: list of \nint\n. Vocabulary size (number of ids).\n\n\noutput_dim\n: list of \nint\n. Embedding size.\n\n\nvalidate_indices\n: \nbool\n. Whether or not to validate gather indices.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n\n\nDefault\n: 'truncated_normal'.\n\n\n\n\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\n    loading a model.\n\n\nname\n: A name for this layer (optional). Default: 'Embedding'.", 
            "title": "Embedding Layers"
        }, 
        {
            "location": "/layers/embeddings/#embedding", 
            "text": "polyaxon . layers . embedding . Embedding ( mode ,   input_dim ,   output_dim ,   validate_indices = False ,   weights_init = truncated_normal ,   trainable = True ,   restore = True ,   name = Embedding )   Embedding layer for a sequence of integer ids or floats.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  input_dim : list of  int . Vocabulary size (number of ids).  output_dim : list of  int . Embedding size.  validate_indices :  bool . Whether or not to validate gather indices.  weights_init :  str  (name) or  Tensor . Weights initialization.  Default : 'truncated_normal'.    trainable :  bool . If True, weights will be trainable.  restore :  bool . If True, this layer weights will be restored when\n    loading a model.  name : A name for this layer (optional). Default: 'Embedding'.", 
            "title": "Embedding"
        }, 
        {
            "location": "/processing/categorical_vocabulary/", 
            "text": "[source]\n\n\nCategoricalVocabulary\n\n\npolyaxon\n.\nprocessing\n.\ncategorical\n.\nCategoricalVocabulary\n(\nunknown_token\n=\nUNK\n,\n \nsupport_reverse\n=\nTrue\n)\n\n\n\n\n\n\nCategorical variables vocabulary class.\n\n\nAccumulates and provides mapping from classes to indexes.\nCan be easily used for words.\n\n\n\n\nfreeze\n\n\nfreeze\n(\nself\n,\n \nfreeze\n=\nTrue\n)\n\n\n\n\n\n\nFreezes the vocabulary, after which new words return unknown token id.\n\n\n\n\nArgs\n:\n\n\nfreeze\n: True to freeze, False to unfreeze.\n\n\n\n\n\n\n\n\n\n\nget\n\n\nget\n(\nself\n,\n \ncategory\n)\n\n\n\n\n\n\nReturns word's id in the vocabulary.\n\n\nIf category is new, creates a new id for it.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ncategory\n: string or integer to lookup in vocabulary.\n\n\n\n\n\n\n\n\nReturns\n:\n    interger, id in the vocabulary.\n\n\n\n\n\n\n\n\nadd\n\n\nadd\n(\nself\n,\n \ncategory\n,\n \ncount\n=\n1\n)\n\n\n\n\n\n\nAdds count of the category to the frequency table.\n\n\n\n\nArgs\n:\n\n\ncategory\n: string or integer, category to add frequency to.\n\n\ncount\n: optional integer, how many to add.\n\n\n\n\n\n\n\n\n\n\ntrim\n\n\ntrim\n(\nself\n,\n \nmin_frequency\n,\n \nmax_frequency\n=-\n1\n)\n\n\n\n\n\n\nTrims vocabulary for minimum frequency.\n\n\nRemaps ids from 1..n in sort frequency order.\nwhere n - number of elements left.\n\n\n\n\nArgs\n:\n\n\nmin_frequency\n: minimum frequency to keep.\n\n\nmax_frequency\n: optional, maximum frequency to keep.\nUseful to remove very frequent categories (like stop words).\n\n\n\n\n\n\n\n\n\n\nreverse\n\n\nreverse\n(\nself\n,\n \nclass_id\n)\n\n\n\n\n\n\nGiven class id reverse to original class name.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nclass_id\n: Id of the class.\n\n\n\n\n\n\n\n\nReturns\n:\n    Class name.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if this vocabulary wasn't initialized with support_reverse.", 
            "title": "Categorical Vocabulary"
        }, 
        {
            "location": "/processing/categorical_vocabulary/#categoricalvocabulary", 
            "text": "polyaxon . processing . categorical . CategoricalVocabulary ( unknown_token = UNK ,   support_reverse = True )   Categorical variables vocabulary class.  Accumulates and provides mapping from classes to indexes.\nCan be easily used for words.", 
            "title": "CategoricalVocabulary"
        }, 
        {
            "location": "/processing/categorical_vocabulary/#freeze", 
            "text": "freeze ( self ,   freeze = True )   Freezes the vocabulary, after which new words return unknown token id.   Args :  freeze : True to freeze, False to unfreeze.", 
            "title": "freeze"
        }, 
        {
            "location": "/processing/categorical_vocabulary/#get", 
            "text": "get ( self ,   category )   Returns word's id in the vocabulary.  If category is new, creates a new id for it.    Args :   category : string or integer to lookup in vocabulary.     Returns :\n    interger, id in the vocabulary.", 
            "title": "get"
        }, 
        {
            "location": "/processing/categorical_vocabulary/#add", 
            "text": "add ( self ,   category ,   count = 1 )   Adds count of the category to the frequency table.   Args :  category : string or integer, category to add frequency to.  count : optional integer, how many to add.", 
            "title": "add"
        }, 
        {
            "location": "/processing/categorical_vocabulary/#trim", 
            "text": "trim ( self ,   min_frequency ,   max_frequency =- 1 )   Trims vocabulary for minimum frequency.  Remaps ids from 1..n in sort frequency order.\nwhere n - number of elements left.   Args :  min_frequency : minimum frequency to keep.  max_frequency : optional, maximum frequency to keep.\nUseful to remove very frequent categories (like stop words).", 
            "title": "trim"
        }, 
        {
            "location": "/processing/categorical_vocabulary/#reverse", 
            "text": "reverse ( self ,   class_id )   Given class id reverse to original class name.    Args :   class_id : Id of the class.     Returns :\n    Class name.    Raises :   ValueError : if this vocabulary wasn't initialized with support_reverse.", 
            "title": "reverse"
        }, 
        {
            "location": "/processing/vocabulary_processor/", 
            "text": "[source]\n\n\nVocabularyProcessor\n\n\npolyaxon\n.\nprocessing\n.\ntext\n.\nVocabularyProcessor\n(\nmax_document_length\n,\n \nmin_frequency\n=\n0\n,\n \nvocabulary\n=\nNone\n,\n \ntokenizer_fn\n=\nNone\n)\n\n\n\n\n\n\nA mirror to tf.contrib.learn VocabularyProcessor.\n\n\nMaps documents to sequences of word ids.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmax_document_length\n: Maximum length of documents.\n    if documents are longer, they will be trimmed, if shorter - padded.\n\n\nmin_frequency\n: Minimum frequency of words in the vocabulary.\n\n\nvocabulary\n: CategoricalVocabulary object.\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nvocabulary\n: CategoricalVocabulary object.\n\n\n\n\n\n\n\n\n\n\nfit\n\n\nfit\n(\nself\n,\n \nraw_documents\n,\n \nunused_y\n=\nNone\n)\n\n\n\n\n\n\nfit.\n\n\nLearn a vocabulary dictionary of all tokens in the raw documents.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nraw_documents\n: An iterable which yield either str or unicode.\n\n\nunused_y\n: to match fit format signature of estimators.\n\n\n\n\n\n\n\n\nReturns\n:\n    self\n\n\n\n\n\n\n\n\ntransform\n\n\ntransform\n(\nself\n,\n \nraw_documents\n)\n\n\n\n\n\n\ntransform.\n\n\nTransform documents to word-id matrix.\n\n\nConvert words to ids with vocabulary fitted with fit or the one\nprovided in the constructor.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nraw_documents\n: An iterable which yield either str or unicode.\n\n\n\n\n\n\n\n\nYields\n:\n\n\n\n\nX\n: iterable, [n_samples, max_document_length] Word-id matrix.\n\n\n\n\n\n\n\n\n\n\nfit_transform\n\n\nfit_transform\n(\nself\n,\n \nraw_documents\n,\n \nunused_y\n=\nNone\n)\n\n\n\n\n\n\nfit_transform.\n\n\nLearn the vocabulary dictionary and return indexies of words.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nraw_documents\n: An iterable which yield either str or unicode.\n\n\nunused_y\n: to match fit_transform signature of estimators.\n\n\n\n\n\n\n\n\nReturns\n:\n\n\n\n\nX\n: iterable, [n_samples, max_document_length] Word-id matrix.\n\n\n\n\n\n\n\n\n\n\nreverse\n\n\nreverse\n(\nself\n,\n \ndocuments\n)\n\n\n\n\n\n\nreverse.\n\n\nReverses output of vocabulary mapping to words.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ndocuments\n: iterable, list of class ids.\n\n\n\n\n\n\n\n\nReturns\n:\n    Iterator over mapped in words documents.\n\n\n\n\n\n\n\n\nsave\n\n\nsave\n(\nself\n,\n \nfilename\n)\n\n\n\n\n\n\nsave.\n\n\nSaves vocabulary processor into given file.\n\n\n\n\nArgs\n:\n\n\nfilename\n: Path to output file.\n\n\n\n\n\n\n\n\n\n\nrestore\n\n\nrestore\n(\ncls\n,\n \nfilename\n)\n\n\n\n\n\n\nrestore.\n\n\nRestores vocabulary processor from given file.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nfilename\n: Path to file to load from.\n\n\n\n\n\n\n\n\nReturns\n:\n    VocabularyProcessor object.", 
            "title": "Vocabulary Processor"
        }, 
        {
            "location": "/processing/vocabulary_processor/#vocabularyprocessor", 
            "text": "polyaxon . processing . text . VocabularyProcessor ( max_document_length ,   min_frequency = 0 ,   vocabulary = None ,   tokenizer_fn = None )   A mirror to tf.contrib.learn VocabularyProcessor.  Maps documents to sequences of word ids.    Args :   max_document_length : Maximum length of documents.\n    if documents are longer, they will be trimmed, if shorter - padded.  min_frequency : Minimum frequency of words in the vocabulary.  vocabulary : CategoricalVocabulary object.     Attributes :   vocabulary : CategoricalVocabulary object.", 
            "title": "VocabularyProcessor"
        }, 
        {
            "location": "/processing/vocabulary_processor/#fit", 
            "text": "fit ( self ,   raw_documents ,   unused_y = None )   fit.  Learn a vocabulary dictionary of all tokens in the raw documents.    Args :   raw_documents : An iterable which yield either str or unicode.  unused_y : to match fit format signature of estimators.     Returns :\n    self", 
            "title": "fit"
        }, 
        {
            "location": "/processing/vocabulary_processor/#transform", 
            "text": "transform ( self ,   raw_documents )   transform.  Transform documents to word-id matrix.  Convert words to ids with vocabulary fitted with fit or the one\nprovided in the constructor.    Args :   raw_documents : An iterable which yield either str or unicode.     Yields :   X : iterable, [n_samples, max_document_length] Word-id matrix.", 
            "title": "transform"
        }, 
        {
            "location": "/processing/vocabulary_processor/#fit_transform", 
            "text": "fit_transform ( self ,   raw_documents ,   unused_y = None )   fit_transform.  Learn the vocabulary dictionary and return indexies of words.    Args :   raw_documents : An iterable which yield either str or unicode.  unused_y : to match fit_transform signature of estimators.     Returns :   X : iterable, [n_samples, max_document_length] Word-id matrix.", 
            "title": "fit_transform"
        }, 
        {
            "location": "/processing/vocabulary_processor/#reverse", 
            "text": "reverse ( self ,   documents )   reverse.  Reverses output of vocabulary mapping to words.    Args :   documents : iterable, list of class ids.     Returns :\n    Iterator over mapped in words documents.", 
            "title": "reverse"
        }, 
        {
            "location": "/processing/vocabulary_processor/#save", 
            "text": "save ( self ,   filename )   save.  Saves vocabulary processor into given file.   Args :  filename : Path to output file.", 
            "title": "save"
        }, 
        {
            "location": "/processing/vocabulary_processor/#restore", 
            "text": "restore ( cls ,   filename )   restore.  Restores vocabulary processor from given file.    Args :   filename : Path to file to load from.     Returns :\n    VocabularyProcessor object.", 
            "title": "restore"
        }, 
        {
            "location": "/processing/image/", 
            "text": "[source]\n\n\nResize\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nResize\n(\nmode\n,\n \nheight\n,\n \nwidth\n,\n \nmethod\n=\nNone\n,\n \nalign_corners\n=\nFalse\n,\n \nname\n=\nResize\n)\n\n\n\n\n\n\nSee \nplx.image.resize\n's docstring\n\n\n\n\n[source]\n\n\nCentralCrop\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nCentralCrop\n(\nmode\n,\n \ncentral_fraction\n,\n \nname\n=\nCentralCrop\n)\n\n\n\n\n\n\nSee \nplx.image.central_crop\n's docstring\n\n\n\n\n[source]\n\n\nRandomCrop\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nRandomCrop\n(\nmode\n,\n \nheight\n,\n \nwidth\n,\n \nname\n=\nRandomCrop\n)\n\n\n\n\n\n\nSee \nplx.image.random_crop\n's docstring\n\n\n\n\n[source]\n\n\nExtractGlimpse\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nExtractGlimpse\n(\nmode\n,\n \nsize\n,\n \noffsets\n,\n \ncentered\n=\nNone\n,\n \nnormalized\n=\nNone\n,\n \nuniform_noise\n=\nNone\n,\n \nname\n=\nExtractGlimpse\n)\n\n\n\n\n\n\nSee \nplx.image.extract_glimpse\n's docstring\n\n\n\n\n[source]\n\n\nToBoundingBox\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nToBoundingBox\n(\nmode\n,\n \noffset_height\n,\n \noffset_width\n,\n \ntarget_height\n,\n \ntarget_width\n,\n \nmethod\n=\ncrop\n,\n \nname\n=\nToBoundingBox\n)\n\n\n\n\n\n\nSee \nplx.image.to_bounding_box\n's docstring\n\n\n\n\n[source]\n\n\nFlip\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nFlip\n(\nmode\n,\n \naxis\n=\n0\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n,\n \nname\n=\nFlip\n)\n\n\n\n\n\n\nSee \nplx.image.flip\n's docstring\n\n\n\n\n[source]\n\n\nTranspose\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nTranspose\n(\nmode\n,\n \nname\n=\nTranspose\n)\n\n\n\n\n\n\nSee \nplx.image.transpose\n's docstring\n\n\n\n\n[source]\n\n\nRotate90\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nRotate90\n(\nmode\n,\n \nk\n=\n1\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n,\n \nname\n=\nRotate90\n)\n\n\n\n\n\n\nSee \nplx.image.rotate90\n's docstring\n\n\n\n\n[source]\n\n\nConvertColorSpace\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nConvertColorSpace\n(\nmode\n,\n \nfrom_space\n,\n \nto_space\n,\n \nname\n=\nConvertColorSpace\n)\n\n\n\n\n\n\nSee \nplx.image.convert_color_space\n's docstring\n\n\n\n\n[source]\n\n\nConvertImagesDtype\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nConvertImagesDtype\n(\nmode\n,\n \ndtype\n,\n \nsaturate\n=\nFalse\n,\n \nname\n=\nConvertImagesDtype\n)\n\n\n\n\n\n\nSee \nplx.image.convert_images_dtype\n's docstring\n\n\n\n\n[source]\n\n\nAdjustBrightness\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nAdjustBrightness\n(\nmode\n,\n \ndelta\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n,\n \nname\n=\nAdjustBrightness\n)\n\n\n\n\n\n\nSee \nplx.image.adjust_brightness\n's docstring\n\n\n\n\n[source]\n\n\nAdjustContrast\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nAdjustContrast\n(\nmode\n,\n \ncontrast_factor\n,\n \ncontrast_factor_max\n=\nNone\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n,\n \nname\n=\nAdjustContrast\n)\n\n\n\n\n\n\nSee \nplx.image.adjust_contrast\n's docstring\n\n\n\n\n[source]\n\n\nAdjustHue\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nAdjustHue\n(\nmode\n,\n \ndelta\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n,\n \nname\n=\nAdjustHue\n)\n\n\n\n\n\n\nSee \nplx.image.adjust_hue\n's docstring\n\n\n\n\n[source]\n\n\nAdjustSaturation\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nAdjustSaturation\n(\nmode\n,\n \nsaturation_factor\n,\n \nsaturation_factor_max\n=\nNone\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n,\n \nname\n=\nAdjustSaturation\n)\n\n\n\n\n\n\nSee \nplx.image.adjust_saturation\n's docstring\n\n\n\n\n[source]\n\n\nAdjustGamma\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nAdjustGamma\n(\nmode\n,\n \ngamma\n=\n1\n,\n \ngain\n=\n1\n,\n \nname\n=\nAdjustGamma\n)\n\n\n\n\n\n\nSee \nplx.image.adjust_gamma\n's docstring\n\n\n\n\n[source]\n\n\nStandardization\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nStandardization\n(\nmode\n,\n \nname\n=\nStandardization\n)\n\n\n\n\n\n\nSee \nplx.image.standardize\n's docstring\n\n\n\n\n[source]\n\n\nDrawBoundingBoxes\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nDrawBoundingBoxes\n(\nmode\n,\n \nboxes\n,\n \nname\n=\nDrawBoundingBoxes\n)\n\n\n\n\n\n\nSee \nplx.image.draw_bounding_boxes\n's docstring\n\n\n\n\n[source]\n\n\nTotalVariation\n\n\npolyaxon\n.\nprocessing\n.\nimage\n.\nTotalVariation\n(\nmode\n,\n \nname\n=\nTotalVariation\n)\n\n\n\n\n\n\nSee \nplx.image.total_variation\n's docstring\n\n\n\n\nresize\n\n\nresize\n(\nimages\n,\n \nheight\n,\n \nwidth\n,\n \nmethod\n=\nNone\n,\n \nalign_corners\n=\nFalse\n)\n\n\n\n\n\n\nResize \nimages\n to \nsize\n using the specified \nmethod\n.\n(A mirror to tf.image resize_images and resize_image_with_crop_or_pad)\n\n\n\n\n\n\nIf method is None: Resizes an image to a target width and height by either centrally\ncropping the image or padding it evenly with zeros.\nIf \nwidth\n or \nheight\n is greater than the specified \ntarget_width\n or\n\ntarget_height\n respectively, this op centrally crops along that dimension.\nIf \nwidth\n or \nheight\n is smaller than the specified \ntarget_width\n or\n\ntarget_height\n respectively, this op centrally pads with 0 along that dimension.\n\n\n\n\n\n\nIf method is not None: the resized images will be distorted if their original aspect\nratio is not the same as \nsize\n.\n\n\n\n\n\n\nmethod\n can be one of:\n\n\n\n\nNone\n no distortion.\n\n\nResizeMethod.BILINEAR\n: \nBilinear interpolation.\n\n\nResizeMethod.NEAREST_NEIGHBOR\n: \nNearest neighbor interpolation.\n\n\nResizeMethod.BICUBIC\n: \nBicubic interpolation.\n\n\n\n\nResizeMethod.AREA\n: Area interpolation.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: 4-D Tensor of shape \n[batch, height, width, channels]\n or\n3-D Tensor of shape \n[height, width, channels]\n.\n\n\nheight\n: int32 Target height.\n\n\nwidth\n: int32 Target width.\n\n\nmethod\n: ResizeMethod.  Defaults to \nResizeMethod.BILINEAR\n.\nPossible values: BILINEAR, NEAREST_NEIGHBOR, BICUBIC, AREA\n\n\nalign_corners\n: bool. If true, exactly align all 4 corners of the input and output.\nOnly used if method is not None. Defaults to \nfalse\n.\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if the shape of \nimages\n is incompatible with the\nshape arguments to this function.\n\n\nValueError\n: if \nsize\n has invalid shape or type.\n\n\nValueError\n: if an unsupported resize method is specified.\n\n\n\n\n\n\n\n\nReturns\n:\n    If \nimages\n was 4-D, a 4-D float Tensor of shape\n    \n[batch, new_height, new_width, channels]\n.\n    If \nimages\n was 3-D, a 3-D float Tensor of shape\n    \n[new_height, new_width, channels]\n.\n\n\n\n\n\n\n\n\ncentral_crop\n\n\ncentral_crop\n(\nimages\n,\n \ncentral_fraction\n)\n\n\n\n\n\n\nCrop the central region of the image.\n(A mirror to tf.image central_crop)\n\n\nRemove the outer parts of an image but retain the central region of the image\nalong each dimension. If we specify central_fraction = 0.5, this function\nreturns the region marked with \"X\" in the below diagram.\n\n\n --------\n|........|\n|..XXXX..|\n|..XXXX..|\n|........|   where \nX\n is the central 50% of the image.\n --------\n\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: 4-D Tensor of shape \n[batch, height, width, channels]\n or\n3-D Tensor of shape \n[height, width, channels]\n.\n\n\ncentral_fraction\n: float (0, 1], fraction of size to crop\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if central_crop_fraction is not within (0, 1].\n\n\n\n\n\n\n\n\nReturns\n:\n    If \nimages\n was 4-D, a 4-D float Tensor of shape\n    \n[batch, new_height, new_width, channels]\n.\n    If \nimages\n was 3-D, a 3-D float Tensor of shape\n    \n[new_height, new_width, channels]\n.\n\n\n\n\n\n\n\n\nrandom_crop\n\n\nrandom_crop\n(\nimages\n,\n \nheight\n,\n \nwidth\n)\n\n\n\n\n\n\nRandomly crops an image/images to a given size.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: 4-D Tensor of shape \n[batch, height, width, channels]\n or\n3-D Tensor of shape \n[height, width, channels]\n.\n\n\nheight\n: \nfloat\n. The height to crop to.\n\n\nwidth\n: \nfloat\n. The width to crop to.\n\n\n\n\n\n\n\n\nReturns\n:\n    If \nimages\n was 4-D, a 4-D float Tensor of shape\n    \n[batch, new_height, new_width, channels]\n.\n    If \nimages\n was 3-D, a 3-D float Tensor of shape\n    \n[new_height, new_width, channels]\n.\n\n\n\n\n\n\n\n\nextract_glimpse\n\n\nextract_glimpse\n(\nimages\n,\n \nsize\n,\n \noffsets\n,\n \ncentered\n=\nNone\n,\n \nnormalized\n=\nNone\n,\n \nuniform_noise\n=\nNone\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nExtracts a glimpse from the input tensor.\n(A mirror to tf.image extract_glimpse)\n\n\nReturns a set of windows called glimpses extracted at location \noffsets\n\nfrom the input tensor. If the windows only partially overlaps the inputs,\nthe non overlapping areas will be filled with random noise.\n\n\nThe result is a 4-D tensor of shape \n[batch_size, glimpse_height,\nglimpse_width, channels]\n. The channels and batch dimensions are the\nsame as that of the input tensor. The height and width of the output\nwindows are specified in the \nsize\n parameter.\n\n\nThe argument \nnormalized\n and \ncentered\n controls how the windows are built:\n\n\n* If the coordinates are normalized but not centered, 0.0 and 1.0\ncorrespond to the minimum and maximum of each height and width\ndimension.\n* If the coordinates are both normalized and centered, they range from -1.0 to 1.0.\nThe coordinates (-1.0, -1.0) correspond to the upper left corner, the lower right\ncorner is located at (1.0, 1.0) and the center is at (0, 0).\n* If the coordinates are not normalized they are interpreted as numbers of pixels.\n\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: A \nTensor\n of type \nfloat32\n.\nA 4-D float tensor of shape \n[batch_size, height, width, channels]\n.\n\n\nsize\n: A \nTensor\n of type \nint32\n.\nA 1-D tensor of 2 elements containing the size of the glimpses to extract.\nThe glimpse height must be specified first, following by the glimpse width.\n\n\noffsets\n: A \nTensor\n of type \nfloat32\n.\nA 2-D integer tensor of shape \n[batch_size, 2]\n containing\nthe y, x locations of the center of each window.\n\n\ncentered\n: An optional \nbool\n. Defaults to \nTrue\n.\nindicates if the offset coordinates are centered relative to the image,\nin which case the (0, 0) offset is relative to the center of the input images.\nIf false, the (0,0) offset corresponds to the upper left corner of the input images.\n\n\nnormalized\n: An optional \nbool\n. Defaults to \nTrue\n.\nindicates if the offset coordinates are normalized.\n\n\nuniform_noise\n: An optional \nbool\n. Defaults to \nTrue\n.\nindicates if the noise should be generated using a\nuniform distribution or a Gaussian distribution.\n\n\nname\n: A name for the operation (optional).\n\n\n\n\n\n\n\n\nReturns\n:\n    A \nTensor\n of type \nfloat32\n.\n    A tensor representing the glimpses \n[batch_size, glimpse_height, glimpse_width, channels]\n.\n\n\n\n\n\n\n\n\nto_bounding_box\n\n\nto_bounding_box\n(\nimages\n,\n \noffset_height\n,\n \noffset_width\n,\n \ntarget_height\n,\n \ntarget_width\n,\n \nmethod\n=\ncrop\n)\n\n\n\n\n\n\nPad/Crop \nimage\n with zeros to the specified \nheight\n and \nwidth\n.\n(A mirror to tf.image pad_to_bounding_box and crop_to_bounding_box)\n\n\nIf method == 'pad':\n    Adds \noffset_height\n rows of zeros on top, \noffset_width\n columns of\n    zeros on the left, and then pads the image on the bottom and right\n    with zeros until it has dimensions \ntarget_height\n, \ntarget_width\n.\n\n\nThis op does nothing if `offset_*` is zero and the image already has size\n`target_height` by `target_width`.\n\n\n\n\n\nIf method == 'crop':\n    Crops an image to a specified bounding box.\n\n\nThis op cuts a rectangular part out of `image`. The top-left corner of the\nreturned image is at `offset_height, offset_width` in `image`, and its\nlower-right corner is at `offset_height + target_height, offset_width + target_width`.\n\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: 4-D Tensor of shape \n[batch, height, width, channels]\n or\n   3-D Tensor of shape \n[height, width, channels]\n.\n\n\noffset_height\n:\n\n\npad: Number of rows of zeros to add on top.\n\n\ncrop: Vertical coordinate of the top-left corner of the result the input.\n\n\noffset_width\n:\n\n\npad: Number of columns of zeros to add on the left.\n\n\ncrop: Horizontal coordinate of the top-left corner of the result in the input.\n\n\ntarget_height\n: Height of output image.\n\n\ntarget_width\n: Width of output image.\n\n\nmethod\n: \ncrop\n or \npad\n\n\n\n\n\n\n\n\nReturns\n:\n    If \nimage\n was 4-D, a 4-D float Tensor of shape\n    \n[batch, target_height, target_width, channels]\n\n    If \nimage\n was 3-D, a 3-D float Tensor of shape\n    \n[target_height, target_width, channels]\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If the shape of \nimage\n is incompatible with the \noffset_*\n or\n\ntarget_*\n arguments, or either \noffset_height\n or \noffset_width\n is negative.\n\n\n\n\n\n\n\n\n\n\nflip\n\n\nflip\n(\nimages\n,\n \naxis\n=\n0\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n)\n\n\n\n\n\n\nFlip (randomly) an image/images.\n(A mirror to tf.image flip_left_right, flip_up_down, random_flip_left_right, and\nrandom_flip_up_down)\n\n\nif axis is 0:\n    * flip horizontally (left to right)\n if axis is 1:\n    * vertically (upside down).\n\n\nOutputs the contents of \nimages\n flipped along the given axis.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: 4-D Tensor of shape \n[batch, height, width, channels]\n or\n3-D Tensor of shape \n[height, width, channels]\n.\n\n\naxis\n: \nint\n. 0 for horizontal, 1 for vertical\n\n\nis_random\n: \nbool\n, If True, flip randomly.\n\n\nseed\n: A Python integer. Used to create a random seed. See @{tf.set_random_seed}.\n\n\n\n\n\n\n\n\nReturns\n:\n    If \nimage\n was 4-D, a 4-D float Tensor of shape\n    \n[batch, target_height, target_width, channels]\n\n    If \nimage\n was 3-D, a 3-D float Tensor of shape\n    `[target_height, target_width, channels]\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if the shape of \nimage\n not supported.\n\n\n\n\n\n\n\n\n\n\ntranspose\n\n\ntranspose\n(\nimages\n)\n\n\n\n\n\n\nTranspose an image/images by swapping the first and second dimension.\n(A mirror to tf.image transpose_image)\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: 4-D Tensor of shape \n[batch, height, width, channels]\n or\n3-D Tensor of shape \n[height, width, channels]\n.\n\n\n\n\n\n\n\n\nReturns\n:\n    If \nimage\n was 4-D, a 4-D float Tensor of shape\n    \n[batch, target_height, target_width, channels]\n\n    If \nimage\n was 3-D, a 3-D float Tensor of shape\n    `[target_height, target_width, channels]\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if the shape of \nimage\n not supported.\n\n\n\n\n\n\n\n\n\n\nrotate90\n\n\nrotate90\n(\nimages\n,\n \nk\n=\n1\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nRotate (randomly) images counter-clockwise by 90 degrees.\n(A mirror to tf.image rot90)\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: 4-D Tensor of shape \n[batch, height, width, channels]\n or\n3-D Tensor of shape \n[height, width, channels]\n.\n\n\nk\n: A scalar integer. The number of times the image is rotated by 90 degrees.\n\n\nis_random\n: \nbool\n, If True, adjust randomly.\n\n\nseed\n: A Python integer. Used to create a random seed. See @{tf.set_random_seed}.\n\n\nname\n: A name for this operation (optional).\n\n\n\n\n\n\n\n\nReturns\n:\n    If \nimage\n was 4-D, a 4-D float Tensor of shape\n    \n[batch, target_height, target_width, channels]\n\n    If \nimage\n was 3-D, a 3-D float Tensor of shape\n    `[target_height, target_width, channels]\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if the shape of \nimage\n not supported.\n\n\n\n\n\n\n\n\n\n\nconvert_color_space\n\n\nconvert_color_space\n(\nimages\n,\n \nfrom_space\n,\n \nto_space\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nConverts one or more images from RGB to Grayscale.\n(A mirror to tf.image rgb_to_grayscale, rgb_to_hsv, grayscale_to_rgb, and hsv_to_rgb)\n\n\nOutputs a tensor of the same \nDType\n and rank as \nimages\n.\n\n\nPossible conversions:\n    * rgb_to_grayscale: The size of the last dimension of the output is 1,\n    containing the Grayscale value of the pixels.\n    * grayscale_to_rgb: The size of the last dimension of the output is 3,\n    containing the RGB value of the pixels.\n    * hsv_to_rgb: The output is only well defined if the value in \nimages\n are in \n[0,1]\n.\n    * rgb_to_hsv: The output is only well defined if the value in \nimages\n are in \n[0,1]\n.\n    \noutput[..., 0]\n contains hue, \noutput[..., 1]\n contains saturation, and\n    \noutput[..., 2]\n contains value. All HSV values are in \n[0,1]\n. A hue of 0\n    corresponds to pure red, hue 1/3 is pure green, and 2/3 is pure blue.\n    * grayscale_to_hsv: grayscale_to_rgb then rgb_to_hsv\n    * hsv_to_grayscale: hsv_to_rgb then rgb_to_grayscale.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: 4-D Tensor of shape \n[batch, height, width, channels]\n\n\nfrom_space\n: The color to convert from.\n\n\nto_space\n: The color space to convert to.\n\n\nname\n: A name for the operation (optional).\n\n\n\n\n\n\n\n\nReturns\n:\n    The converted image(s).\n\n\n\n\n\n\n\n\nconvert_images_dtype\n\n\nconvert_images_dtype\n(\nimages\n,\n \ndtype\n,\n \nsaturate\n=\nFalse\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nConvert image(s) to \ndtype\n, scaling its values if needed.\n(A mirror to tf.image convert_image_dtype)\n\n\nImages that are represented using floating point values are expected to have\n  values in the range [0,1). Image data stored in integer data types are\n  expected to have values in the range \n[0,MAX]\n, where \nMAX\n is the largest\n  positive representable number for the data type.\n\n\nThis op converts between data types, scaling the values appropriately before\n  casting.\n\n\nNote that converting from floating point inputs to integer types may lead to\n  over/underflow problems. Set saturate to \nTrue\n to avoid such problem in\n  problematic conversions. If enabled, saturation will clip the output into the\n  allowed range before performing a potentially dangerous cast (and only before\n  performing such a cast, i.e., when casting from a floating point to an integer\n  type, and when casting from a signed to an unsigned type; \nsaturate\n has no\n  effect on casts between floats, or on casts that increase the type's range).\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: An image.\n\n\ndtype\n: A \nDType\n to convert \nimage\n to.\n\n\nsaturate\n: If \nTrue\n, clip the input before casting (if necessary).\n\n\nname\n: A name for this operation (optional).\n\n\n\n\n\n\n\n\nReturns\n:\n    \nimage\n, converted to \ndtype\n.\n\n\n\n\n\n\n\n\nadjust_brightness\n\n\nadjust_brightness\n(\nimages\n,\n \ndelta\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n)\n\n\n\n\n\n\nAdjust (randomly) the brightness of RGB or Grayscale images.\n(A mirror to tf.image adjust_brightness, random_birightness)\n\n\nThis is a convenience method that converts an RGB image to float\nrepresentation, adjusts its brightness, and then converts it back to the\noriginal data type. If several adjustments are chained it is advisable to\nminimize the number of redundant conversions.\n\n\nThe value \ndelta\n is added to all components of the tensor \nimage\n. Both\n\nimage\n and \ndelta\n are converted to \nfloat\n before adding (and \nimage\n is\nscaled appropriately if it is in fixed-point representation). For regular\nimages, \ndelta\n should be in the range \n[0,1)\n, as it is added to the image in\nfloating point representation, where pixel values are in the \n[0,1)\n range.\n\n\nIf \nis_random\n is \nTrue\n, adjust brightness using a value randomly picked in the\ninterval \n[-delta, delta)\n.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: A tensor.\n\n\ndelta\n: \nfloat\n. Amount to add to the pixel values.\n\n\nis_random\n: \nbool\n, If True, adjust randomly.\n\n\nseed\n: A Python integer. Used to create a random seed. See @{tf.set_random_seed}.\n\n\n\n\n\n\n\n\nReturns\n:\n    A brightness-adjusted tensor of the same shape and type as \nimages\n.\n\n\n\n\n\n\n\n\nadjust_contrast\n\n\nadjust_contrast\n(\nimages\n,\n \ncontrast_factor\n,\n \ncontrast_factor_max\n=\nNone\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n)\n\n\n\n\n\n\nAdjust (randomly) the contrast of RGB or grayscale images by contrast factor.\n(A mirror to tf.image adjust_contrast, random_contrast)\n\n\nThis is a convenience method that converts an RGB image to float\nrepresentation, adjusts its contrast, and then converts it back to the\noriginal data type. If several adjustments are chained it is advisable to\nminimize the number of redundant conversions.\n\n\nimages\n is a tensor of at least 3 dimensions.  The last 3 dimensions are\ninterpreted as \n[height, width, channels]\n.  The other dimensions only\nrepresent a collection of images, such as \n[batch, height, width, channels].\n\n\nContrast is adjusted independently for each channel of each image.\n\n\nFor each channel, this Op computes the mean of the image pixels in the\nchannel and then adjusts each component \nx\n of each pixel to\n\n(x - mean) * contrast_factor + mean\n.\n\n\nIf \nis_random\n is \nTrue\n: Equivalent to \nadjust_contrast()\n but the value is\n    randomly picked in the interval \n[contrast_factor, contrast_factor_max]\n.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: \ntensor\n. images tensor with 3 or more dimensions.\n\n\ncontrast_factor\n: \nfloat\n.  Lower bound for the random contrast factor.\n\n\ncontrast_factor_max\n: \nfloat\n.  Upper bound for the random contrast factor.\nUsed for random adjustment.\n\n\nis_random\n: \nbool\n, If True, adjust randomly.\n\n\nseed\n: A Python integer. Used to create a random seed. See @{tf.set_random_seed}.\n\n\n\n\n\n\n\n\nReturns\n:\n    The contrast-adjusted tensor.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if \ncontrast_factor_max \n= contrast_factor\n\n    if \ncontrast_factor \n 0\n\n    if \ncontrast_factor_max\n is None (for random.)\n\n\n\n\n\n\n\n\n\n\nadjust_hue\n\n\nadjust_hue\n(\nimages\n,\n \ndelta\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nAdjust (randomly) hue of an RGB images.\n(A mirror to tf.image adjust_hue, random_hue)\n\n\nThis is a convenience method that converts an RGB image to float\nrepresentation, converts it to HSV, add an offset to the hue channel, converts\nback to RGB and then back to the original data type. If several adjustments\nare chained it is advisable to minimize the number of redundant conversions.\n\n\nimage\n is an RGB image.  The image hue is adjusted by converting the\nimage to HSV and rotating the hue channel (H) by \ndelta\n.\nThe image is then converted back to RGB.\n\n\ndelta\n must be in the interval \n[-1, 1]\n.\n\n\nIf \nis_random\n is \nTrue\n adjust hue but uses a value randomly picked in\nthe interval \n[-delta, delta]\n.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: RGB image or images. Size of the last dimension must be 3.\n\n\ndelta\n: float.  How much to add to the hue channel.\n\n\nis_random\n: \nbool\n, If True, adjust randomly.\n\n\nseed\n: A Python integer. Used to create a random seed. See @{tf.set_random_seed}.\n\n\nname\n: A name for this operation (optional).\n\n\n\n\n\n\n\n\nReturns\n:\n    Adjusted image(s), same shape and DType as \nimage\n.\n\n\n\n\n\n\n\n\nadjust_saturation\n\n\nadjust_saturation\n(\nimages\n,\n \nsaturation_factor\n,\n \nsaturation_factor_max\n=\nNone\n,\n \nis_random\n=\nFalse\n,\n \nseed\n=\nNone\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nAdjust (randomly) saturation of RGB images.\n(A mirror to tf.image adjust_saturation, random_saturation)\n\n\nThis is a convenience method that converts an RGB image to float\nrepresentation, converts it to HSV, add an offset to the saturation channel,\nconverts back to RGB and then back to the original data type. If several\nadjustments are chained it is advisable to minimize the number of redundant\nconversions.\n\n\nThe image saturation is adjusted by converting the images to HSV and\nmultiplying the saturation (S) channel by \nsaturation_factor\n and clipping.\nThe images is then converted back to RGB.\n\n\nIf \nis_random\n is \nTrue\n adjust saturation but uses a value randomly picked in\nthe interval \n[saturation_factor, saturation_factor_max]\n.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: RGB image or images. Size of the last dimension must be 3.\n\n\nsaturation_factor\n: float.  Lower bound for the random saturation factor.\n\n\nsaturation_factor_max\n: float.  Upper bound for the random saturation factor.\n\n\nis_random\n: \nbool\n, If True, adjust randomly.\n\n\nseed\n: An operation-specific seed. It will be used in conjunction\n  with the graph-level seed to determine the real seeds that will be\n  used in this operation. Please see the documentation of\n  set_random_seed for its interaction with the graph-level random seed.\n\n\nname\n: A name for this operation (optional).\n\n\n\n\n\n\n\n\nReturns\n:\n    Adjusted image(s), same shape and DType as \nimage\n.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if \nsaturation_factor_max \n= saturation_factor\n\n    if \nsaturation_factor \n 0\n\n    if \nsaturation_factor_max is None (for random.)\n\n\n\n\n\n\n\n\n\n\nadjust_gamma\n\n\nadjust_gamma\n(\nimage\n,\n \ngamma\n=\n1\n,\n \ngain\n=\n1\n)\n\n\n\n\n\n\nPerforms Gamma Correction on the input image.\nAlso known as Power Law Transform. This function transforms the\ninput image pixelwise according to the equation Out = In**gamma\nafter scaling each pixel to the range 0 to 1.\n(A mirror to tf.image adjust_gamma)\n\n\n\n\n\n\nArgs\n:\n    image : A Tensor.\n    gamma : A scalar. Non negative real number.\n    gain  : A scalar. The constant multiplier.\n\n\n\n\n\n\nReturns\n:\n    A Tensor. Gamma corrected output image.\n\n\n\n\n\n\nNotes\n:\n    For gamma greater than 1, the histogram will shift towards left and\n    the output image will be darker than the input image.\n    For gamma less than 1, the histogram will shift towards right and\n    the output image will be brighter than the input image.\n\n\n\n\n\n\nReferences\n:\n    [1] http://en.wikipedia.org/wiki/Gamma_correction\n\n\n\n\n\n\n\n\nstandardize\n\n\nstandardize\n(\nimages\n)\n\n\n\n\n\n\nLinearly scales \nimage\n to have zero mean and unit norm.\n(A mirror to tf.image per_image_standardization)\n\n\nThis op computes \n(x - mean) / adjusted_stddev\n, where \nmean\n is the average\nof all values in image, and\n\nadjusted_stddev = max(stddev, 1.0/sqrt(image.NumElements()))\n.\n\n\nstddev\n is the standard deviation of all values in \nimage\n. It is capped\naway from zero to protect against division by 0 when handling uniform images.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: 4-D Tensor of shape \n[batch, height, width, channels]\n or\n    3-D Tensor of shape \n[height, width, channels]\n.\n\n\n\n\n\n\n\n\nReturns\n:\n    The standardized image with same shape as \nimage\n.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if the shape of 'image' is incompatible with this function.\n\n\n\n\n\n\n\n\n\n\ndraw_bounding_boxes\n\n\ndraw_bounding_boxes\n(\nimages\n,\n \nboxes\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nDraw bounding boxes on a batch of images.\n(A mirror to tf.image draw_bounding_boxes)\n\n\nOutputs a copy of \nimages\n but draws on top of the pixels zero or more bounding\nboxes specified by the locations in \nboxes\n. The coordinates of the each\nbounding box in \nboxes\n are encoded as \n[y_min, x_min, y_max, x_max]\n. The\nbounding box coordinates are floats in \n[0.0, 1.0]\n relative to the width and\nheight of the underlying image.\n\n\nFor example, if an image is 100 x 200 pixels and the bounding box is\n\n[0.1, 0.2, 0.5, 0.9]\n, the bottom-left and upper-right coordinates of the\nbounding box will be \n(10, 40)\n to \n(50, 180)\n.\n\n\nParts of the bounding box may fall outside the image.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimages\n: A \nTensor\n. Must be one of the following types: \nfloat32\n, \nhalf\n.\n4-D with shape \n[batch, height, width, depth]\n. A batch of images.\n\n\nboxes\n: A \nTensor\n of type \nfloat32\n.\n3-D with shape \n[batch, num_bounding_boxes, 4]\n containing bounding boxes.\n\n\nname\n: A name for the operation (optional).\n\n\n\n\n\n\n\n\nReturns\n:\n    A \nTensor\n. Has the same type as \nimages\n.\n    4-D with the same shape as \nimages\n. The batch of input images with\n    bounding boxes drawn on the images.\n\n\n\n\n\n\n\n\nnon_max_suppression\n\n\nnon_max_suppression\n(\nboxes\n,\n \nscores\n,\n \nmax_output_size\n,\n \niou_threshold\n=\nNone\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nGreedily selects a subset of bounding boxes in descending order of score,\npruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system.  Note that this\nalgorithm is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\n\n\n(A mirror to tf.image non_max_suppression)\n\n\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the \ntf.gather operation\n.  For example:\n\n\n\n\nExamples\n:\n\n\n\n\n \nselected_indices\n \n=\n \ntf\n.\nimage\n.\nnon_max_suppression\n(\n\n\n...\n \nboxes\n,\n \nscores\n,\n \nmax_output_size\n,\n \niou_threshold\n)\n\n\n \nselected_boxes\n \n=\n \ntf\n.\ngather\n(\nboxes\n,\n \nselected_indices\n)\n\n\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nboxes\n: A \nTensor\n of type \nfloat32\n.\nA 2-D float tensor of shape \n[num_boxes, 4]\n.\n\n\nscores\n: A \nTensor\n of type \nfloat32\n.\nA 1-D float tensor of shape \n[num_boxes]\n representing a single\nscore corresponding to each box (each row of boxes).\n\n\nmax_output_size\n: A \nTensor\n of type \nint32\n.\nA scalar integer tensor representing the maximum number of\nboxes to be selected by non max suppression.\n\n\niou_threshold\n: An optional \nfloat\n. Defaults to \n0.5\n.\nA float representing the threshold for deciding whether boxes\noverlap too much with respect to IOU.\n\n\nname\n: A name for the operation (optional).\n\n\n\n\n\n\n\n\nReturns\n:\n    A \nTensor\n of type \nint32\n.\n    A 1-D integer tensor of shape \n[M]\n representing the selected\n    indices from the boxes tensor, where \nM \n= max_output_size\n.\n\n\n\n\n\n\n\n\nsample_distorted_bounding_box\n\n\nsample_distorted_bounding_box\n(\nimage_size\n,\n \nbounding_boxes\n,\n \nseed\n=\nNone\n,\n \nseed2\n=\nNone\n,\n \nmin_object_covered\n=\nNone\n,\n \naspect_ratio_range\n=\nNone\n,\n \narea_range\n=\nNone\n,\n \nmax_attempts\n=\nNone\n,\n \nuse_image_if_no_bounding_boxes\n=\nNone\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nGenerate a single randomly distorted bounding box for an image.\n\n\nBounding box annotations are often supplied in addition to ground-truth labels\nin image recognition or object localization tasks. A common technique for\ntraining such a system is to randomly distort an image while preserving\nits content, i.e. \ndata augmentation\n. This Op outputs a randomly distorted\nlocalization of an object, i.e. bounding box, given an \nimage_size\n,\n\nbounding_boxes\n and a series of constraints.\n\n\n(A mirror to tf.image sample_distorted_bounding_box)\n\n\nThe output of this Op is a single bounding box that may be used to crop the\noriginal image. The output is returned as 3 tensors: \nbegin\n, \nsize\n and\n\nbboxes\n. The first 2 tensors can be fed directly into \ntf.slice\n to crop the\nimage. The latter may be supplied to \ntf.image.draw_bounding_boxes\n to visualize\nwhat the bounding box looks like.\n\n\nBounding boxes are supplied and returned as \n[y_min, x_min, y_max, x_max]\n. The\nbounding box coordinates are floats in \n[0.0, 1.0]\n relative to the width and\nheight of the underlying image.\n\n\n\n\nExamples\n:\n\n\n\n\n \n# Generate a single distorted bounding box.\n\n\n \nbegin\n,\n \nsize\n,\n \nbbox_for_draw\n \n=\n \ntf\n.\nimage\n.\nsample_distorted_bounding_box\n(\n\n\n...\n \ntf\n.\nshape\n(\nimage\n),\n \nbounding_boxes\n=\nbounding_boxes\n)\n\n\n\n \n# Draw the bounding box in an image summary.\n\n\n \nimage_with_box\n \n=\n \ntf\n.\nimage\n.\ndraw_bounding_boxes\n(\n\n\n...\n \ntf\n.\nexpand_dims\n(\nimage\n,\n \n0\n),\n \nbbox_for_draw\n)\n\n\n \ntf\n.\nimage_summary\n(\nimages_with_box\n,\n \nimage_with_box\n)\n\n\n\n \n# Employ the bounding box to distort the image.\n\n\n \ndistorted_image\n \n=\n \ntf\n.\nslice\n(\nimage\n,\n \nbegin\n,\n \nsize\n)\n\n\n\n\n\n\nNote that if no bounding box information is available, setting\n\nuse_image_if_no_bounding_boxes = true\n will assume there is a single implicit\nbounding box covering the whole image. If \nuse_image_if_no_bounding_boxes\n is\nfalse and no bounding boxes are supplied, an error is raised.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nimage_size\n: A \nTensor\n. Must be one of the following types:\n\nuint8\n, \nint8\n, \nint16\n, \nint32\n, \nint64\n.\n1-D, containing \n[height, width, channels]\n.\n\n\nbounding_boxes\n: A \nTensor\n of type \nfloat32\n.\n3-D with shape \n[batch, N, 4]\n describing the N bounding boxes\nassociated with the image.\n\n\nseed\n: An optional \nint\n. Defaults to \n0\n.\nIf either \nseed\n or \nseed2\n are set to non-zero, the random number\ngenerator is seeded by the given \nseed\n.  Otherwise, it is seeded by a random seed.\n\n\nseed2\n: An optional \nint\n. Defaults to \n0\n. A second seed to avoid seed collision.\n\n\nmin_object_covered\n: An optional \nfloat\n. Defaults to \n0.1\n.\nThe cropped area of the image must contain at least this\nfraction of any bounding box supplied. The value of this parameter should be\nnon-negative. In the case of 0, the cropped area does not need to overlap\nany of the bounding boxes supplied.\n\n\naspect_ratio_range\n: An optional list of \nfloats\n. Defaults to \n[0.75, 1.33]\n.\nThe cropped area of the image must have an aspect ratio =\nwidth / height within this range.\n\n\narea_range\n: An optional list of \nfloats\n. Defaults to \n[0.05, 1]\n.\nThe cropped area of the image must contain a fraction of the\nsupplied image within in this range.\n\n\nmax_attempts\n: An optional \nint\n. Defaults to \n100\n.\nNumber of attempts at generating a cropped region of the image\nof the specified constraints. After \nmax_attempts\n failures, return the entire image.\n\n\nuse_image_if_no_bounding_boxes\n: An optional \nbool\n. Defaults to \nFalse\n.\nControls behavior if no bounding boxes supplied.\nIf true, assume an implicit bounding box covering the whole input. If false,\nraise an error.\n\n\nname\n: A name for the operation (optional).\n\n\n\n\n\n\n\n\nReturns\n:\n    A tuple of \nTensor\n objects (begin, size, bboxes).\n\n\n\n\nbegin\n: A \nTensor\n. Has the same type as \nimage_size\n. 1-D, containing\n\n[offset_height, offset_width, 0]\n. Provide as input to \ntf.slice\n.\n\n\nsize\n: A \nTensor\n. Has the same type as \nimage_size\n. 1-D, containing\n\n[target_height, target_width, -1]\n. Provide as input to \ntf.slice\n.\n\n\nbboxes\n: A \nTensor\n of type \nfloat32\n. 3-D with shape \n[1, 1, 4]\n containing\nthe distorted bounding box. Provide as input to \ntf.image.draw_bounding_boxes\n.\n\n\n\n\n\n\n\n\n\n\ntotal_variation\n\n\ntotal_variation\n(\nimages\n,\n \nname\n=\nNone\n)\n\n\n\n\n\n\nCalculate and return the total variation for one or more images.\n\n\n(A mirror to tf.image total_variation)\n\n\nThe total variation is the sum of the absolute differences for neighboring\npixel-values in the input images. This measures how much noise is in the\nimages.\n\n\nThis can be used as a loss-function during optimization so as to suppress\nnoise in images. If you have a batch of images, then you should calculate\nthe scalar loss-value as the sum:\n\nloss = tf.reduce_sum(tf.image.total_variation(images))\n\n\nThis implements the anisotropic 2-D version of the formula described here:\n\n\n\n\n\n\nhttps\n://en.wikipedia.org/wiki/Total_variation_denoising\n\n\n\n\n\n\nArgs\n:\n\n\n\n\n\n\nimages\n: 4-D Tensor of shape \n[batch, height, width, channels]\n or\n    3-D Tensor of shape \n[height, width, channels]\n.\n\n\n\n\n\n\nname\n: A name for the operation (optional).\n\n\n\n\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if images.shape is not a 3-D or 4-D vector.\n\n\n\n\n\n\n\n\nReturns\n:\n    The total variation of \nimages\n.\n\n\nIf \nimages\n was 4-D, return a 1-D float Tensor of shape \n[batch]\n with the\ntotal variation for each image in the batch.\nIf \nimages\n was 3-D, return a scalar float with the total variation for\nthat image.", 
            "title": "Image"
        }, 
        {
            "location": "/processing/image/#resize", 
            "text": "polyaxon . processing . image . Resize ( mode ,   height ,   width ,   method = None ,   align_corners = False ,   name = Resize )   See  plx.image.resize 's docstring   [source]", 
            "title": "Resize"
        }, 
        {
            "location": "/processing/image/#centralcrop", 
            "text": "polyaxon . processing . image . CentralCrop ( mode ,   central_fraction ,   name = CentralCrop )   See  plx.image.central_crop 's docstring   [source]", 
            "title": "CentralCrop"
        }, 
        {
            "location": "/processing/image/#randomcrop", 
            "text": "polyaxon . processing . image . RandomCrop ( mode ,   height ,   width ,   name = RandomCrop )   See  plx.image.random_crop 's docstring   [source]", 
            "title": "RandomCrop"
        }, 
        {
            "location": "/processing/image/#extractglimpse", 
            "text": "polyaxon . processing . image . ExtractGlimpse ( mode ,   size ,   offsets ,   centered = None ,   normalized = None ,   uniform_noise = None ,   name = ExtractGlimpse )   See  plx.image.extract_glimpse 's docstring   [source]", 
            "title": "ExtractGlimpse"
        }, 
        {
            "location": "/processing/image/#toboundingbox", 
            "text": "polyaxon . processing . image . ToBoundingBox ( mode ,   offset_height ,   offset_width ,   target_height ,   target_width ,   method = crop ,   name = ToBoundingBox )   See  plx.image.to_bounding_box 's docstring   [source]", 
            "title": "ToBoundingBox"
        }, 
        {
            "location": "/processing/image/#flip", 
            "text": "polyaxon . processing . image . Flip ( mode ,   axis = 0 ,   is_random = False ,   seed = None ,   name = Flip )   See  plx.image.flip 's docstring   [source]", 
            "title": "Flip"
        }, 
        {
            "location": "/processing/image/#transpose", 
            "text": "polyaxon . processing . image . Transpose ( mode ,   name = Transpose )   See  plx.image.transpose 's docstring   [source]", 
            "title": "Transpose"
        }, 
        {
            "location": "/processing/image/#rotate90", 
            "text": "polyaxon . processing . image . Rotate90 ( mode ,   k = 1 ,   is_random = False ,   seed = None ,   name = Rotate90 )   See  plx.image.rotate90 's docstring   [source]", 
            "title": "Rotate90"
        }, 
        {
            "location": "/processing/image/#convertcolorspace", 
            "text": "polyaxon . processing . image . ConvertColorSpace ( mode ,   from_space ,   to_space ,   name = ConvertColorSpace )   See  plx.image.convert_color_space 's docstring   [source]", 
            "title": "ConvertColorSpace"
        }, 
        {
            "location": "/processing/image/#convertimagesdtype", 
            "text": "polyaxon . processing . image . ConvertImagesDtype ( mode ,   dtype ,   saturate = False ,   name = ConvertImagesDtype )   See  plx.image.convert_images_dtype 's docstring   [source]", 
            "title": "ConvertImagesDtype"
        }, 
        {
            "location": "/processing/image/#adjustbrightness", 
            "text": "polyaxon . processing . image . AdjustBrightness ( mode ,   delta ,   is_random = False ,   seed = None ,   name = AdjustBrightness )   See  plx.image.adjust_brightness 's docstring   [source]", 
            "title": "AdjustBrightness"
        }, 
        {
            "location": "/processing/image/#adjustcontrast", 
            "text": "polyaxon . processing . image . AdjustContrast ( mode ,   contrast_factor ,   contrast_factor_max = None ,   is_random = False ,   seed = None ,   name = AdjustContrast )   See  plx.image.adjust_contrast 's docstring   [source]", 
            "title": "AdjustContrast"
        }, 
        {
            "location": "/processing/image/#adjusthue", 
            "text": "polyaxon . processing . image . AdjustHue ( mode ,   delta ,   is_random = False ,   seed = None ,   name = AdjustHue )   See  plx.image.adjust_hue 's docstring   [source]", 
            "title": "AdjustHue"
        }, 
        {
            "location": "/processing/image/#adjustsaturation", 
            "text": "polyaxon . processing . image . AdjustSaturation ( mode ,   saturation_factor ,   saturation_factor_max = None ,   is_random = False ,   seed = None ,   name = AdjustSaturation )   See  plx.image.adjust_saturation 's docstring   [source]", 
            "title": "AdjustSaturation"
        }, 
        {
            "location": "/processing/image/#adjustgamma", 
            "text": "polyaxon . processing . image . AdjustGamma ( mode ,   gamma = 1 ,   gain = 1 ,   name = AdjustGamma )   See  plx.image.adjust_gamma 's docstring   [source]", 
            "title": "AdjustGamma"
        }, 
        {
            "location": "/processing/image/#standardization", 
            "text": "polyaxon . processing . image . Standardization ( mode ,   name = Standardization )   See  plx.image.standardize 's docstring   [source]", 
            "title": "Standardization"
        }, 
        {
            "location": "/processing/image/#drawboundingboxes", 
            "text": "polyaxon . processing . image . DrawBoundingBoxes ( mode ,   boxes ,   name = DrawBoundingBoxes )   See  plx.image.draw_bounding_boxes 's docstring   [source]", 
            "title": "DrawBoundingBoxes"
        }, 
        {
            "location": "/processing/image/#totalvariation", 
            "text": "polyaxon . processing . image . TotalVariation ( mode ,   name = TotalVariation )   See  plx.image.total_variation 's docstring", 
            "title": "TotalVariation"
        }, 
        {
            "location": "/processing/image/#resize_1", 
            "text": "resize ( images ,   height ,   width ,   method = None ,   align_corners = False )   Resize  images  to  size  using the specified  method .\n(A mirror to tf.image resize_images and resize_image_with_crop_or_pad)    If method is None: Resizes an image to a target width and height by either centrally\ncropping the image or padding it evenly with zeros.\nIf  width  or  height  is greater than the specified  target_width  or target_height  respectively, this op centrally crops along that dimension.\nIf  width  or  height  is smaller than the specified  target_width  or target_height  respectively, this op centrally pads with 0 along that dimension.    If method is not None: the resized images will be distorted if their original aspect\nratio is not the same as  size .    method  can be one of:   None  no distortion.  ResizeMethod.BILINEAR :  Bilinear interpolation.  ResizeMethod.NEAREST_NEIGHBOR :  Nearest neighbor interpolation.  ResizeMethod.BICUBIC :  Bicubic interpolation.   ResizeMethod.AREA : Area interpolation.    Args :   images : 4-D Tensor of shape  [batch, height, width, channels]  or\n3-D Tensor of shape  [height, width, channels] .  height : int32 Target height.  width : int32 Target width.  method : ResizeMethod.  Defaults to  ResizeMethod.BILINEAR .\nPossible values: BILINEAR, NEAREST_NEIGHBOR, BICUBIC, AREA  align_corners : bool. If true, exactly align all 4 corners of the input and output.\nOnly used if method is not None. Defaults to  false .     Raises :   ValueError : if the shape of  images  is incompatible with the\nshape arguments to this function.  ValueError : if  size  has invalid shape or type.  ValueError : if an unsupported resize method is specified.     Returns :\n    If  images  was 4-D, a 4-D float Tensor of shape\n     [batch, new_height, new_width, channels] .\n    If  images  was 3-D, a 3-D float Tensor of shape\n     [new_height, new_width, channels] .", 
            "title": "resize"
        }, 
        {
            "location": "/processing/image/#central_crop", 
            "text": "central_crop ( images ,   central_fraction )   Crop the central region of the image.\n(A mirror to tf.image central_crop)  Remove the outer parts of an image but retain the central region of the image\nalong each dimension. If we specify central_fraction = 0.5, this function\nreturns the region marked with \"X\" in the below diagram.   --------\n|........|\n|..XXXX..|\n|..XXXX..|\n|........|   where  X  is the central 50% of the image.\n --------    Args :   images : 4-D Tensor of shape  [batch, height, width, channels]  or\n3-D Tensor of shape  [height, width, channels] .  central_fraction : float (0, 1], fraction of size to crop     Raises :   ValueError : if central_crop_fraction is not within (0, 1].     Returns :\n    If  images  was 4-D, a 4-D float Tensor of shape\n     [batch, new_height, new_width, channels] .\n    If  images  was 3-D, a 3-D float Tensor of shape\n     [new_height, new_width, channels] .", 
            "title": "central_crop"
        }, 
        {
            "location": "/processing/image/#random_crop", 
            "text": "random_crop ( images ,   height ,   width )   Randomly crops an image/images to a given size.    Args :   images : 4-D Tensor of shape  [batch, height, width, channels]  or\n3-D Tensor of shape  [height, width, channels] .  height :  float . The height to crop to.  width :  float . The width to crop to.     Returns :\n    If  images  was 4-D, a 4-D float Tensor of shape\n     [batch, new_height, new_width, channels] .\n    If  images  was 3-D, a 3-D float Tensor of shape\n     [new_height, new_width, channels] .", 
            "title": "random_crop"
        }, 
        {
            "location": "/processing/image/#extract_glimpse", 
            "text": "extract_glimpse ( images ,   size ,   offsets ,   centered = None ,   normalized = None ,   uniform_noise = None ,   name = None )   Extracts a glimpse from the input tensor.\n(A mirror to tf.image extract_glimpse)  Returns a set of windows called glimpses extracted at location  offsets \nfrom the input tensor. If the windows only partially overlaps the inputs,\nthe non overlapping areas will be filled with random noise.  The result is a 4-D tensor of shape  [batch_size, glimpse_height,\nglimpse_width, channels] . The channels and batch dimensions are the\nsame as that of the input tensor. The height and width of the output\nwindows are specified in the  size  parameter.  The argument  normalized  and  centered  controls how the windows are built:  * If the coordinates are normalized but not centered, 0.0 and 1.0\ncorrespond to the minimum and maximum of each height and width\ndimension.\n* If the coordinates are both normalized and centered, they range from -1.0 to 1.0.\nThe coordinates (-1.0, -1.0) correspond to the upper left corner, the lower right\ncorner is located at (1.0, 1.0) and the center is at (0, 0).\n* If the coordinates are not normalized they are interpreted as numbers of pixels.    Args :   images : A  Tensor  of type  float32 .\nA 4-D float tensor of shape  [batch_size, height, width, channels] .  size : A  Tensor  of type  int32 .\nA 1-D tensor of 2 elements containing the size of the glimpses to extract.\nThe glimpse height must be specified first, following by the glimpse width.  offsets : A  Tensor  of type  float32 .\nA 2-D integer tensor of shape  [batch_size, 2]  containing\nthe y, x locations of the center of each window.  centered : An optional  bool . Defaults to  True .\nindicates if the offset coordinates are centered relative to the image,\nin which case the (0, 0) offset is relative to the center of the input images.\nIf false, the (0,0) offset corresponds to the upper left corner of the input images.  normalized : An optional  bool . Defaults to  True .\nindicates if the offset coordinates are normalized.  uniform_noise : An optional  bool . Defaults to  True .\nindicates if the noise should be generated using a\nuniform distribution or a Gaussian distribution.  name : A name for the operation (optional).     Returns :\n    A  Tensor  of type  float32 .\n    A tensor representing the glimpses  [batch_size, glimpse_height, glimpse_width, channels] .", 
            "title": "extract_glimpse"
        }, 
        {
            "location": "/processing/image/#to_bounding_box", 
            "text": "to_bounding_box ( images ,   offset_height ,   offset_width ,   target_height ,   target_width ,   method = crop )   Pad/Crop  image  with zeros to the specified  height  and  width .\n(A mirror to tf.image pad_to_bounding_box and crop_to_bounding_box)  If method == 'pad':\n    Adds  offset_height  rows of zeros on top,  offset_width  columns of\n    zeros on the left, and then pads the image on the bottom and right\n    with zeros until it has dimensions  target_height ,  target_width .  This op does nothing if `offset_*` is zero and the image already has size\n`target_height` by `target_width`.  If method == 'crop':\n    Crops an image to a specified bounding box.  This op cuts a rectangular part out of `image`. The top-left corner of the\nreturned image is at `offset_height, offset_width` in `image`, and its\nlower-right corner is at `offset_height + target_height, offset_width + target_width`.    Args :   images : 4-D Tensor of shape  [batch, height, width, channels]  or\n   3-D Tensor of shape  [height, width, channels] .  offset_height :  pad: Number of rows of zeros to add on top.  crop: Vertical coordinate of the top-left corner of the result the input.  offset_width :  pad: Number of columns of zeros to add on the left.  crop: Horizontal coordinate of the top-left corner of the result in the input.  target_height : Height of output image.  target_width : Width of output image.  method :  crop  or  pad     Returns :\n    If  image  was 4-D, a 4-D float Tensor of shape\n     [batch, target_height, target_width, channels] \n    If  image  was 3-D, a 3-D float Tensor of shape\n     [target_height, target_width, channels]    Raises :   ValueError : If the shape of  image  is incompatible with the  offset_*  or target_*  arguments, or either  offset_height  or  offset_width  is negative.", 
            "title": "to_bounding_box"
        }, 
        {
            "location": "/processing/image/#flip_1", 
            "text": "flip ( images ,   axis = 0 ,   is_random = False ,   seed = None )   Flip (randomly) an image/images.\n(A mirror to tf.image flip_left_right, flip_up_down, random_flip_left_right, and\nrandom_flip_up_down)  if axis is 0:\n    * flip horizontally (left to right)\n if axis is 1:\n    * vertically (upside down).  Outputs the contents of  images  flipped along the given axis.    Args :   images : 4-D Tensor of shape  [batch, height, width, channels]  or\n3-D Tensor of shape  [height, width, channels] .  axis :  int . 0 for horizontal, 1 for vertical  is_random :  bool , If True, flip randomly.  seed : A Python integer. Used to create a random seed. See @{tf.set_random_seed}.     Returns :\n    If  image  was 4-D, a 4-D float Tensor of shape\n     [batch, target_height, target_width, channels] \n    If  image  was 3-D, a 3-D float Tensor of shape\n    `[target_height, target_width, channels]    Raises :   ValueError : if the shape of  image  not supported.", 
            "title": "flip"
        }, 
        {
            "location": "/processing/image/#transpose_1", 
            "text": "transpose ( images )   Transpose an image/images by swapping the first and second dimension.\n(A mirror to tf.image transpose_image)    Args :   images : 4-D Tensor of shape  [batch, height, width, channels]  or\n3-D Tensor of shape  [height, width, channels] .     Returns :\n    If  image  was 4-D, a 4-D float Tensor of shape\n     [batch, target_height, target_width, channels] \n    If  image  was 3-D, a 3-D float Tensor of shape\n    `[target_height, target_width, channels]    Raises :   ValueError : if the shape of  image  not supported.", 
            "title": "transpose"
        }, 
        {
            "location": "/processing/image/#rotate90_1", 
            "text": "rotate90 ( images ,   k = 1 ,   is_random = False ,   seed = None ,   name = None )   Rotate (randomly) images counter-clockwise by 90 degrees.\n(A mirror to tf.image rot90)    Args :   images : 4-D Tensor of shape  [batch, height, width, channels]  or\n3-D Tensor of shape  [height, width, channels] .  k : A scalar integer. The number of times the image is rotated by 90 degrees.  is_random :  bool , If True, adjust randomly.  seed : A Python integer. Used to create a random seed. See @{tf.set_random_seed}.  name : A name for this operation (optional).     Returns :\n    If  image  was 4-D, a 4-D float Tensor of shape\n     [batch, target_height, target_width, channels] \n    If  image  was 3-D, a 3-D float Tensor of shape\n    `[target_height, target_width, channels]    Raises :   ValueError : if the shape of  image  not supported.", 
            "title": "rotate90"
        }, 
        {
            "location": "/processing/image/#convert_color_space", 
            "text": "convert_color_space ( images ,   from_space ,   to_space ,   name = None )   Converts one or more images from RGB to Grayscale.\n(A mirror to tf.image rgb_to_grayscale, rgb_to_hsv, grayscale_to_rgb, and hsv_to_rgb)  Outputs a tensor of the same  DType  and rank as  images .  Possible conversions:\n    * rgb_to_grayscale: The size of the last dimension of the output is 1,\n    containing the Grayscale value of the pixels.\n    * grayscale_to_rgb: The size of the last dimension of the output is 3,\n    containing the RGB value of the pixels.\n    * hsv_to_rgb: The output is only well defined if the value in  images  are in  [0,1] .\n    * rgb_to_hsv: The output is only well defined if the value in  images  are in  [0,1] .\n     output[..., 0]  contains hue,  output[..., 1]  contains saturation, and\n     output[..., 2]  contains value. All HSV values are in  [0,1] . A hue of 0\n    corresponds to pure red, hue 1/3 is pure green, and 2/3 is pure blue.\n    * grayscale_to_hsv: grayscale_to_rgb then rgb_to_hsv\n    * hsv_to_grayscale: hsv_to_rgb then rgb_to_grayscale.    Args :   images : 4-D Tensor of shape  [batch, height, width, channels]  from_space : The color to convert from.  to_space : The color space to convert to.  name : A name for the operation (optional).     Returns :\n    The converted image(s).", 
            "title": "convert_color_space"
        }, 
        {
            "location": "/processing/image/#convert_images_dtype", 
            "text": "convert_images_dtype ( images ,   dtype ,   saturate = False ,   name = None )   Convert image(s) to  dtype , scaling its values if needed.\n(A mirror to tf.image convert_image_dtype)  Images that are represented using floating point values are expected to have\n  values in the range [0,1). Image data stored in integer data types are\n  expected to have values in the range  [0,MAX] , where  MAX  is the largest\n  positive representable number for the data type.  This op converts between data types, scaling the values appropriately before\n  casting.  Note that converting from floating point inputs to integer types may lead to\n  over/underflow problems. Set saturate to  True  to avoid such problem in\n  problematic conversions. If enabled, saturation will clip the output into the\n  allowed range before performing a potentially dangerous cast (and only before\n  performing such a cast, i.e., when casting from a floating point to an integer\n  type, and when casting from a signed to an unsigned type;  saturate  has no\n  effect on casts between floats, or on casts that increase the type's range).    Args :   images : An image.  dtype : A  DType  to convert  image  to.  saturate : If  True , clip the input before casting (if necessary).  name : A name for this operation (optional).     Returns :\n     image , converted to  dtype .", 
            "title": "convert_images_dtype"
        }, 
        {
            "location": "/processing/image/#adjust_brightness", 
            "text": "adjust_brightness ( images ,   delta ,   is_random = False ,   seed = None )   Adjust (randomly) the brightness of RGB or Grayscale images.\n(A mirror to tf.image adjust_brightness, random_birightness)  This is a convenience method that converts an RGB image to float\nrepresentation, adjusts its brightness, and then converts it back to the\noriginal data type. If several adjustments are chained it is advisable to\nminimize the number of redundant conversions.  The value  delta  is added to all components of the tensor  image . Both image  and  delta  are converted to  float  before adding (and  image  is\nscaled appropriately if it is in fixed-point representation). For regular\nimages,  delta  should be in the range  [0,1) , as it is added to the image in\nfloating point representation, where pixel values are in the  [0,1)  range.  If  is_random  is  True , adjust brightness using a value randomly picked in the\ninterval  [-delta, delta) .    Args :   images : A tensor.  delta :  float . Amount to add to the pixel values.  is_random :  bool , If True, adjust randomly.  seed : A Python integer. Used to create a random seed. See @{tf.set_random_seed}.     Returns :\n    A brightness-adjusted tensor of the same shape and type as  images .", 
            "title": "adjust_brightness"
        }, 
        {
            "location": "/processing/image/#adjust_contrast", 
            "text": "adjust_contrast ( images ,   contrast_factor ,   contrast_factor_max = None ,   is_random = False ,   seed = None )   Adjust (randomly) the contrast of RGB or grayscale images by contrast factor.\n(A mirror to tf.image adjust_contrast, random_contrast)  This is a convenience method that converts an RGB image to float\nrepresentation, adjusts its contrast, and then converts it back to the\noriginal data type. If several adjustments are chained it is advisable to\nminimize the number of redundant conversions.  images  is a tensor of at least 3 dimensions.  The last 3 dimensions are\ninterpreted as  [height, width, channels] .  The other dimensions only\nrepresent a collection of images, such as  [batch, height, width, channels].  Contrast is adjusted independently for each channel of each image.  For each channel, this Op computes the mean of the image pixels in the\nchannel and then adjusts each component  x  of each pixel to (x - mean) * contrast_factor + mean .  If  is_random  is  True : Equivalent to  adjust_contrast()  but the value is\n    randomly picked in the interval  [contrast_factor, contrast_factor_max] .    Args :   images :  tensor . images tensor with 3 or more dimensions.  contrast_factor :  float .  Lower bound for the random contrast factor.  contrast_factor_max :  float .  Upper bound for the random contrast factor.\nUsed for random adjustment.  is_random :  bool , If True, adjust randomly.  seed : A Python integer. Used to create a random seed. See @{tf.set_random_seed}.     Returns :\n    The contrast-adjusted tensor.    Raises :   ValueError : if  contrast_factor_max  = contrast_factor \n    if  contrast_factor   0 \n    if  contrast_factor_max  is None (for random.)", 
            "title": "adjust_contrast"
        }, 
        {
            "location": "/processing/image/#adjust_hue", 
            "text": "adjust_hue ( images ,   delta ,   is_random = False ,   seed = None ,   name = None )   Adjust (randomly) hue of an RGB images.\n(A mirror to tf.image adjust_hue, random_hue)  This is a convenience method that converts an RGB image to float\nrepresentation, converts it to HSV, add an offset to the hue channel, converts\nback to RGB and then back to the original data type. If several adjustments\nare chained it is advisable to minimize the number of redundant conversions.  image  is an RGB image.  The image hue is adjusted by converting the\nimage to HSV and rotating the hue channel (H) by  delta .\nThe image is then converted back to RGB.  delta  must be in the interval  [-1, 1] .  If  is_random  is  True  adjust hue but uses a value randomly picked in\nthe interval  [-delta, delta] .    Args :   images : RGB image or images. Size of the last dimension must be 3.  delta : float.  How much to add to the hue channel.  is_random :  bool , If True, adjust randomly.  seed : A Python integer. Used to create a random seed. See @{tf.set_random_seed}.  name : A name for this operation (optional).     Returns :\n    Adjusted image(s), same shape and DType as  image .", 
            "title": "adjust_hue"
        }, 
        {
            "location": "/processing/image/#adjust_saturation", 
            "text": "adjust_saturation ( images ,   saturation_factor ,   saturation_factor_max = None ,   is_random = False ,   seed = None ,   name = None )   Adjust (randomly) saturation of RGB images.\n(A mirror to tf.image adjust_saturation, random_saturation)  This is a convenience method that converts an RGB image to float\nrepresentation, converts it to HSV, add an offset to the saturation channel,\nconverts back to RGB and then back to the original data type. If several\nadjustments are chained it is advisable to minimize the number of redundant\nconversions.  The image saturation is adjusted by converting the images to HSV and\nmultiplying the saturation (S) channel by  saturation_factor  and clipping.\nThe images is then converted back to RGB.  If  is_random  is  True  adjust saturation but uses a value randomly picked in\nthe interval  [saturation_factor, saturation_factor_max] .    Args :   images : RGB image or images. Size of the last dimension must be 3.  saturation_factor : float.  Lower bound for the random saturation factor.  saturation_factor_max : float.  Upper bound for the random saturation factor.  is_random :  bool , If True, adjust randomly.  seed : An operation-specific seed. It will be used in conjunction\n  with the graph-level seed to determine the real seeds that will be\n  used in this operation. Please see the documentation of\n  set_random_seed for its interaction with the graph-level random seed.  name : A name for this operation (optional).     Returns :\n    Adjusted image(s), same shape and DType as  image .    Raises :   ValueError : if  saturation_factor_max  = saturation_factor \n    if  saturation_factor   0 \n    if  saturation_factor_max is None (for random.)", 
            "title": "adjust_saturation"
        }, 
        {
            "location": "/processing/image/#adjust_gamma", 
            "text": "adjust_gamma ( image ,   gamma = 1 ,   gain = 1 )   Performs Gamma Correction on the input image.\nAlso known as Power Law Transform. This function transforms the\ninput image pixelwise according to the equation Out = In**gamma\nafter scaling each pixel to the range 0 to 1.\n(A mirror to tf.image adjust_gamma)    Args :\n    image : A Tensor.\n    gamma : A scalar. Non negative real number.\n    gain  : A scalar. The constant multiplier.    Returns :\n    A Tensor. Gamma corrected output image.    Notes :\n    For gamma greater than 1, the histogram will shift towards left and\n    the output image will be darker than the input image.\n    For gamma less than 1, the histogram will shift towards right and\n    the output image will be brighter than the input image.    References :\n    [1] http://en.wikipedia.org/wiki/Gamma_correction", 
            "title": "adjust_gamma"
        }, 
        {
            "location": "/processing/image/#standardize", 
            "text": "standardize ( images )   Linearly scales  image  to have zero mean and unit norm.\n(A mirror to tf.image per_image_standardization)  This op computes  (x - mean) / adjusted_stddev , where  mean  is the average\nof all values in image, and adjusted_stddev = max(stddev, 1.0/sqrt(image.NumElements())) .  stddev  is the standard deviation of all values in  image . It is capped\naway from zero to protect against division by 0 when handling uniform images.    Args :   images : 4-D Tensor of shape  [batch, height, width, channels]  or\n    3-D Tensor of shape  [height, width, channels] .     Returns :\n    The standardized image with same shape as  image .    Raises :   ValueError : if the shape of 'image' is incompatible with this function.", 
            "title": "standardize"
        }, 
        {
            "location": "/processing/image/#draw_bounding_boxes", 
            "text": "draw_bounding_boxes ( images ,   boxes ,   name = None )   Draw bounding boxes on a batch of images.\n(A mirror to tf.image draw_bounding_boxes)  Outputs a copy of  images  but draws on top of the pixels zero or more bounding\nboxes specified by the locations in  boxes . The coordinates of the each\nbounding box in  boxes  are encoded as  [y_min, x_min, y_max, x_max] . The\nbounding box coordinates are floats in  [0.0, 1.0]  relative to the width and\nheight of the underlying image.  For example, if an image is 100 x 200 pixels and the bounding box is [0.1, 0.2, 0.5, 0.9] , the bottom-left and upper-right coordinates of the\nbounding box will be  (10, 40)  to  (50, 180) .  Parts of the bounding box may fall outside the image.    Args :   images : A  Tensor . Must be one of the following types:  float32 ,  half .\n4-D with shape  [batch, height, width, depth] . A batch of images.  boxes : A  Tensor  of type  float32 .\n3-D with shape  [batch, num_bounding_boxes, 4]  containing bounding boxes.  name : A name for the operation (optional).     Returns :\n    A  Tensor . Has the same type as  images .\n    4-D with the same shape as  images . The batch of input images with\n    bounding boxes drawn on the images.", 
            "title": "draw_bounding_boxes"
        }, 
        {
            "location": "/processing/image/#non_max_suppression", 
            "text": "non_max_suppression ( boxes ,   scores ,   max_output_size ,   iou_threshold = None ,   name = None )   Greedily selects a subset of bounding boxes in descending order of score,\npruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system.  Note that this\nalgorithm is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.  (A mirror to tf.image non_max_suppression)  The output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the  tf.gather operation .  For example:   Examples :     selected_indices   =   tf . image . non_max_suppression (  ...   boxes ,   scores ,   max_output_size ,   iou_threshold )    selected_boxes   =   tf . gather ( boxes ,   selected_indices )     Args :   boxes : A  Tensor  of type  float32 .\nA 2-D float tensor of shape  [num_boxes, 4] .  scores : A  Tensor  of type  float32 .\nA 1-D float tensor of shape  [num_boxes]  representing a single\nscore corresponding to each box (each row of boxes).  max_output_size : A  Tensor  of type  int32 .\nA scalar integer tensor representing the maximum number of\nboxes to be selected by non max suppression.  iou_threshold : An optional  float . Defaults to  0.5 .\nA float representing the threshold for deciding whether boxes\noverlap too much with respect to IOU.  name : A name for the operation (optional).     Returns :\n    A  Tensor  of type  int32 .\n    A 1-D integer tensor of shape  [M]  representing the selected\n    indices from the boxes tensor, where  M  = max_output_size .", 
            "title": "non_max_suppression"
        }, 
        {
            "location": "/processing/image/#sample_distorted_bounding_box", 
            "text": "sample_distorted_bounding_box ( image_size ,   bounding_boxes ,   seed = None ,   seed2 = None ,   min_object_covered = None ,   aspect_ratio_range = None ,   area_range = None ,   max_attempts = None ,   use_image_if_no_bounding_boxes = None ,   name = None )   Generate a single randomly distorted bounding box for an image.  Bounding box annotations are often supplied in addition to ground-truth labels\nin image recognition or object localization tasks. A common technique for\ntraining such a system is to randomly distort an image while preserving\nits content, i.e.  data augmentation . This Op outputs a randomly distorted\nlocalization of an object, i.e. bounding box, given an  image_size , bounding_boxes  and a series of constraints.  (A mirror to tf.image sample_distorted_bounding_box)  The output of this Op is a single bounding box that may be used to crop the\noriginal image. The output is returned as 3 tensors:  begin ,  size  and bboxes . The first 2 tensors can be fed directly into  tf.slice  to crop the\nimage. The latter may be supplied to  tf.image.draw_bounding_boxes  to visualize\nwhat the bounding box looks like.  Bounding boxes are supplied and returned as  [y_min, x_min, y_max, x_max] . The\nbounding box coordinates are floats in  [0.0, 1.0]  relative to the width and\nheight of the underlying image.   Examples :     # Generate a single distorted bounding box.    begin ,   size ,   bbox_for_draw   =   tf . image . sample_distorted_bounding_box (  ...   tf . shape ( image ),   bounding_boxes = bounding_boxes )    # Draw the bounding box in an image summary.    image_with_box   =   tf . image . draw_bounding_boxes (  ...   tf . expand_dims ( image ,   0 ),   bbox_for_draw )    tf . image_summary ( images_with_box ,   image_with_box )    # Employ the bounding box to distort the image.    distorted_image   =   tf . slice ( image ,   begin ,   size )   Note that if no bounding box information is available, setting use_image_if_no_bounding_boxes = true  will assume there is a single implicit\nbounding box covering the whole image. If  use_image_if_no_bounding_boxes  is\nfalse and no bounding boxes are supplied, an error is raised.    Args :   image_size : A  Tensor . Must be one of the following types: uint8 ,  int8 ,  int16 ,  int32 ,  int64 .\n1-D, containing  [height, width, channels] .  bounding_boxes : A  Tensor  of type  float32 .\n3-D with shape  [batch, N, 4]  describing the N bounding boxes\nassociated with the image.  seed : An optional  int . Defaults to  0 .\nIf either  seed  or  seed2  are set to non-zero, the random number\ngenerator is seeded by the given  seed .  Otherwise, it is seeded by a random seed.  seed2 : An optional  int . Defaults to  0 . A second seed to avoid seed collision.  min_object_covered : An optional  float . Defaults to  0.1 .\nThe cropped area of the image must contain at least this\nfraction of any bounding box supplied. The value of this parameter should be\nnon-negative. In the case of 0, the cropped area does not need to overlap\nany of the bounding boxes supplied.  aspect_ratio_range : An optional list of  floats . Defaults to  [0.75, 1.33] .\nThe cropped area of the image must have an aspect ratio =\nwidth / height within this range.  area_range : An optional list of  floats . Defaults to  [0.05, 1] .\nThe cropped area of the image must contain a fraction of the\nsupplied image within in this range.  max_attempts : An optional  int . Defaults to  100 .\nNumber of attempts at generating a cropped region of the image\nof the specified constraints. After  max_attempts  failures, return the entire image.  use_image_if_no_bounding_boxes : An optional  bool . Defaults to  False .\nControls behavior if no bounding boxes supplied.\nIf true, assume an implicit bounding box covering the whole input. If false,\nraise an error.  name : A name for the operation (optional).     Returns :\n    A tuple of  Tensor  objects (begin, size, bboxes).   begin : A  Tensor . Has the same type as  image_size . 1-D, containing [offset_height, offset_width, 0] . Provide as input to  tf.slice .  size : A  Tensor . Has the same type as  image_size . 1-D, containing [target_height, target_width, -1] . Provide as input to  tf.slice .  bboxes : A  Tensor  of type  float32 . 3-D with shape  [1, 1, 4]  containing\nthe distorted bounding box. Provide as input to  tf.image.draw_bounding_boxes .", 
            "title": "sample_distorted_bounding_box"
        }, 
        {
            "location": "/processing/image/#total_variation", 
            "text": "total_variation ( images ,   name = None )   Calculate and return the total variation for one or more images.  (A mirror to tf.image total_variation)  The total variation is the sum of the absolute differences for neighboring\npixel-values in the input images. This measures how much noise is in the\nimages.  This can be used as a loss-function during optimization so as to suppress\nnoise in images. If you have a batch of images, then you should calculate\nthe scalar loss-value as the sum: loss = tf.reduce_sum(tf.image.total_variation(images))  This implements the anisotropic 2-D version of the formula described here:    https ://en.wikipedia.org/wiki/Total_variation_denoising    Args :    images : 4-D Tensor of shape  [batch, height, width, channels]  or\n    3-D Tensor of shape  [height, width, channels] .    name : A name for the operation (optional).      Raises :   ValueError : if images.shape is not a 3-D or 4-D vector.     Returns :\n    The total variation of  images .  If  images  was 4-D, return a 1-D float Tensor of shape  [batch]  with the\ntotal variation for each image in the batch.\nIf  images  was 3-D, return a scalar float with the total variation for\nthat image.", 
            "title": "total_variation"
        }, 
        {
            "location": "/processing/input_fn/", 
            "text": "create_input_data_fn\n\n\ncreate_input_data_fn\n(\nmode\n,\n \npipeline_config\n,\n \nscope\n=\nNone\n,\n \ninput_type\n=\nNone\n,\n \nx\n=\nNone\n,\n \ny\n=\nNone\n)\n\n\n\n\n\n\nCreates an input data function that can be used with estimators.\nNote that you must pass \"factory functions\" for both the data provider and\nfeaturizer to ensure that everything will be created in  the same graph.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\npipeline_config\n: the configuration to create a Pipeline instance.\n\n\nscope\n: \nstr\n. scope to use for this input data block.\n\n\ninput_type\n: \nstr\n. The type of the input, values: \nNUMPY\n, \nPANDAS\n.\n    If \nNone\n, will create a function based on the pipeline config.\n\n\nx\n: \nnp.ndarray\n or \nnp.Dataframe\n or \nNone\n.\n\n\ny\n: \nnp.ndarray\n or \nNone\n.\n\n\n\n\n\n\n\n\nReturns\n:\n    An input function that returns \n(feature_batch, labels_batch)\n\n    tuples when called.", 
            "title": "Input function"
        }, 
        {
            "location": "/processing/input_fn/#create_input_data_fn", 
            "text": "create_input_data_fn ( mode ,   pipeline_config ,   scope = None ,   input_type = None ,   x = None ,   y = None )   Creates an input data function that can be used with estimators.\nNote that you must pass \"factory functions\" for both the data provider and\nfeaturizer to ensure that everything will be created in  the same graph.    Args :   mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  pipeline_config : the configuration to create a Pipeline instance.  scope :  str . scope to use for this input data block.  input_type :  str . The type of the input, values:  NUMPY ,  PANDAS .\n    If  None , will create a function based on the pipeline config.  x :  np.ndarray  or  np.Dataframe  or  None .  y :  np.ndarray  or  None .     Returns :\n    An input function that returns  (feature_batch, labels_batch) \n    tuples when called.", 
            "title": "create_input_data_fn"
        }, 
        {
            "location": "/processing/pipelines/", 
            "text": "[source]\n\n\nPipeline\n\n\npolyaxon\n.\nprocessing\n.\npipelines\n.\nPipeline\n(\nmode\n,\n \nname\n=\nPipeline\n,\n \nsubgraphs_by_features\n=\nNone\n,\n \nshuffle\n=\nTrue\n,\n \nnum_epochs\n=\nNone\n)\n\n\n\n\n\n\nAbstract InputPipeline class. All input pipelines must inherit from this.\nAn InputPipeline defines how data is read, parsed, and separated into\nfeatures and labels.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nname\n: \nstr\n, name to give for this pipeline.\n\n\nsubgraphs_by_features\n: \ndict\n, list of modules to call for each feature to be processed.\n\n\nshuffle\n: If true, shuffle the data.\n\n\nnum_epochs\n: Number of times to iterate through the dataset. If None, iterate forever.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nTFRecordImagePipeline\n\n\npolyaxon\n.\nprocessing\n.\npipelines\n.\nTFRecordImagePipeline\n(\nmode\n,\n \nname\n=\nTFRecordImagePipeline\n,\n \nsubgraphs_by_features\n=\nNone\n,\n \nshuffle\n=\nTrue\n,\n \nnum_epochs\n=\nNone\n,\n \ndata_files\n=\nNone\n,\n \nmeta_data_file\n=\nNone\n)\n\n\n\n\n\n\nAbstract InputPipeline class. All input pipelines must inherit from this.\nAn InputPipeline defines how data is read, parsed, and separated into\nfeatures and labels.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nname\n: \nstr\n, name to give for this pipeline.\n\n\nsubgraphs_by_features\n: \ndict\n, list of modules to call for each feature to be processed\n\n\nshuffle\n: If true, shuffle the data.\n\n\nnum_epochs\n: Number of times to iterate through the dataset. If None, iterate forever.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nParallelTextPipeline\n\n\npolyaxon\n.\nprocessing\n.\npipelines\n.\nParallelTextPipeline\n(\nmode\n,\n \nname\n=\nParallelTextPipeline\n,\n \nsubgraphs_by_features\n=\nNone\n,\n \nshuffle\n=\nTrue\n,\n \nnum_epochs\n=\nNone\n,\n \nsource_files\n=\nNone\n,\n \ntarget_files\n=\nNone\n,\n \nsource_delimiter\n=\n,\n \ntarget_delimiter\n=\n)\n\n\n\n\n\n\nAn input pipeline that reads two parallel (line-by-line aligned) text files.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nname\n: \nstr\n, name to give for this pipeline.\n\n\nsubgraphs_by_features\n: \ndict\n, list of modules to call for each feature to be processed\n\n\nshuffle\n: If true, shuffle the data.\n\n\nnum_epochs\n: Number of times to iterate through the dataset. If None, iterate forever.\n\n\nsource_files\n: An array of file names for the source data.\n\n\ntarget_files\n: An array of file names for the target data. These must\n  be aligned to the \nsource_files\n.\n\n\nsource_delimiter\n: A character to split the source text on. Defaults\n  to  \" \" (space). For character-level training this can be set to the\n  empty string.\n\n\ntarget_delimiter\n: Same as \nsource_delimiter\n but for the target text.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nTFRecordSourceSequencePipeline\n\n\npolyaxon\n.\nprocessing\n.\npipelines\n.\nTFRecordSourceSequencePipeline\n(\nmode\n,\n \nname\n=\nTFRecordSourceSequencePipeline\n,\n \nsubgraphs_by_features\n=\nNone\n,\n \nshuffle\n=\nTrue\n,\n \nnum_epochs\n=\nNone\n,\n \nfiles\n=\nNone\n,\n \nsource_field\n=\nsource\n,\n \ntarget_field\n=\ntarget\n,\n \nsource_delimiter\n=\n,\n \ntarget_delimiter\n=\n)\n\n\n\n\n\n\nAn input pipeline that reads a TFRecords containing both source and target sequences.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nname\n: \nstr\n, name to give for this pipeline.\n\n\nsubgraphs_by_features\n: \ndict\n, list of modules to call for each feature to be processed\n\n\nshuffle\n: If true, shuffle the data.\n\n\nnum_epochs\n: Number of times to iterate through the dataset. If None, iterate forever.\n\n\nfiles\n: An array of file names to read from.\n\n\nsource_field\n: The TFRecord feature field containing the source text.\n\n\ntarget_field\n: The TFRecord feature field containing the target text.\n\n\nsource_delimiter\n: A character to split the source text on. Defaults\n  to  \" \" (space). For character-level training this can be set to the\n  empty string.\n\n\ntarget_delimiter\n: Same as \nsource_delimiter\n but for the target text.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nImageCaptioningPipeline\n\n\npolyaxon\n.\nprocessing\n.\npipelines\n.\nImageCaptioningPipeline\n(\nmode\n,\n \nname\n=\nImageCaptioningPipeline\n,\n \nsubgraphs_by_features\n=\nNone\n,\n \nshuffle\n=\nTrue\n,\n \nnum_epochs\n=\nNone\n,\n \nfiles\n=\nNone\n,\n \nimage_field\n=\nimage/data\n,\n \nimage_format\n=\njpg\n,\n \ncaption_ids_field\n=\nimage/caption_ids\n,\n \ncaption_tokens_field\n=\nimage/caption\n)\n\n\n\n\n\n\nAn input pipeline that reads a TFRecords containing both source and target sequences.\n\n\n\n\nArgs\n:\n\n\nmode\n: \nstr\n, Specifies if this training, evaluation or prediction. See \nModeKeys\n.\n\n\nname\n: \nstr\n, name to give for this pipeline.\n\n\nsubgraphs_by_features\n: \ndict\n, list of modules to call for each feature to be processed\n\n\nshuffle\n: If true, shuffle the data.\n\n\nnum_epochs\n: Number of times to iterate through the dataset. If None, iterate forever.\n\n\nfiles\n: An array of file names to read from.\n\n\nimage_field\n: The TFRecord feature field containing the source images.\n\n\nimage_format\n: The images extensions.\n\n\ncaption_ids_field\n: The caption ids field.\n\n\ncaption_tokens_field\n: the caption tokends field.", 
            "title": "Pipelines"
        }, 
        {
            "location": "/processing/pipelines/#pipeline", 
            "text": "polyaxon . processing . pipelines . Pipeline ( mode ,   name = Pipeline ,   subgraphs_by_features = None ,   shuffle = True ,   num_epochs = None )   Abstract InputPipeline class. All input pipelines must inherit from this.\nAn InputPipeline defines how data is read, parsed, and separated into\nfeatures and labels.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  name :  str , name to give for this pipeline.  subgraphs_by_features :  dict , list of modules to call for each feature to be processed.  shuffle : If true, shuffle the data.  num_epochs : Number of times to iterate through the dataset. If None, iterate forever.      [source]", 
            "title": "Pipeline"
        }, 
        {
            "location": "/processing/pipelines/#tfrecordimagepipeline", 
            "text": "polyaxon . processing . pipelines . TFRecordImagePipeline ( mode ,   name = TFRecordImagePipeline ,   subgraphs_by_features = None ,   shuffle = True ,   num_epochs = None ,   data_files = None ,   meta_data_file = None )   Abstract InputPipeline class. All input pipelines must inherit from this.\nAn InputPipeline defines how data is read, parsed, and separated into\nfeatures and labels.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  name :  str , name to give for this pipeline.  subgraphs_by_features :  dict , list of modules to call for each feature to be processed  shuffle : If true, shuffle the data.  num_epochs : Number of times to iterate through the dataset. If None, iterate forever.      [source]", 
            "title": "TFRecordImagePipeline"
        }, 
        {
            "location": "/processing/pipelines/#paralleltextpipeline", 
            "text": "polyaxon . processing . pipelines . ParallelTextPipeline ( mode ,   name = ParallelTextPipeline ,   subgraphs_by_features = None ,   shuffle = True ,   num_epochs = None ,   source_files = None ,   target_files = None ,   source_delimiter = ,   target_delimiter = )   An input pipeline that reads two parallel (line-by-line aligned) text files.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  name :  str , name to give for this pipeline.  subgraphs_by_features :  dict , list of modules to call for each feature to be processed  shuffle : If true, shuffle the data.  num_epochs : Number of times to iterate through the dataset. If None, iterate forever.  source_files : An array of file names for the source data.  target_files : An array of file names for the target data. These must\n  be aligned to the  source_files .  source_delimiter : A character to split the source text on. Defaults\n  to  \" \" (space). For character-level training this can be set to the\n  empty string.  target_delimiter : Same as  source_delimiter  but for the target text.      [source]", 
            "title": "ParallelTextPipeline"
        }, 
        {
            "location": "/processing/pipelines/#tfrecordsourcesequencepipeline", 
            "text": "polyaxon . processing . pipelines . TFRecordSourceSequencePipeline ( mode ,   name = TFRecordSourceSequencePipeline ,   subgraphs_by_features = None ,   shuffle = True ,   num_epochs = None ,   files = None ,   source_field = source ,   target_field = target ,   source_delimiter = ,   target_delimiter = )   An input pipeline that reads a TFRecords containing both source and target sequences.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  name :  str , name to give for this pipeline.  subgraphs_by_features :  dict , list of modules to call for each feature to be processed  shuffle : If true, shuffle the data.  num_epochs : Number of times to iterate through the dataset. If None, iterate forever.  files : An array of file names to read from.  source_field : The TFRecord feature field containing the source text.  target_field : The TFRecord feature field containing the target text.  source_delimiter : A character to split the source text on. Defaults\n  to  \" \" (space). For character-level training this can be set to the\n  empty string.  target_delimiter : Same as  source_delimiter  but for the target text.      [source]", 
            "title": "TFRecordSourceSequencePipeline"
        }, 
        {
            "location": "/processing/pipelines/#imagecaptioningpipeline", 
            "text": "polyaxon . processing . pipelines . ImageCaptioningPipeline ( mode ,   name = ImageCaptioningPipeline ,   subgraphs_by_features = None ,   shuffle = True ,   num_epochs = None ,   files = None ,   image_field = image/data ,   image_format = jpg ,   caption_ids_field = image/caption_ids ,   caption_tokens_field = image/caption )   An input pipeline that reads a TFRecords containing both source and target sequences.   Args :  mode :  str , Specifies if this training, evaluation or prediction. See  ModeKeys .  name :  str , name to give for this pipeline.  subgraphs_by_features :  dict , list of modules to call for each feature to be processed  shuffle : If true, shuffle the data.  num_epochs : Number of times to iterate through the dataset. If None, iterate forever.  files : An array of file names to read from.  image_field : The TFRecord feature field containing the source images.  image_format : The images extensions.  caption_ids_field : The caption ids field.  caption_tokens_field : the caption tokends field.", 
            "title": "ImageCaptioningPipeline"
        }, 
        {
            "location": "/processing/data_decoders/", 
            "text": "[source]\n\n\nDataDecoder\n\n\npolyaxon\n.\nprocessing\n.\ndata_decoders\n.\nDataDecoder\n()\n\n\n\n\n\n\nAn abstract class which is used to decode data for a provider.\n\n\n(A mirror to tf.slim.data DataDecoder)\n\n\n\n\n[source]\n\n\nTFExampleDecoder\n\n\npolyaxon\n.\nprocessing\n.\ndata_decoders\n.\nTFExampleDecoder\n(\nkeys_to_features\n,\n \nitems_to_handlers\n)\n\n\n\n\n\n\nA decoder for TensorFlow Examples.\n(A mirror to tf.slim.data TFExampleDecoder)\n\n\nDecoding Example proto buffers is comprised of two stages: (1) Example parsing\nand (2) tensor manipulation.\n\n\nIn the first stage, the tf.parse_example function is called with a list of\nFixedLenFeatures and SparseLenFeatures. These instances tell TF how to parse\nthe example. The output of this stage is a set of tensors.\n\n\nIn the second stage, the resulting tensors are manipulated to provide the\nrequested 'item' tensors.\n\n\nTo perform this decoding operation, an ExampleDecoder is given a list of\nItemHandlers. Each ItemHandler indicates the set of features for stage 1 and\ncontains the instructions for post_processing its tensors for stage 2.\n\n\n\n\n[source]\n\n\nSplitTokensDecoder\n\n\npolyaxon\n.\nprocessing\n.\ndata_decoders\n.\nSplitTokensDecoder\n(\ndelimiter\n=\n \n,\n \ntokens_feature_name\n=\ntokens\n,\n \nlength_feature_name\n=\nlength\n,\n \nprepend_token\n=\nNone\n,\n \nappend_token\n=\nNone\n)\n\n\n\n\n\n\nA DataDecoder that splits a string tensor into individual tokens and\nreturns the tokens and the length.\nOptionally prepends or appends special tokens.\n\n\n\n\nArgs\n:\n\n\ndelimiter\n: Delimiter to split on. Must be a single character.\n\n\ntokens_feature_name\n: A descriptive feature name for the token values\n\n\nlength_feature_name\n: A descriptive feature name for the length value\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nTFSequenceExampleDecoder\n\n\npolyaxon\n.\nprocessing\n.\ndata_decoders\n.\nTFSequenceExampleDecoder\n(\ncontext_keys_to_features\n,\n \nsequence_keys_to_features\n,\n \nitems_to_handlers\n)\n\n\n\n\n\n\nA decoder for TensorFlow Examples.\nDecoding Example proto buffers is comprised of two stages: (1) Example parsing\nand (2) tensor manipulation.\nIn the first stage, the tf.parse_example function is called with a list of\nFixedLenFeatures and SparseLenFeatures. These instances tell TF how to parse\nthe example. The output of this stage is a set of tensors.\nIn the second stage, the resulting tensors are manipulated to provide the\nrequested 'item' tensors.\nTo perform this decoding operation, an ExampleDecoder is given a list of\nItemHandlers. Each ItemHandler indicates the set of features for stage 1 and\ncontains the instructions for post_processing its tensors for stage 2.", 
            "title": "Data Decoders"
        }, 
        {
            "location": "/processing/data_decoders/#datadecoder", 
            "text": "polyaxon . processing . data_decoders . DataDecoder ()   An abstract class which is used to decode data for a provider.  (A mirror to tf.slim.data DataDecoder)   [source]", 
            "title": "DataDecoder"
        }, 
        {
            "location": "/processing/data_decoders/#tfexampledecoder", 
            "text": "polyaxon . processing . data_decoders . TFExampleDecoder ( keys_to_features ,   items_to_handlers )   A decoder for TensorFlow Examples.\n(A mirror to tf.slim.data TFExampleDecoder)  Decoding Example proto buffers is comprised of two stages: (1) Example parsing\nand (2) tensor manipulation.  In the first stage, the tf.parse_example function is called with a list of\nFixedLenFeatures and SparseLenFeatures. These instances tell TF how to parse\nthe example. The output of this stage is a set of tensors.  In the second stage, the resulting tensors are manipulated to provide the\nrequested 'item' tensors.  To perform this decoding operation, an ExampleDecoder is given a list of\nItemHandlers. Each ItemHandler indicates the set of features for stage 1 and\ncontains the instructions for post_processing its tensors for stage 2.   [source]", 
            "title": "TFExampleDecoder"
        }, 
        {
            "location": "/processing/data_decoders/#splittokensdecoder", 
            "text": "polyaxon . processing . data_decoders . SplitTokensDecoder ( delimiter =   ,   tokens_feature_name = tokens ,   length_feature_name = length ,   prepend_token = None ,   append_token = None )   A DataDecoder that splits a string tensor into individual tokens and\nreturns the tokens and the length.\nOptionally prepends or appends special tokens.   Args :  delimiter : Delimiter to split on. Must be a single character.  tokens_feature_name : A descriptive feature name for the token values  length_feature_name : A descriptive feature name for the length value      [source]", 
            "title": "SplitTokensDecoder"
        }, 
        {
            "location": "/processing/data_decoders/#tfsequenceexampledecoder", 
            "text": "polyaxon . processing . data_decoders . TFSequenceExampleDecoder ( context_keys_to_features ,   sequence_keys_to_features ,   items_to_handlers )   A decoder for TensorFlow Examples.\nDecoding Example proto buffers is comprised of two stages: (1) Example parsing\nand (2) tensor manipulation.\nIn the first stage, the tf.parse_example function is called with a list of\nFixedLenFeatures and SparseLenFeatures. These instances tell TF how to parse\nthe example. The output of this stage is a set of tensors.\nIn the second stage, the resulting tensors are manipulated to provide the\nrequested 'item' tensors.\nTo perform this decoding operation, an ExampleDecoder is given a list of\nItemHandlers. Each ItemHandler indicates the set of features for stage 1 and\ncontains the instructions for post_processing its tensors for stage 2.", 
            "title": "TFSequenceExampleDecoder"
        }, 
        {
            "location": "/processing/data_providers/", 
            "text": "[source]\n\n\nDataset\n\n\npolyaxon\n.\nprocessing\n.\ndata_providers\n.\nDataset\n(\ndata_sources\n,\n \nreader\n,\n \ndecoder\n,\n \nnum_samples\n=\nNone\n,\n \nitems_to_descriptions\n=\nNone\n,\n \nmeta_data\n=\nNone\n)\n\n\n\n\n\n\nRepresents a Dataset specification.\n\n\n\n\nArgs\n:\n\n\ndata_sources\n: A list of files that make up the dataset.\n\n\nreader\n: The reader class, a subclass of BaseReader, e.g. TFRecordReader.\n\n\ndecoder\n: An instance of a data_decoder.\n\n\nnum_samples\n: The number of samples in the dataset.\n\n\nitems_to_descriptions\n: A map from the items that the dataset provides to\n    the descriptions of those items.\n\n\nmeta_data\n: extra information about the current dataset, e.g. num_samples, channels ...\n    Generally read from the meta_data.json file\n\n\n**kwargs\n: Any remaining dataset-specific fields.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nDataProvider\n\n\npolyaxon\n.\nprocessing\n.\ndata_providers\n.\nDataProvider\n(\nitems_to_tensors\n,\n \nnum_samples\n)\n\n\n\n\n\n\nMaps a list of requested data items to tensors from a data source.\n(A mirror to tf.slim.data DataProvider)\n\n\nAll data providers must inherit from DataProvider and implement the Get\nmethod which returns arbitrary types of data. No assumption is made about the\nsource of the data nor the mechanism for providing it.\n\n\n\n\nArgs\n:\n\n\nitems_to_tensors\n: a dictionary of names to tensors.\n\n\nnum_samples\n: the number of samples in the dataset being provided.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nDatasetDataProvider\n\n\npolyaxon\n.\nprocessing\n.\ndata_providers\n.\nDatasetDataProvider\n(\ndataset\n,\n \nnum_readers\n=\n1\n,\n \nreader_kwargs\n=\nNone\n,\n \nshuffle\n=\nTrue\n,\n \nnum_epochs\n=\nNone\n,\n \ncommon_queue_capacity\n=\n256\n,\n \ncommon_queue_min\n=\n128\n,\n \nrecord_key\n=\n__record_key__\n,\n \nseed\n=\nNone\n,\n \nscope\n=\nNone\n)\n\n\n\n\n\n\nCreates a DatasetDataProvider.\n\n\n\n\nArgs\n:\n\n\ndataset\n: An instance of the Dataset class.\n\n\nnum_readers\n: The number of parallel readers to use.\n\n\nreader_kwargs\n: An optional dict of kwargs for the reader.\n\n\nshuffle\n: Whether to shuffle the data sources and common queue when reading.\n\n\nnum_epochs\n: The number of times each data source is read. If left as None,\n    the data will be cycled through indefinitely.\n\n\ncommon_queue_capacity\n: The capacity of the common queue.\n\n\ncommon_queue_min\n: The minimum number of elements in the common queue after a dequeue.\n\n\nrecord_key\n: The item name to use for the dataset record keys in the provided tensors.\n\n\nseed\n: The seed to use if shuffling.\n\n\nscope\n: Optional name scope for the ops.\n\n\n\n\n\n\nRaises\n:\n\n\nValueError\n: If \nrecord_key\n matches one of the items in the dataset.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nParallelDatasetProvider\n\n\npolyaxon\n.\nprocessing\n.\ndata_providers\n.\nParallelDatasetProvider\n(\ndataset_source\n,\n \ndataset_target\n,\n \nshuffle\n=\nTrue\n,\n \nnum_epochs\n=\nNone\n,\n \ncommon_queue_capacity\n=\n4096\n,\n \ncommon_queue_min\n=\n1024\n,\n \nseed\n=\nNone\n)\n\n\n\n\n\n\nCreates a ParallelDatasetProvider. This data provider reads two datasets\nin parallel, keeping them aligned.\n\n\n\n\nArgs\n:\n\n\ndataset_source\n: The first dataset. An instance of the Dataset class.\n\n\ndataset_target\n: The second dataset. An instance of the Dataset class.\n    Can be None. If None, only \ndataset1\n is read.\n\n\nshuffle\n: Whether to shuffle the data sources and common queue when\n  reading.\n\n\nnum_epochs\n: The number of times each data source is read. If left as None,\n  the data will be cycled through indefinitely.\n\n\ncommon_queue_capacity\n: The capacity of the common queue.\n\n\ncommon_queue_min\n: The minimum number of elements in the common queue after a dequeue.\n\n\nseed\n: The seed to use if shuffling.", 
            "title": "Data Providers"
        }, 
        {
            "location": "/processing/data_providers/#dataset", 
            "text": "polyaxon . processing . data_providers . Dataset ( data_sources ,   reader ,   decoder ,   num_samples = None ,   items_to_descriptions = None ,   meta_data = None )   Represents a Dataset specification.   Args :  data_sources : A list of files that make up the dataset.  reader : The reader class, a subclass of BaseReader, e.g. TFRecordReader.  decoder : An instance of a data_decoder.  num_samples : The number of samples in the dataset.  items_to_descriptions : A map from the items that the dataset provides to\n    the descriptions of those items.  meta_data : extra information about the current dataset, e.g. num_samples, channels ...\n    Generally read from the meta_data.json file  **kwargs : Any remaining dataset-specific fields.      [source]", 
            "title": "Dataset"
        }, 
        {
            "location": "/processing/data_providers/#dataprovider", 
            "text": "polyaxon . processing . data_providers . DataProvider ( items_to_tensors ,   num_samples )   Maps a list of requested data items to tensors from a data source.\n(A mirror to tf.slim.data DataProvider)  All data providers must inherit from DataProvider and implement the Get\nmethod which returns arbitrary types of data. No assumption is made about the\nsource of the data nor the mechanism for providing it.   Args :  items_to_tensors : a dictionary of names to tensors.  num_samples : the number of samples in the dataset being provided.      [source]", 
            "title": "DataProvider"
        }, 
        {
            "location": "/processing/data_providers/#datasetdataprovider", 
            "text": "polyaxon . processing . data_providers . DatasetDataProvider ( dataset ,   num_readers = 1 ,   reader_kwargs = None ,   shuffle = True ,   num_epochs = None ,   common_queue_capacity = 256 ,   common_queue_min = 128 ,   record_key = __record_key__ ,   seed = None ,   scope = None )   Creates a DatasetDataProvider.   Args :  dataset : An instance of the Dataset class.  num_readers : The number of parallel readers to use.  reader_kwargs : An optional dict of kwargs for the reader.  shuffle : Whether to shuffle the data sources and common queue when reading.  num_epochs : The number of times each data source is read. If left as None,\n    the data will be cycled through indefinitely.  common_queue_capacity : The capacity of the common queue.  common_queue_min : The minimum number of elements in the common queue after a dequeue.  record_key : The item name to use for the dataset record keys in the provided tensors.  seed : The seed to use if shuffling.  scope : Optional name scope for the ops.    Raises :  ValueError : If  record_key  matches one of the items in the dataset.      [source]", 
            "title": "DatasetDataProvider"
        }, 
        {
            "location": "/processing/data_providers/#paralleldatasetprovider", 
            "text": "polyaxon . processing . data_providers . ParallelDatasetProvider ( dataset_source ,   dataset_target ,   shuffle = True ,   num_epochs = None ,   common_queue_capacity = 4096 ,   common_queue_min = 1024 ,   seed = None )   Creates a ParallelDatasetProvider. This data provider reads two datasets\nin parallel, keeping them aligned.   Args :  dataset_source : The first dataset. An instance of the Dataset class.  dataset_target : The second dataset. An instance of the Dataset class.\n    Can be None. If None, only  dataset1  is read.  shuffle : Whether to shuffle the data sources and common queue when\n  reading.  num_epochs : The number of times each data source is read. If left as None,\n  the data will be cycled through indefinitely.  common_queue_capacity : The capacity of the common queue.  common_queue_min : The minimum number of elements in the common queue after a dequeue.  seed : The seed to use if shuffling.", 
            "title": "ParallelDatasetProvider"
        }, 
        {
            "location": "/activations/", 
            "text": "built_activation\n\n\nbuilt_activation\n(\nfct\n,\n \nname\n,\n \ncollect\n)\n\n\n\n\n\n\nBuilds the metric function.\n\n\n\n\nArgs\n:\n\n\nfct\n: the activation function to build.\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nlinear\n\n\nlinear\n(\nname\n=\nLinear\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes linear/identity function.\n\n\n\n\nArgs\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\ntanh\n\n\ntanh\n(\nname\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes hyperbolic tangent of x element-wise.\n\n\n\n\nArgs\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nsigmoid\n\n\nsigmoid\n(\nname\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes sigmoid of \nx\n element-wise: \ny = 1 / (1 + exp(-x))\n.\n\n\n\n\nArgs\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nsoftmax\n\n\nsoftmax\n(\nname\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes softmax activations.\n\n\nFor each batch \ni\n and class \nj\n we have\n    \nsoftmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))\n\n\n\n\nArgs\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nsoftplus\n\n\nsoftplus\n(\nname\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes softplus. \nlog(exp(features) + 1)\n.\n\n\n\n\nArgs\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nsoftsign\n\n\nsoftsign\n(\nname\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes softsign: \nfeatures / (abs(features) + 1)\n.\n\n\n\n\nArgs\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nrelu\n\n\nrelu\n(\nname\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes ReLU, rectified linear: \nmax(features, 0)\n.\n\n\n\n\nArgs\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nrelu6\n\n\nrelu6\n(\nname\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes Rectified Linear 6: \nmin(max(features, 0), 6)\n.\n\n\n\n\nArgs\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nleaky_relu\n\n\nleaky_relu\n(\nalpha\n=\n0.1\n,\n \nname\n=\nLeakyReLU\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nModified version of ReLU, introducing a nonzero gradient for negative input.\n\n\n\n\nArgs\n:\n\n\nalpha\n: \nint\n, the multiplier.\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nprelu\n\n\nprelu\n(\nchannel_shared\n=\nFalse\n,\n \nweights_init\n=\nzeros\n,\n \nrestore\n=\nTrue\n,\n \nname\n=\nPReLU\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nParametric Rectified Linear Unit.\n\n\n\n\nArgs\n:\n\n\nchannel_shared\n:\n\n\nweights_init\n:\n\n\nrestore\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nelu\n\n\nelu\n(\nname\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes Exponential Linear Unit.\n\n\n\n\nArgs\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\ncrelu\n\n\ncrelu\n(\nname\n=\nCRelu\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes Concatenated ReLU.\n\n\n\n\nArgs\n:\n\n\nname\n: operation name.\n\n\ncollect\n: whether to collect this metric under the metric collection.", 
            "title": "Activations"
        }, 
        {
            "location": "/activations/#built_activation", 
            "text": "built_activation ( fct ,   name ,   collect )   Builds the metric function.   Args :  fct : the activation function to build.  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "built_activation"
        }, 
        {
            "location": "/activations/#linear", 
            "text": "linear ( name = Linear ,   collect = False )   Computes linear/identity function.   Args :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "linear"
        }, 
        {
            "location": "/activations/#tanh", 
            "text": "tanh ( name = None ,   collect = False )   Computes hyperbolic tangent of x element-wise.   Args :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "tanh"
        }, 
        {
            "location": "/activations/#sigmoid", 
            "text": "sigmoid ( name = None ,   collect = False )   Computes sigmoid of  x  element-wise:  y = 1 / (1 + exp(-x)) .   Args :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "sigmoid"
        }, 
        {
            "location": "/activations/#softmax", 
            "text": "softmax ( name = None ,   collect = False )   Computes softmax activations.  For each batch  i  and class  j  we have\n     softmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))   Args :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "softmax"
        }, 
        {
            "location": "/activations/#softplus", 
            "text": "softplus ( name = None ,   collect = False )   Computes softplus.  log(exp(features) + 1) .   Args :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "softplus"
        }, 
        {
            "location": "/activations/#softsign", 
            "text": "softsign ( name = None ,   collect = False )   Computes softsign:  features / (abs(features) + 1) .   Args :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "softsign"
        }, 
        {
            "location": "/activations/#relu", 
            "text": "relu ( name = None ,   collect = False )   Computes ReLU, rectified linear:  max(features, 0) .   Args :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "relu"
        }, 
        {
            "location": "/activations/#relu6", 
            "text": "relu6 ( name = None ,   collect = False )   Computes Rectified Linear 6:  min(max(features, 0), 6) .   Args :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "relu6"
        }, 
        {
            "location": "/activations/#leaky_relu", 
            "text": "leaky_relu ( alpha = 0.1 ,   name = LeakyReLU ,   collect = False )   Modified version of ReLU, introducing a nonzero gradient for negative input.   Args :  alpha :  int , the multiplier.  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "leaky_relu"
        }, 
        {
            "location": "/activations/#prelu", 
            "text": "prelu ( channel_shared = False ,   weights_init = zeros ,   restore = True ,   name = PReLU ,   collect = False )   Parametric Rectified Linear Unit.   Args :  channel_shared :  weights_init :  restore :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "prelu"
        }, 
        {
            "location": "/activations/#elu", 
            "text": "elu ( name = None ,   collect = False )   Computes Exponential Linear Unit.   Args :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "elu"
        }, 
        {
            "location": "/activations/#crelu", 
            "text": "crelu ( name = CRelu ,   collect = False )   Computes Concatenated ReLU.   Args :  name : operation name.  collect : whether to collect this metric under the metric collection.", 
            "title": "crelu"
        }, 
        {
            "location": "/losses/", 
            "text": "absolute_difference\n\n\nabsolute_difference\n(\nweights\n=\n1.0\n,\n \nname\n=\nAbsoluteDifference\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nTrue\n)\n\n\n\n\n\n\nAdds an Absolute Difference loss to the training procedure.\n\n\nweights\n acts as a coefficient for the loss. If a scalar is provided, then\nthe loss is simply scaled by the given value. If \nweights\n is a \nTensor\n of\nshape \n[batch_size]\n, then the total loss for each sample of the batch is\nrescaled by the corresponding element in the \nweights\n vector. If the shape of\n\nweights\n matches the shape of \npredictions\n, then the loss of each\nmeasurable element of \npredictions\n is scaled by the corresponding value of\n\nweights\n.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nweights\n: Optional \nTensor\n whose rank is either 0, or the same rank as\n\nlabels\n, and must be broadcastable to \nlabels\n (i.e., all dimensions must\nbe either \n1\n, or the same as the corresponding \nlosses\n dimension).\n\n\nname\n: operation name.\n\n\nscope\n: operation scope.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    A scalar \nTensor\n representing the loss value.\n\n\n\n\n\n\n\n\nlog_loss\n\n\nlog_loss\n(\nweights\n=\n1.0\n,\n \nepsilon\n=\n1e-07\n,\n \nname\n=\nLogLoss\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nTrue\n)\n\n\n\n\n\n\n\n\nmean_squared_error\n\n\nmean_squared_error\n(\nweights\n=\n1.0\n,\n \nname\n=\nMeanSquaredError\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nTrue\n)\n\n\n\n\n\n\nComputes Mean Square Loss.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nweights\n: Coefficients for the loss a \nscalar\n.\n\n\nscope\n: scope to add the op to.\n\n\nname\n: name of the op.\n\n\ncollect\n: add to losses collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    A scalar \nTensor\n representing the loss value.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \npredictions\n shape doesn't match \nlabels\n shape, or \nweights\n is \nNone\n.\n\n\n\n\n\n\n\n\n\n\nsoftmax_cross_entropy\n\n\nsoftmax_cross_entropy\n(\nweights\n=\n1.0\n,\n \nlabel_smoothing\n=\n0\n,\n \nname\n=\nSoftmaxCrossEntropy\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nTrue\n)\n\n\n\n\n\n\nComputes Softmax Cross entropy (softmax categorical cross entropy).\n\n\nComputes softmax cross entropy between y_pred (logits) and\ny_true (labels).\n\n\nMeasures the probability error in discrete classification tasks in which\nthe classes are mutually exclusive (each entry is in exactly one class).\nFor example, each CIFAR-10 image is labeled with one and only one label:\nan image can be a dog or a truck, but not both.\n\n\n\n\n**WARNING\n:** This op expects unscaled logits, since it performs a \nsoftmax\n\non \ny_pred\n internally for efficiency.  Do not call this op with the\noutput of \nsoftmax\n, as it will produce incorrect results.\n\n\n\n\ny_pred\n and \ny_true\n must have the same shape \n[batch_size, num_classes]\n\nand the same dtype (either \nfloat32\n or \nfloat64\n). It is also required\nthat \ny_true\n (labels) are binary arrays (For example, class 2 out of a\ntotal of 5 different classes, will be define as [0., 1., 0., 0., 0.])\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nweights\n: Coefficients for the loss a \nscalar\n.\n\n\nlabel_smoothing\n: If greater than \n0\n then smooth the labels.\n\n\nscope\n: scope to add the op to.\n\n\nname\n: name of the op.\n\n\ncollect\n: add to losses collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    A scalar \nTensor\n representing the loss value.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \npredictions\n shape doesn't match \nlabels\n shape, or \nweights\n is \nNone\n.\n\n\n\n\n\n\n\n\n\n\nsigmoid_cross_entropy\n\n\nsigmoid_cross_entropy\n(\nweights\n=\n1.0\n,\n \nlabel_smoothing\n=\n0\n,\n \nname\n=\nSigmoidCrossEntropy\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nTrue\n)\n\n\n\n\n\n\nComputes Sigmoid cross entropy.(binary cross entropy):\n\n\nComputes sigmoid cross entropy between y_pred (logits) and y_true\n(labels).\n\n\nMeasures the probability error in discrete classification tasks in which\neach class is independent and not mutually exclusive. For instance,\none could perform multilabel classification where a picture can contain\nboth an elephant and a dog at the same time.\n\n\nFor brevity, let \nx = logits\n, \nz = targets\n.  The logistic loss is\n\n\nx - x * z + log(1 + exp(-x))\n\n\nTo ensure stability and avoid overflow, the implementation uses\n\n\nmax(x, 0) - x * z + log(1 + exp(-abs(x)))\n\n\ny_pred\n and \ny_true\n must have the same type and shape.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nweights\n: Coefficients for the loss a \nscalar\n.\n\n\nlabel_smoothing\n: If greater than \n0\n then smooth the labels.\n\n\nscope\n: scope to add the op to.\n\n\nname\n: name of the op.\n\n\ncollect\n: add to losses collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    A scalar \nTensor\n representing the loss value.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \npredictions\n shape doesn't match \nlabels\n shape, or \nweights\n is \nNone\n.\n\n\n\n\n\n\n\n\n\n\nhinge_loss\n\n\nhinge_loss\n(\nweights\n=\n1.0\n,\n \nname\n=\nHingeLoss\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nTrue\n)\n\n\n\n\n\n\nHinge Loss.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nweights\n: Coefficients for the loss a \nscalar\n.\n\n\nname\n: name of the op.\n\n\nscope\n: The scope for the operations performed in computing the loss.\n\n\ncollect\n: add to losses collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    A scalar \nTensor\n representing the loss value.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \npredictions\n shape doesn't match \nlabels\n shape, or \nweights\n is \nNone\n.\n\n\n\n\n\n\n\n\n\n\ncosine_distance\n\n\ncosine_distance\n(\ndim\n,\n \nweights\n=\n1.0\n,\n \nname\n=\nCosineDistance\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nTrue\n)\n\n\n\n\n\n\nAdds a cosine-distance loss to the training procedure.\n\n\nNote that the function assumes that \npredictions\n and \nlabels\n are already unit-normalized.\n\n\n\n\n\n\nWARNING\n: \nweights\n also supports dimensions of 1, but the broadcasting does\nnot work as advertised, you'll wind up with weighted sum instead of weighted\nmean for any but the last dimension. This will be cleaned up soon, so please\ndo not rely on the current behavior for anything but the shapes documented for\n\nweights\n below.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\ndim\n: The dimension along which the cosine distance is computed.\n\n\nweights\n: Coefficients for the loss a \nscalar\n.\n\n\nname\n: name of the op.\n\n\nscope\n: The scope for the operations performed in computing the loss.\n\n\ncollect\n: add to losses collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    A scalar \nTensor\n representing the loss value.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \npredictions\n shape doesn't match \nlabels\n shape, or \nweights\n is \nNone\n.\n\n\n\n\n\n\n\n\n\n\nkullback_leibler_divergence\n\n\nkullback_leibler_divergence\n(\nweights\n=\n1.0\n,\n \nname\n=\nKullbackLeiberDivergence\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nAdds a Kullback leiber diverenge loss to the training procedure.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nname\n: name of the op.\n\n\nscope\n: The scope for the operations performed in computing the loss.\n\n\ncollect\n: add to losses collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    A scalar \nTensor\n representing the loss value.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \npredictions\n shape doesn't match \nlabels\n shape, or \nweights\n is \nNone\n.\n\n\n\n\n\n\n\n\n\n\npoisson_loss\n\n\npoisson_loss\n(\nweights\n=\n1.0\n,\n \nname\n=\nPoissonLoss\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nAdds a poisson loss to the training procedure.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nname\n: name of the op.\n\n\nscope\n: The scope for the operations performed in computing the loss.\n\n\ncollect\n: add to losses collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    A scalar \nTensor\n representing the loss value.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \npredictions\n shape doesn't match \nlabels\n shape, or \nweights\n is \nNone\n.", 
            "title": "Losses"
        }, 
        {
            "location": "/losses/#absolute_difference", 
            "text": "absolute_difference ( weights = 1.0 ,   name = AbsoluteDifference ,   scope = None ,   collect = True )   Adds an Absolute Difference loss to the training procedure.  weights  acts as a coefficient for the loss. If a scalar is provided, then\nthe loss is simply scaled by the given value. If  weights  is a  Tensor  of\nshape  [batch_size] , then the total loss for each sample of the batch is\nrescaled by the corresponding element in the  weights  vector. If the shape of weights  matches the shape of  predictions , then the loss of each\nmeasurable element of  predictions  is scaled by the corresponding value of weights .    Args :   weights : Optional  Tensor  whose rank is either 0, or the same rank as labels , and must be broadcastable to  labels  (i.e., all dimensions must\nbe either  1 , or the same as the corresponding  losses  dimension).  name : operation name.  scope : operation scope.  collect : whether to collect this metric under the metric collection.     Returns :\n    A scalar  Tensor  representing the loss value.", 
            "title": "absolute_difference"
        }, 
        {
            "location": "/losses/#log_loss", 
            "text": "log_loss ( weights = 1.0 ,   epsilon = 1e-07 ,   name = LogLoss ,   scope = None ,   collect = True )", 
            "title": "log_loss"
        }, 
        {
            "location": "/losses/#mean_squared_error", 
            "text": "mean_squared_error ( weights = 1.0 ,   name = MeanSquaredError ,   scope = None ,   collect = True )   Computes Mean Square Loss.    Args :   weights : Coefficients for the loss a  scalar .  scope : scope to add the op to.  name : name of the op.  collect : add to losses collection.     Returns :\n    A scalar  Tensor  representing the loss value.    Raises :   ValueError : If  predictions  shape doesn't match  labels  shape, or  weights  is  None .", 
            "title": "mean_squared_error"
        }, 
        {
            "location": "/losses/#softmax_cross_entropy", 
            "text": "softmax_cross_entropy ( weights = 1.0 ,   label_smoothing = 0 ,   name = SoftmaxCrossEntropy ,   scope = None ,   collect = True )   Computes Softmax Cross entropy (softmax categorical cross entropy).  Computes softmax cross entropy between y_pred (logits) and\ny_true (labels).  Measures the probability error in discrete classification tasks in which\nthe classes are mutually exclusive (each entry is in exactly one class).\nFor example, each CIFAR-10 image is labeled with one and only one label:\nan image can be a dog or a truck, but not both.   **WARNING :** This op expects unscaled logits, since it performs a  softmax \non  y_pred  internally for efficiency.  Do not call this op with the\noutput of  softmax , as it will produce incorrect results.   y_pred  and  y_true  must have the same shape  [batch_size, num_classes] \nand the same dtype (either  float32  or  float64 ). It is also required\nthat  y_true  (labels) are binary arrays (For example, class 2 out of a\ntotal of 5 different classes, will be define as [0., 1., 0., 0., 0.])    Args :   weights : Coefficients for the loss a  scalar .  label_smoothing : If greater than  0  then smooth the labels.  scope : scope to add the op to.  name : name of the op.  collect : add to losses collection.     Returns :\n    A scalar  Tensor  representing the loss value.    Raises :   ValueError : If  predictions  shape doesn't match  labels  shape, or  weights  is  None .", 
            "title": "softmax_cross_entropy"
        }, 
        {
            "location": "/losses/#sigmoid_cross_entropy", 
            "text": "sigmoid_cross_entropy ( weights = 1.0 ,   label_smoothing = 0 ,   name = SigmoidCrossEntropy ,   scope = None ,   collect = True )   Computes Sigmoid cross entropy.(binary cross entropy):  Computes sigmoid cross entropy between y_pred (logits) and y_true\n(labels).  Measures the probability error in discrete classification tasks in which\neach class is independent and not mutually exclusive. For instance,\none could perform multilabel classification where a picture can contain\nboth an elephant and a dog at the same time.  For brevity, let  x = logits ,  z = targets .  The logistic loss is  x - x * z + log(1 + exp(-x))  To ensure stability and avoid overflow, the implementation uses  max(x, 0) - x * z + log(1 + exp(-abs(x)))  y_pred  and  y_true  must have the same type and shape.    Args :   weights : Coefficients for the loss a  scalar .  label_smoothing : If greater than  0  then smooth the labels.  scope : scope to add the op to.  name : name of the op.  collect : add to losses collection.     Returns :\n    A scalar  Tensor  representing the loss value.    Raises :   ValueError : If  predictions  shape doesn't match  labels  shape, or  weights  is  None .", 
            "title": "sigmoid_cross_entropy"
        }, 
        {
            "location": "/losses/#hinge_loss", 
            "text": "hinge_loss ( weights = 1.0 ,   name = HingeLoss ,   scope = None ,   collect = True )   Hinge Loss.    Args :   weights : Coefficients for the loss a  scalar .  name : name of the op.  scope : The scope for the operations performed in computing the loss.  collect : add to losses collection.     Returns :\n    A scalar  Tensor  representing the loss value.    Raises :   ValueError : If  predictions  shape doesn't match  labels  shape, or  weights  is  None .", 
            "title": "hinge_loss"
        }, 
        {
            "location": "/losses/#cosine_distance", 
            "text": "cosine_distance ( dim ,   weights = 1.0 ,   name = CosineDistance ,   scope = None ,   collect = True )   Adds a cosine-distance loss to the training procedure.  Note that the function assumes that  predictions  and  labels  are already unit-normalized.    WARNING :  weights  also supports dimensions of 1, but the broadcasting does\nnot work as advertised, you'll wind up with weighted sum instead of weighted\nmean for any but the last dimension. This will be cleaned up soon, so please\ndo not rely on the current behavior for anything but the shapes documented for weights  below.    Args :   dim : The dimension along which the cosine distance is computed.  weights : Coefficients for the loss a  scalar .  name : name of the op.  scope : The scope for the operations performed in computing the loss.  collect : add to losses collection.     Returns :\n    A scalar  Tensor  representing the loss value.    Raises :   ValueError : If  predictions  shape doesn't match  labels  shape, or  weights  is  None .", 
            "title": "cosine_distance"
        }, 
        {
            "location": "/losses/#kullback_leibler_divergence", 
            "text": "kullback_leibler_divergence ( weights = 1.0 ,   name = KullbackLeiberDivergence ,   scope = None ,   collect = False )   Adds a Kullback leiber diverenge loss to the training procedure.    Args :   name : name of the op.  scope : The scope for the operations performed in computing the loss.  collect : add to losses collection.     Returns :\n    A scalar  Tensor  representing the loss value.    Raises :   ValueError : If  predictions  shape doesn't match  labels  shape, or  weights  is  None .", 
            "title": "kullback_leibler_divergence"
        }, 
        {
            "location": "/losses/#poisson_loss", 
            "text": "poisson_loss ( weights = 1.0 ,   name = PoissonLoss ,   scope = None ,   collect = False )   Adds a poisson loss to the training procedure.    Args :   name : name of the op.  scope : The scope for the operations performed in computing the loss.  collect : add to losses collection.     Returns :\n    A scalar  Tensor  representing the loss value.    Raises :   ValueError : If  predictions  shape doesn't match  labels  shape, or  weights  is  None .", 
            "title": "poisson_loss"
        }, 
        {
            "location": "/metrics/", 
            "text": "check_metric_data\n\n\ncheck_metric_data\n(\ny_pred\n,\n \ny_true\n)\n\n\n\n\n\n\n\n\nbuilt_metric\n\n\nbuilt_metric\n(\nfct\n,\n \nname\n,\n \nscope\n,\n \ncollect\n)\n\n\n\n\n\n\nBuilds the metric function.\n\n\n\n\nArgs\n:\n\n\nfct\n: the metric function to build.\n\n\nname\n: operation name.\n\n\nscope\n: operation scope.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\naccuracy\n\n\naccuracy\n(\nname\n=\nAccuracy\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nComputes the accuracy.\n\n\nAn op that calculates mean accuracy:\n    * y_pred are y_True are both one-hot encoded. (categorical accuracy)\n    * y_pred are logits are binary encoded (and represented as int32). (binary accuracy)\n\n\n\n\nExamples\n:\n\n\n\n\n \ninput_data\n \n=\n \nplaceholder\n(\nshape\n=\n[\nNone\n,\n \n784\n])\n\n\n \ny_pred\n \n=\n \nmy_network\n(\ninput_data\n)\n \n# Apply some ops\n\n\n \ny_true\n \n=\n \nplaceholder\n(\nshape\n=\n[\nNone\n,\n \n10\n])\n \n# Labels\n\n\n \naccuracy_op\n \n=\n \naccuracy\n(\ny_pred\n,\n \ny_true\n)\n\n\n \n# Calculate accuracy by feeding data X and labels Y\n\n\n \naccuracy_op\n \n=\n \nsess\n.\nrun\n(\naccuracy_op\n,\n \nfeed_dict\n=\n{\ninput_data\n:\n \nX\n,\n \ny_true\n:\n \nY\n})\n\n\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nscope\n: scope to add the op to.\n\n\nname\n: name of the op.\n\n\ncollect\n: add to metrics collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    \nFloat\n. The mean accuracy.\n\n\n\n\n\n\n\n\ntop_k\n\n\ntop_k\n(\nk\n=\n1\n,\n \nname\n=\nTopK\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\ntop_k_op.\n\n\nAn op that calculates top-k mean accuracy.\n\n\n\n\nExamples\n:\n\n\n\n\n \ninput_data\n \n=\n \nplaceholder\n(\nshape\n=\n[\nNone\n,\n \n784\n])\n\n\n \ny_pred\n \n=\n \nmy_network\n(\ninput_data\n)\n \n# Apply some ops\n\n\n \ny_true\n \n=\n \nplaceholder\n(\nshape\n=\n[\nNone\n,\n \n10\n])\n \n# Labels\n\n\n \ntop3_op\n \n=\n \ntop_k\n(\ny_pred\n,\n \ny_true\n,\n \n3\n)\n\n\n\n \n# Calculate Top-3 accuracy by feeding data X and labels Y\n\n\n \ntop3_accuracy\n \n=\n \nsess\n.\nrun\n(\ntop3_op\n,\n \nfeed_dict\n=\n{\ninput_data\n:\n \nX\n,\n \ny_true\n:\n \nY\n})\n\n\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nk\n: \nint\n. Number of top elements to look at for computing precision.\n\n\nscope\n: scope to add the op to.\n\n\nname\n: name of the op.\n\n\ncollect\n: add to metrics collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    \nFloat\n. The top-k mean accuracy.\n\n\n\n\n\n\n\n\nstd_error\n\n\nstd_error\n(\nname\n=\nStandardError\n,\n \nscope\n=\nNone\n,\n \ncollect\n=\nFalse\n)\n\n\n\n\n\n\nstandard error.\n\n\nAn op that calculates the standard error.\n\n\n\n\nExamples\n:\n\n\n\n\n \ninput_data\n \n=\n \nplaceholder\n(\nshape\n=\n[\nNone\n,\n \n784\n])\n\n\n \ny_pred\n \n=\n \nmy_network\n(\ninput_data\n)\n \n# Apply some ops\n\n\n \ny_true\n \n=\n \nplaceholder\n(\nshape\n=\n[\nNone\n,\n \n10\n])\n \n# Labels\n\n\n \nstderr\n \n=\n \nstd_error\n(\ny_pred\n,\n \ny_true\n)\n\n\n\n \n# Calculate standard error by feeding data X and labels Y\n\n\n \nstd_error\n \n=\n \nsess\n.\nrun\n(\nstderr_op\n,\n \nfeed_dict\n=\n{\ninput_data\n:\n \nX\n,\n \ny_true\n:\n \nY\n})\n\n\n\n\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nscope\n: scope to add the op to.\n\n\nname\n: name of the op.\n\n\ncollect\n: add to metrics collection.\n\n\n\n\n\n\n\n\nReturns\n:\n    \nFloat\n. The standard error.", 
            "title": "Metrics"
        }, 
        {
            "location": "/metrics/#check_metric_data", 
            "text": "check_metric_data ( y_pred ,   y_true )", 
            "title": "check_metric_data"
        }, 
        {
            "location": "/metrics/#built_metric", 
            "text": "built_metric ( fct ,   name ,   scope ,   collect )   Builds the metric function.   Args :  fct : the metric function to build.  name : operation name.  scope : operation scope.  collect : whether to collect this metric under the metric collection.", 
            "title": "built_metric"
        }, 
        {
            "location": "/metrics/#accuracy", 
            "text": "accuracy ( name = Accuracy ,   scope = None ,   collect = False )   Computes the accuracy.  An op that calculates mean accuracy:\n    * y_pred are y_True are both one-hot encoded. (categorical accuracy)\n    * y_pred are logits are binary encoded (and represented as int32). (binary accuracy)   Examples :     input_data   =   placeholder ( shape = [ None ,   784 ])    y_pred   =   my_network ( input_data )   # Apply some ops    y_true   =   placeholder ( shape = [ None ,   10 ])   # Labels    accuracy_op   =   accuracy ( y_pred ,   y_true )    # Calculate accuracy by feeding data X and labels Y    accuracy_op   =   sess . run ( accuracy_op ,   feed_dict = { input_data :   X ,   y_true :   Y })     Args :   scope : scope to add the op to.  name : name of the op.  collect : add to metrics collection.     Returns :\n     Float . The mean accuracy.", 
            "title": "accuracy"
        }, 
        {
            "location": "/metrics/#top_k", 
            "text": "top_k ( k = 1 ,   name = TopK ,   scope = None ,   collect = False )   top_k_op.  An op that calculates top-k mean accuracy.   Examples :     input_data   =   placeholder ( shape = [ None ,   784 ])    y_pred   =   my_network ( input_data )   # Apply some ops    y_true   =   placeholder ( shape = [ None ,   10 ])   # Labels    top3_op   =   top_k ( y_pred ,   y_true ,   3 )    # Calculate Top-3 accuracy by feeding data X and labels Y    top3_accuracy   =   sess . run ( top3_op ,   feed_dict = { input_data :   X ,   y_true :   Y })     Args :   k :  int . Number of top elements to look at for computing precision.  scope : scope to add the op to.  name : name of the op.  collect : add to metrics collection.     Returns :\n     Float . The top-k mean accuracy.", 
            "title": "top_k"
        }, 
        {
            "location": "/metrics/#std_error", 
            "text": "std_error ( name = StandardError ,   scope = None ,   collect = False )   standard error.  An op that calculates the standard error.   Examples :     input_data   =   placeholder ( shape = [ None ,   784 ])    y_pred   =   my_network ( input_data )   # Apply some ops    y_true   =   placeholder ( shape = [ None ,   10 ])   # Labels    stderr   =   std_error ( y_pred ,   y_true )    # Calculate standard error by feeding data X and labels Y    std_error   =   sess . run ( stderr_op ,   feed_dict = { input_data :   X ,   y_true :   Y })     Args :   scope : scope to add the op to.  name : name of the op.  collect : add to metrics collection.     Returns :\n     Float . The standard error.", 
            "title": "std_error"
        }, 
        {
            "location": "/optimizers/", 
            "text": "create_learning_rate_decay_fn\n\n\ncreate_learning_rate_decay_fn\n(\nlearning_rate\n,\n \ndecay_type\n,\n \ndecay_steps\n,\n \ndecay_rate\n,\n \nstart_decay_at\n=\n0\n,\n \nstop_decay_at\n=\n1000000000.0\n,\n \nmin_learning_rate\n=\nNone\n,\n \nstaircase\n=\nFalse\n,\n \nglobal_step\n=\nNone\n)\n\n\n\n\n\n\nCreates a function that decays the learning rate.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nlearning_rate\n: A Tensor or a floating point value. The learning rate to use.\n\n\ndecay_steps\n: How often to apply decay.\n\n\ndecay_rate\n: A Python number. The decay rate.\n\n\nstart_decay_at\n: Don't decay before this step\n\n\nstop_decay_at\n: Don't decay after this step\n\n\nmin_learning_rate\n: Don't decay below this number\n\n\ndecay_type\n: A decay function name defined in \ntf.train\n\npossible Values: exponential_decay, inverse_time_decay, natural_exp_decay,\n         piecewise_constant, polynomial_decay.\n\n\nstaircase\n: Whether to apply decay in a discrete staircase,\nas opposed to continuous, fashion.\n\n\nglobal_step\n: Scalar int \nTensor\n, step counter for each update. If not supplied,\nit will be fetched from the default graph (see \ntf.contrib.framework.get_global_step\n\nfor details). If it's not been created, no step will be incremented with each weight\nupdate. \nlearning_rate_decay_fn\n requires \nglobal_step\n.\n\n\n\n\n\n\n\n\nReturns\n:\n    A function that takes (learning_rate, global_step) as inputs\n    and returns the learning rate for the given step.\n    Returns \nNone\n if decay_type is empty or None.\n\n\n\n\n\n\n\n\nsgd\n\n\nsgd\n(\nlearning_rate\n=\n0.001\n,\n \ndecay_type\n=\n,\n \ndecay_rate\n=\n0.0\n,\n \ndecay_steps\n=\n100\n,\n \nstart_decay_at\n=\n0\n,\n \nstop_decay_at\n=\n2147483647\n,\n \nmin_learning_rate\n=\n1e-12\n,\n \nstaircase\n=\nFalse\n,\n \nglobal_step\n=\nNone\n,\n \nuse_locking\n=\nFalse\n,\n \nname\n=\nSGD\n)\n\n\n\n\n\n\nOptimizer that implements the gradient descent algorithm.\n\n\n\n\nArgs\n:\n\n\nlearning_rate\n: A Tensor or a floating point value. The learning rate to use.\n\n\ndecay_type\n: A decay function name defined in \ntf.train\n\n\ndecay_rate\n: \nfloat\n. The learning rate decay to apply.\n\n\ndecay_steps\n: \nint\n. Apply decay every provided steps.\n\n\nstart_decay_at\n: \nint\n. Don't decay before this step.\n\n\nstop_decay_at\n: \nint\n. Don't decay after this step.\n\n\nmin_learning_rate\n: \nfloat\n. Don't decay below this number.\n\n\nstaircase\n: \nbool\n. It \nTrue\n decay learning rate at discrete intervals.\n\n\nglobal_step\n: Scalar int \nTensor\n, step counter for each update.\n\n\nuse_locking\n: If True use locks for update operations.\n\n\nname\n: Optional name prefix for the operations created when applying gradients.\n\n\n\n\n\n\n\n\n\n\nmomentum\n\n\nmomentum\n(\nlearning_rate\n=\n0.001\n,\n \nmomentum\n=\n0.9\n,\n \ndecay_type\n=\n,\n \ndecay_rate\n=\n0.0\n,\n \ndecay_steps\n=\n100\n,\n \nstart_decay_at\n=\n0\n,\n \nstop_decay_at\n=\n2147483647\n,\n \nmin_learning_rate\n=\n1e-12\n,\n \nstaircase\n=\nFalse\n,\n \nglobal_step\n=\nNone\n,\n \nuse_locking\n=\nFalse\n,\n \nname\n=\nMomentum\n)\n\n\n\n\n\n\nOptimizer that implements the Momentum.\n\n\nMomentum Optimizer accepts learning rate decay. When training a model,\nit is often recommended to lower the learning rate as the training\nprogresses. The function returns the decayed learning rate.  It is\ncomputed as:\n\n\n \ndecayed_learning_rate\n \n=\n \nlearning_rate\n \n*\n \ndecay_rate\n \n^\n \n(\nglobal_step\n \n/\n \nlr_decay_steps\n)\n\n\n\n\n\n\n\n\nArgs\n:\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\nmomentum\n: \nfloat\n. Momentum.\n\n\ndecay_type\n: A decay function name defined in \ntf.train\n\n\ndecay_rate\n: \nfloat\n. The learning rate decay to apply.\n\n\ndecay_steps\n: \nint\n. Apply decay every provided steps.\n\n\nstart_decay_at\n: \nint\n. Don't decay before this step.\n\n\nstop_decay_at\n: \nint\n. Don't decay after this step.\n\n\nmin_learning_rate\n: \nfloat\n. Don't decay below this number.\n\n\nstaircase\n: \nbool\n. It \nTrue\n decay learning rate at discrete intervals.\n\n\nglobal_step\n: Scalar int \nTensor\n, step counter for each update.\n\n\nuse_locking\n: If True use locks for update operations.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when applying gradients.\n\n\n\n\n\n\n\n\n\n\nnesterov\n\n\nnesterov\n(\nlearning_rate\n=\n0.001\n,\n \nmomentum\n=\n0.9\n,\n \ndecay_type\n=\n,\n \ndecay_rate\n=\n0.0\n,\n \ndecay_steps\n=\n100\n,\n \nstart_decay_at\n=\n0\n,\n \nstop_decay_at\n=\n2147483647\n,\n \nmin_learning_rate\n=\n1e-12\n,\n \nstaircase\n=\nFalse\n,\n \nuse_locking\n=\nFalse\n,\n \nglobal_step\n=\nNone\n,\n \nname\n=\nMomentum\n)\n\n\n\n\n\n\nOptimizer that implements the Nesterov.\n\n\nSame as Momentum optimizer but uses nestrov\nSee \nSutskever et. al., 2013\n\n\n \ndecayed_learning_rate\n \n=\n \nlearning_rate\n \n*\n \ndecay_rate\n \n^\n \n(\nglobal_step\n \n/\n \nlr_decay_steps\n)\n\n\n\n\n\n\n\n\nArgs\n:\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\nmomentum\n: \nfloat\n. Momentum.\n\n\ndecay_type\n: A decay function name defined in \ntf.train\n\n\ndecay_rate\n: \nfloat\n. The learning rate decay to apply.\n\n\ndecay_steps\n: \nint\n. Apply decay every provided steps.\n\n\nstart_decay_at\n: \nint\n. Don't decay before this step.\n\n\nstop_decay_at\n: \nint\n. Don't decay after this step.\n\n\nmin_learning_rate\n: \nfloat\n. Don't decay below this number.\n\n\nstaircase\n: \nbool\n. It \nTrue\n decay learning rate at discrete intervals.\n\n\nglobal_step\n: Scalar int \nTensor\n, step counter for each update.\n\n\nuse_locking\n: If True use locks for update operations.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when applying gradients.\n\n\n\n\n\n\n\n\n\n\nrmsprop\n\n\nrmsprop\n(\nlearning_rate\n=\n0.001\n,\n \ndecay\n=\n0.9\n,\n \nmomentum\n=\n0.0\n,\n \nepsilon\n=\n1e-10\n,\n \ndecay_type\n=\n,\n \ndecay_rate\n=\n0.0\n,\n \ndecay_steps\n=\n100\n,\n \nstart_decay_at\n=\n0\n,\n \nstop_decay_at\n=\n2147483647\n,\n \nmin_learning_rate\n=\n1e-12\n,\n \nstaircase\n=\nFalse\n,\n \nglobal_step\n=\nNone\n,\n \nuse_locking\n=\nFalse\n,\n \nname\n=\nRMSProp\n)\n\n\n\n\n\n\nOptimizer that implements the RMSprop.\n\n\nMaintain a moving (discounted) average of the square of gradients.\nDivide gradient by the root of this average.\n\n\n\n\nArgs\n:\n\n\nlearning_rate\n: \nfloat\n. learning rate.\n\n\ndecay\n: \nfloat\n. Discounting factor for the history/coming gradient.\n\n\nmomentum\n: \nfloat\n. Momentum.\n\n\nepsilon\n: \nfloat\n. Small value to avoid zero denominator.\n\n\ndecay_type\n: A decay function name defined in \ntf.train\n\n\ndecay_rate\n: \nfloat\n. The learning rate decay to apply.\n\n\ndecay_steps\n: \nint\n. Apply decay every provided steps.\n\n\nstart_decay_at\n: \nint\n. Don't decay before this step.\n\n\nstop_decay_at\n: \nint\n. Don't decay after this step.\n\n\nmin_learning_rate\n: \nfloat\n. Don't decay below this number.\n\n\nstaircase\n: \nbool\n. It \nTrue\n decay learning rate at discrete intervals.\n\n\nglobal_step\n: Scalar int \nTensor\n, step counter for each update.\n\n\nuse_locking\n: If True use locks for update operations.\n\n\nname\n: Optional name prefix for the operations created when applying gradients.\n\n\n\n\n\n\n\n\n\n\nadam\n\n\nadam\n(\nlearning_rate\n=\n0.001\n,\n \nbeta1\n=\n0.9\n,\n \nbeta2\n=\n0.999\n,\n \nepsilon\n=\n1e-08\n,\n \ndecay_type\n=\n,\n \ndecay_rate\n=\n0.0\n,\n \ndecay_steps\n=\n100\n,\n \nstart_decay_at\n=\n0\n,\n \nstop_decay_at\n=\n2147483647\n,\n \nmin_learning_rate\n=\n1e-12\n,\n \nstaircase\n=\nFalse\n,\n \nglobal_step\n=\nNone\n,\n \nuse_locking\n=\nFalse\n,\n \nname\n=\nAdam\n)\n\n\n\n\n\n\nOptimizer that implements the Adam.\n\n\nThe default value of 1e-8 for epsilon might not be a good default in\ngeneral. For example, when training an Inception network on ImageNet a\ncurrent good choice is 1.0 or 0.1.\n\n\n\n\nArgs\n:\n\n\nlearning_rate\n: \nfloat\n. learning rate.\n\n\nbeta1\n: \nfloat\n. The exponential decay rate for the 1st moment estimates.\n\n\nbeta2\n: \nfloat\n. The exponential decay rate for the 2nd moment estimates.\n\n\nepsilon\n: \nfloat\n. A small constant for numerical stability.\n\n\nepsilon\n: \nfloat\n. Small value to avoid zero denominator.\n\n\ndecay_type\n: A decay function name defined in \ntf.train\n\n\ndecay_rate\n: \nfloat\n. The learning rate decay to apply.\n\n\ndecay_steps\n: \nint\n. Apply decay every provided steps.\n\n\nstart_decay_at\n: \nint\n. Don't decay before this step.\n\n\nstop_decay_at\n: \nint\n. Don't decay after this step.\n\n\nmin_learning_rate\n: \nfloat\n. Don't decay below this number.\n\n\nstaircase\n: \nbool\n. It \nTrue\n decay learning rate at discrete intervals.\n\n\nglobal_step\n: Scalar int \nTensor\n, step counter for each update.\n\n\nuse_locking\n: If True use locks for update operations.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when applying gradients.\n\n\n\n\n\n\n\n\n\n\nadagrad\n\n\nadagrad\n(\nlearning_rate\n=\n0.001\n,\n \ninitial_accumulator_value\n=\n0.1\n,\n \ndecay_type\n=\n,\n \ndecay_rate\n=\n0.0\n,\n \ndecay_steps\n=\n100\n,\n \nstart_decay_at\n=\n0\n,\n \nstop_decay_at\n=\n2147483647\n,\n \nmin_learning_rate\n=\n1e-12\n,\n \nstaircase\n=\nFalse\n,\n \nglobal_step\n=\nNone\n,\n \nuse_locking\n=\nFalse\n,\n \nname\n=\nAdaGrad\n)\n\n\n\n\n\n\nOptimizer that implements AdaGrad.\n\n\n\n\nArgs\n:\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\ninitial_accumulator_value\n: \nfloat\n. Starting value for the\naccumulators, must be positive.\n\n\ndecay_type\n: A decay function name defined in \ntf.train\n\n\ndecay_rate\n: \nfloat\n. The learning rate decay to apply.\n\n\ndecay_steps\n: \nint\n. Apply decay every provided steps.\n\n\nstart_decay_at\n: \nint\n. Don't decay before this step.\n\n\nstop_decay_at\n: \nint\n. Don't decay after this step.\n\n\nmin_learning_rate\n: \nfloat\n. Don't decay below this number.\n\n\nstaircase\n: \nbool\n. It \nTrue\n decay learning rate at discrete intervals.\n\n\nglobal_step\n: Scalar int \nTensor\n, step counter for each update.\n\n\nuse_locking\n: If True use locks for update operations.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when applying gradients.\n\n\n\n\n\n\n\n\n\n\nftrl\n\n\nftrl\n(\nlearning_rate\n=\n3.0\n,\n \nlearning_rate_power\n=-\n0.5\n,\n \ninitial_accumulator_value\n=\n0.1\n,\n \nl1_regularization_strength\n=\n0.0\n,\n \nl2_regularization_strength\n=\n0.0\n,\n \ndecay_type\n=\n,\n \ndecay_rate\n=\n0.0\n,\n \ndecay_steps\n=\n100\n,\n \nstart_decay_at\n=\n0\n,\n \nstop_decay_at\n=\n2147483647\n,\n \nmin_learning_rate\n=\n1e-12\n,\n \nstaircase\n=\nFalse\n,\n \nglobal_step\n=\nNone\n,\n \nuse_locking\n=\nFalse\n,\n \nname\n=\nFtrl\n)\n\n\n\n\n\n\nOptimizer that implements Ftrl Proximal.\n\n\nThe Ftrl-proximal algorithm, abbreviated for Follow-the-regularized-leader,\nis described in the paper below.\n\n\nIt can give a good performance vs. sparsity tradeoff.\n\n\nFtrl-proximal uses its own global base learning rate and can behave like\nAdagrad with \nlearning_rate_power=-0.5\n, or like gradient descent with\n\nlearning_rate_power=0.0\n.\n\n\n\n\nArgs\n:\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\nlearning_rate_power\n: \nfloat\n. Must be less or equal to zero.\n\n\ninitial_accumulator_value\n: \nfloat\n. The starting value for accumulators.\nOnly positive values are allowed.\n\n\nl1_regularization_strength\n: \nfloat\n. Must be less or equal to zero.\n\n\nl2_regularization_strength\n: \nfloat\n. Must be less or equal to zero.\n\n\ndecay_type\n: A decay function name defined in \ntf.train\n\n\ndecay_rate\n: \nfloat\n. The learning rate decay to apply.\n\n\ndecay_steps\n: \nint\n. Apply decay every provided steps.\n\n\nstart_decay_at\n: \nint\n. Don't decay before this step.\n\n\nstop_decay_at\n: \nint\n. Don't decay after this step.\n\n\nmin_learning_rate\n: \nfloat\n. Don't decay below this number.\n\n\nstaircase\n: \nbool\n. It \nTrue\n decay learning rate at discrete intervals.\n\n\nglobal_step\n: Scalar int \nTensor\n, step counter for each update.\n\n\nuse_locking\n: If True use locks for update operations.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when applying gradients..\n\n\n\n\n\n\n\n\n\n\nadadelta\n\n\nadadelta\n(\nlearning_rate\n=\n0.001\n,\n \nrho\n=\n0.1\n,\n \nepsilon\n=\n1e-08\n,\n \ndecay_type\n=\n,\n \ndecay_rate\n=\n0.0\n,\n \ndecay_steps\n=\n100\n,\n \nstart_decay_at\n=\n0\n,\n \nstop_decay_at\n=\n2147483647\n,\n \nmin_learning_rate\n=\n1e-12\n,\n \nstaircase\n=\nFalse\n,\n \nglobal_step\n=\nNone\n,\n \nuse_locking\n=\nFalse\n,\n \nname\n=\nAdaDelta\n)\n\n\n\n\n\n\nOptimizer that implements AdaDelta.\n\n\n\n\nArgs\n:\n\n\nlearning_rate\n: A \nTensor\n or a floating point value. The learning rate.\n\n\nrho\n: A \nTensor\n or a floating point value. The decay rate.\n\n\nepsilon\n: A \nTensor\n or a floating point value.  A constant epsilon used to better\nconditioning the grad update.\n\n\ndecay_type\n: A decay function name defined in \ntf.train\n\n\ndecay_rate\n: \nfloat\n. The learning rate decay to apply.\n\n\ndecay_steps\n: \nint\n. Apply decay every provided steps.\n\n\nstart_decay_at\n: \nint\n. Don't decay before this step.\n\n\nstop_decay_at\n: \nint\n. Don't decay after this step.\n\n\nmin_learning_rate\n: \nfloat\n. Don't decay below this number.\n\n\nstaircase\n: \nbool\n. It \nTrue\n decay learning rate at discrete intervals.\n\n\nglobal_step\n: Scalar int \nTensor\n, step counter for each update.\n\n\nuse_locking\n: If True use locks for update operations.\n\n\nname\n: Optional name prefix for the operations created when applying gradients.", 
            "title": "Optimizers"
        }, 
        {
            "location": "/optimizers/#create_learning_rate_decay_fn", 
            "text": "create_learning_rate_decay_fn ( learning_rate ,   decay_type ,   decay_steps ,   decay_rate ,   start_decay_at = 0 ,   stop_decay_at = 1000000000.0 ,   min_learning_rate = None ,   staircase = False ,   global_step = None )   Creates a function that decays the learning rate.    Args :   learning_rate : A Tensor or a floating point value. The learning rate to use.  decay_steps : How often to apply decay.  decay_rate : A Python number. The decay rate.  start_decay_at : Don't decay before this step  stop_decay_at : Don't decay after this step  min_learning_rate : Don't decay below this number  decay_type : A decay function name defined in  tf.train \npossible Values: exponential_decay, inverse_time_decay, natural_exp_decay,\n         piecewise_constant, polynomial_decay.  staircase : Whether to apply decay in a discrete staircase,\nas opposed to continuous, fashion.  global_step : Scalar int  Tensor , step counter for each update. If not supplied,\nit will be fetched from the default graph (see  tf.contrib.framework.get_global_step \nfor details). If it's not been created, no step will be incremented with each weight\nupdate.  learning_rate_decay_fn  requires  global_step .     Returns :\n    A function that takes (learning_rate, global_step) as inputs\n    and returns the learning rate for the given step.\n    Returns  None  if decay_type is empty or None.", 
            "title": "create_learning_rate_decay_fn"
        }, 
        {
            "location": "/optimizers/#sgd", 
            "text": "sgd ( learning_rate = 0.001 ,   decay_type = ,   decay_rate = 0.0 ,   decay_steps = 100 ,   start_decay_at = 0 ,   stop_decay_at = 2147483647 ,   min_learning_rate = 1e-12 ,   staircase = False ,   global_step = None ,   use_locking = False ,   name = SGD )   Optimizer that implements the gradient descent algorithm.   Args :  learning_rate : A Tensor or a floating point value. The learning rate to use.  decay_type : A decay function name defined in  tf.train  decay_rate :  float . The learning rate decay to apply.  decay_steps :  int . Apply decay every provided steps.  start_decay_at :  int . Don't decay before this step.  stop_decay_at :  int . Don't decay after this step.  min_learning_rate :  float . Don't decay below this number.  staircase :  bool . It  True  decay learning rate at discrete intervals.  global_step : Scalar int  Tensor , step counter for each update.  use_locking : If True use locks for update operations.  name : Optional name prefix for the operations created when applying gradients.", 
            "title": "sgd"
        }, 
        {
            "location": "/optimizers/#momentum", 
            "text": "momentum ( learning_rate = 0.001 ,   momentum = 0.9 ,   decay_type = ,   decay_rate = 0.0 ,   decay_steps = 100 ,   start_decay_at = 0 ,   stop_decay_at = 2147483647 ,   min_learning_rate = 1e-12 ,   staircase = False ,   global_step = None ,   use_locking = False ,   name = Momentum )   Optimizer that implements the Momentum.  Momentum Optimizer accepts learning rate decay. When training a model,\nit is often recommended to lower the learning rate as the training\nprogresses. The function returns the decayed learning rate.  It is\ncomputed as:    decayed_learning_rate   =   learning_rate   *   decay_rate   ^   ( global_step   /   lr_decay_steps )    Args :  learning_rate :  float . Learning rate.  momentum :  float . Momentum.  decay_type : A decay function name defined in  tf.train  decay_rate :  float . The learning rate decay to apply.  decay_steps :  int . Apply decay every provided steps.  start_decay_at :  int . Don't decay before this step.  stop_decay_at :  int . Don't decay after this step.  min_learning_rate :  float . Don't decay below this number.  staircase :  bool . It  True  decay learning rate at discrete intervals.  global_step : Scalar int  Tensor , step counter for each update.  use_locking : If True use locks for update operations.  name :  str . Optional name prefix for the operations created when applying gradients.", 
            "title": "momentum"
        }, 
        {
            "location": "/optimizers/#nesterov", 
            "text": "nesterov ( learning_rate = 0.001 ,   momentum = 0.9 ,   decay_type = ,   decay_rate = 0.0 ,   decay_steps = 100 ,   start_decay_at = 0 ,   stop_decay_at = 2147483647 ,   min_learning_rate = 1e-12 ,   staircase = False ,   use_locking = False ,   global_step = None ,   name = Momentum )   Optimizer that implements the Nesterov.  Same as Momentum optimizer but uses nestrov\nSee  Sutskever et. al., 2013    decayed_learning_rate   =   learning_rate   *   decay_rate   ^   ( global_step   /   lr_decay_steps )    Args :  learning_rate :  float . Learning rate.  momentum :  float . Momentum.  decay_type : A decay function name defined in  tf.train  decay_rate :  float . The learning rate decay to apply.  decay_steps :  int . Apply decay every provided steps.  start_decay_at :  int . Don't decay before this step.  stop_decay_at :  int . Don't decay after this step.  min_learning_rate :  float . Don't decay below this number.  staircase :  bool . It  True  decay learning rate at discrete intervals.  global_step : Scalar int  Tensor , step counter for each update.  use_locking : If True use locks for update operations.  name :  str . Optional name prefix for the operations created when applying gradients.", 
            "title": "nesterov"
        }, 
        {
            "location": "/optimizers/#rmsprop", 
            "text": "rmsprop ( learning_rate = 0.001 ,   decay = 0.9 ,   momentum = 0.0 ,   epsilon = 1e-10 ,   decay_type = ,   decay_rate = 0.0 ,   decay_steps = 100 ,   start_decay_at = 0 ,   stop_decay_at = 2147483647 ,   min_learning_rate = 1e-12 ,   staircase = False ,   global_step = None ,   use_locking = False ,   name = RMSProp )   Optimizer that implements the RMSprop.  Maintain a moving (discounted) average of the square of gradients.\nDivide gradient by the root of this average.   Args :  learning_rate :  float . learning rate.  decay :  float . Discounting factor for the history/coming gradient.  momentum :  float . Momentum.  epsilon :  float . Small value to avoid zero denominator.  decay_type : A decay function name defined in  tf.train  decay_rate :  float . The learning rate decay to apply.  decay_steps :  int . Apply decay every provided steps.  start_decay_at :  int . Don't decay before this step.  stop_decay_at :  int . Don't decay after this step.  min_learning_rate :  float . Don't decay below this number.  staircase :  bool . It  True  decay learning rate at discrete intervals.  global_step : Scalar int  Tensor , step counter for each update.  use_locking : If True use locks for update operations.  name : Optional name prefix for the operations created when applying gradients.", 
            "title": "rmsprop"
        }, 
        {
            "location": "/optimizers/#adam", 
            "text": "adam ( learning_rate = 0.001 ,   beta1 = 0.9 ,   beta2 = 0.999 ,   epsilon = 1e-08 ,   decay_type = ,   decay_rate = 0.0 ,   decay_steps = 100 ,   start_decay_at = 0 ,   stop_decay_at = 2147483647 ,   min_learning_rate = 1e-12 ,   staircase = False ,   global_step = None ,   use_locking = False ,   name = Adam )   Optimizer that implements the Adam.  The default value of 1e-8 for epsilon might not be a good default in\ngeneral. For example, when training an Inception network on ImageNet a\ncurrent good choice is 1.0 or 0.1.   Args :  learning_rate :  float . learning rate.  beta1 :  float . The exponential decay rate for the 1st moment estimates.  beta2 :  float . The exponential decay rate for the 2nd moment estimates.  epsilon :  float . A small constant for numerical stability.  epsilon :  float . Small value to avoid zero denominator.  decay_type : A decay function name defined in  tf.train  decay_rate :  float . The learning rate decay to apply.  decay_steps :  int . Apply decay every provided steps.  start_decay_at :  int . Don't decay before this step.  stop_decay_at :  int . Don't decay after this step.  min_learning_rate :  float . Don't decay below this number.  staircase :  bool . It  True  decay learning rate at discrete intervals.  global_step : Scalar int  Tensor , step counter for each update.  use_locking : If True use locks for update operations.  name :  str . Optional name prefix for the operations created when applying gradients.", 
            "title": "adam"
        }, 
        {
            "location": "/optimizers/#adagrad", 
            "text": "adagrad ( learning_rate = 0.001 ,   initial_accumulator_value = 0.1 ,   decay_type = ,   decay_rate = 0.0 ,   decay_steps = 100 ,   start_decay_at = 0 ,   stop_decay_at = 2147483647 ,   min_learning_rate = 1e-12 ,   staircase = False ,   global_step = None ,   use_locking = False ,   name = AdaGrad )   Optimizer that implements AdaGrad.   Args :  learning_rate :  float . Learning rate.  initial_accumulator_value :  float . Starting value for the\naccumulators, must be positive.  decay_type : A decay function name defined in  tf.train  decay_rate :  float . The learning rate decay to apply.  decay_steps :  int . Apply decay every provided steps.  start_decay_at :  int . Don't decay before this step.  stop_decay_at :  int . Don't decay after this step.  min_learning_rate :  float . Don't decay below this number.  staircase :  bool . It  True  decay learning rate at discrete intervals.  global_step : Scalar int  Tensor , step counter for each update.  use_locking : If True use locks for update operations.  name :  str . Optional name prefix for the operations created when applying gradients.", 
            "title": "adagrad"
        }, 
        {
            "location": "/optimizers/#ftrl", 
            "text": "ftrl ( learning_rate = 3.0 ,   learning_rate_power =- 0.5 ,   initial_accumulator_value = 0.1 ,   l1_regularization_strength = 0.0 ,   l2_regularization_strength = 0.0 ,   decay_type = ,   decay_rate = 0.0 ,   decay_steps = 100 ,   start_decay_at = 0 ,   stop_decay_at = 2147483647 ,   min_learning_rate = 1e-12 ,   staircase = False ,   global_step = None ,   use_locking = False ,   name = Ftrl )   Optimizer that implements Ftrl Proximal.  The Ftrl-proximal algorithm, abbreviated for Follow-the-regularized-leader,\nis described in the paper below.  It can give a good performance vs. sparsity tradeoff.  Ftrl-proximal uses its own global base learning rate and can behave like\nAdagrad with  learning_rate_power=-0.5 , or like gradient descent with learning_rate_power=0.0 .   Args :  learning_rate :  float . Learning rate.  learning_rate_power :  float . Must be less or equal to zero.  initial_accumulator_value :  float . The starting value for accumulators.\nOnly positive values are allowed.  l1_regularization_strength :  float . Must be less or equal to zero.  l2_regularization_strength :  float . Must be less or equal to zero.  decay_type : A decay function name defined in  tf.train  decay_rate :  float . The learning rate decay to apply.  decay_steps :  int . Apply decay every provided steps.  start_decay_at :  int . Don't decay before this step.  stop_decay_at :  int . Don't decay after this step.  min_learning_rate :  float . Don't decay below this number.  staircase :  bool . It  True  decay learning rate at discrete intervals.  global_step : Scalar int  Tensor , step counter for each update.  use_locking : If True use locks for update operations.  name :  str . Optional name prefix for the operations created when applying gradients..", 
            "title": "ftrl"
        }, 
        {
            "location": "/optimizers/#adadelta", 
            "text": "adadelta ( learning_rate = 0.001 ,   rho = 0.1 ,   epsilon = 1e-08 ,   decay_type = ,   decay_rate = 0.0 ,   decay_steps = 100 ,   start_decay_at = 0 ,   stop_decay_at = 2147483647 ,   min_learning_rate = 1e-12 ,   staircase = False ,   global_step = None ,   use_locking = False ,   name = AdaDelta )   Optimizer that implements AdaDelta.   Args :  learning_rate : A  Tensor  or a floating point value. The learning rate.  rho : A  Tensor  or a floating point value. The decay rate.  epsilon : A  Tensor  or a floating point value.  A constant epsilon used to better\nconditioning the grad update.  decay_type : A decay function name defined in  tf.train  decay_rate :  float . The learning rate decay to apply.  decay_steps :  int . Apply decay every provided steps.  start_decay_at :  int . Don't decay before this step.  stop_decay_at :  int . Don't decay after this step.  min_learning_rate :  float . Don't decay below this number.  staircase :  bool . It  True  decay learning rate at discrete intervals.  global_step : Scalar int  Tensor , step counter for each update.  use_locking : If True use locks for update operations.  name : Optional name prefix for the operations created when applying gradients.", 
            "title": "adadelta"
        }, 
        {
            "location": "/initializations/", 
            "text": "zeros\n\n\nzeros\n(\nshape\n=\nNone\n,\n \ndtype\n=\ndtype\n:\n \nfloat32\n,\n \nname\n=\nzeros\n)\n\n\n\n\n\n\nZeros.\n\n\nInitialize a tensor with all elements set to zero.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nshape\n: List of \nint\n. A shape to initialize a Tensor (optional).\n\n\ndtype\n: The tensor data type.\n\n\nname\n: name of the op.\n\n\n\n\n\n\n\n\nReturns\n:\n    The Initializer, or an initialized \nTensor\n if a shape is specified.\n\n\n\n\n\n\n\n\nuniform\n\n\nuniform\n(\nshape\n=\nNone\n,\n \nminval\n=\n0\n,\n \nmaxval\n=\nNone\n,\n \ndtype\n=\ndtype\n:\n \nfloat32\n,\n \nseed\n=\nNone\n,\n \nname\n=\nUniform\n)\n\n\n\n\n\n\nUniform.\n\n\nInitialization with random values from a uniform distribution.\n\n\nThe generated values follow a uniform distribution in the range\n\n[minval, maxval)\n. The lower bound \nminval\n is included in the range,\nwhile the upper bound \nmaxval\n is excluded.\n\n\nFor floats, the default range is \n[0, 1)\n.  For ints, at least \nmaxval\n\nmust be specified explicitly.\n\n\nIn the integer case, the random integers are slightly biased unless\n\nmaxval - minval\n is an exact power of two.  The bias is small for values of\n\nmaxval - minval\n significantly smaller than the range of the output (either\n\n2**32\n or \n2**64\n).\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nshape\n: List of \nint\n. A shape to initialize a Tensor (optional).\n\n\ndtype\n: The tensor data type. Only float are supported.\n\n\nseed\n: \nint\n. Used to create a random seed for the distribution.\n\n\nname\n: name of the op.\n\n\n\n\n\n\n\n\nReturns\n:\n    The Initializer, or an initialized \nTensor\n if shape is specified.\n\n\n\n\n\n\n\n\nuniform_scaling\n\n\nuniform_scaling\n(\nshape\n=\nNone\n,\n \nfactor\n=\n1.0\n,\n \ndtype\n=\ndtype\n:\n \nfloat32\n,\n \nseed\n=\nNone\n,\n \nname\n=\nUniformScaling\n)\n\n\n\n\n\n\nUniform Scaling.\n\n\nInitialization with random values from uniform distribution without scaling\nvariance.\n\n\nWhen initializing a deep network, it is in principle advantageous to keep\nthe scale of the input variance constant, so it does not explode or diminish\nby reaching the final layer. If the input is \nx\n and the operation \nx * W\n,\nand we want to initialize \nW\n uniformly at random, we need to pick \nW\n from\n\n\n[-sqrt(3) / sqrt(dim), sqrt(3) / sqrt(dim)]\n\n\nto keep the scale intact, where \ndim = W.shape[0]\n (the size of the input).\nA similar calculation for convolutional networks gives an analogous result\nwith \ndim\n equal to the product of the first 3 dimensions.  When\nnonlinearities are present, we need to multiply this by a constant \nfactor\n.\nSee \nSussillo et al., 2014\n\n(\npdf\n) for deeper motivation, experiments\nand the calculation of constants. In section 2.3 there, the constants were\nnumerically computed: for a linear layer it's 1.0, relu: ~1.43, tanh: ~1.15.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nshape\n: List of \nint\n. A shape to initialize a Tensor (optional).\n\n\nfactor\n: \nfloat\n. A multiplicative factor by which the values will be\nscaled.\n\n\ndtype\n: The tensor data type. Only float are supported.\n\n\nseed\n: \nint\n. Used to create a random seed for the distribution.\n\n\nname\n: name of the op.\n\n\n\n\n\n\n\n\nReturns\n:\n    The Initializer, or an initialized \nTensor\n if shape is specified.\n\n\n\n\n\n\n\n\nnormal\n\n\nnormal\n(\nshape\n=\nNone\n,\n \nmean\n=\n0.0\n,\n \nstddev\n=\n0.02\n,\n \ndtype\n=\ndtype\n:\n \nfloat32\n,\n \nseed\n=\nNone\n,\n \nname\n=\nNormal\n)\n\n\n\n\n\n\nNormal.\n\n\nInitialization with random values from a normal distribution.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nshape\n: List of \nint\n. A shape to initialize a Tensor (optional).\n\n\nmean\n: Same as \ndtype\n. The mean of the truncated normal distribution.\n\n\nstddev\n: Same as \ndtype\n. The standard deviation of the truncated\nnormal distribution.\n\n\ndtype\n: The tensor data type.\n\n\nseed\n: \nint\n. Used to create a random seed for the distribution.\n\n\nscope\n: scope to add the op to.\n\n\nname\n: name of the op.\n\n\n\n\n\n\n\n\nReturns\n:\n    The Initializer, or an initialized \nTensor\n if shape is specified.\n\n\n\n\n\n\n\n\ntruncated_normal\n\n\ntruncated_normal\n(\nshape\n=\nNone\n,\n \nmean\n=\n0.0\n,\n \nstddev\n=\n0.02\n,\n \ndtype\n=\ndtype\n:\n \nfloat32\n,\n \nseed\n=\nNone\n,\n \nname\n=\nTruncatedNormal\n)\n\n\n\n\n\n\nTruncated Normal.\n\n\nInitialization with random values from a normal truncated distribution.\n\n\nThe generated values follow a normal distribution with specified mean and\nstandard deviation, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nshape\n: List of \nint\n. A shape to initialize a Tensor (optional).\n\n\nmean\n: Same as \ndtype\n. The mean of the truncated normal distribution.\n\n\nstddev\n: Same as \ndtype\n. The standard deviation of the truncated\nnormal distribution.\n\n\ndtype\n: The tensor data type.\n\n\nseed\n: \nint\n. Used to create a random seed for the distribution.\n\n\nname\n: name of the op.\n\n\n\n\n\n\n\n\nReturns\n:\n    The Initializer, or an initialized \nTensor\n if shape is specified.\n\n\n\n\n\n\n\n\nxavier\n\n\nxavier\n(\nuniform\n=\nTrue\n,\n \nseed\n=\nNone\n,\n \ndtype\n=\ndtype\n:\n \nfloat32\n,\n \nname\n=\nXavier\n)\n\n\n\n\n\n\nXavier.\n\n\nReturns an initializer performing \"Xavier\" initialization for weights.\n\n\nThis initializer is designed to keep the scale of the gradients roughly the\nsame in all layers. In uniform distribution this ends up being the range:\n\nx = sqrt(6. / (in + out)); [-x, x]\n and for normal distribution a standard\ndeviation of \nsqrt(3. / (in + out))\n is used.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nuniform\n: Whether to use uniform or normal distributed random\ninitialization.\n\n\nseed\n: A Python integer. Used to create random seeds. See\n\nset_random_seed\n for behavior.\n\n\ndtype\n: The data type. Only floating point types are supported.\n\n\nname\n: name of the op.\n\n\n\n\n\n\n\n\nReturns\n:\n    An initializer for a weight matrix.\n\n\n\n\n\n\nReferences\n:\n    Understanding the difficulty of training deep feedforward neural\n    networks. International conference on artificial intelligence and\n    statistics. Xavier Glorot and Yoshua Bengio (2010).\n\n\n\n\n\n\nLinks\n:\n\n\n\n\n[http\n://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf]\n(http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n\n\n\n\n\n\n\n\n\n\nvariance_scaling\n\n\nvariance_scaling\n(\nfactor\n=\n2.0\n,\n \nmode\n=\nFAN_IN\n,\n \nuniform\n=\nFalse\n,\n \nseed\n=\nNone\n,\n \ndtype\n=\ndtype\n:\n \nfloat32\n,\n \nname\n=\nXavier\n)\n\n\n\n\n\n\nVariance Scaling.\n\n\nReturns an initializer that generates tensors without scaling variance.\n\n\nWhen initializing a deep network, it is in principle advantageous to keep\nthe scale of the input variance constant, so it does not explode or diminish\nby reaching the final layer. This initializer use the following formula:\n\n\nif mode=\nFAN_IN\n: # Count only number of input connections.\n  n = fan_in\nelif mode=\nFAN_OUT\n: # Count only number of output connections.\n  n = fan_out\nelif mode=\nFAN_AVG\n: # Average number of inputs and output connections.\n  n = (fan_in + fan_out)/2.0\n\n  truncated_normal(shape, 0.0, stddev=sqrt(factor / n))\n\n\n\n\n\nTo get http://arxiv.org/pdf/1502.01852v1.pdf use (Default):\n- factor=2.0 mode='FAN_IN' uniform=False\n\n\nTo get http://arxiv.org/abs/1408.5093 use:\n- factor=1.0 mode='FAN_IN' uniform=True\n\n\nTo get http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf use:\n- factor=1.0 mode='FAN_AVG' uniform=True.\n\n\nTo get xavier_initializer use either:\n- factor=1.0 mode='FAN_AVG' uniform=True.\n- factor=1.0 mode='FAN_AVG' uniform=False.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nfactor\n: Float.  A multiplicative factor.\n\n\nmode\n: String.  'FAN_IN', 'FAN_OUT', 'FAN_AVG'.\n\n\nuniform\n: Whether to use uniform or normal distributed random\ninitialization.\n\n\nseed\n: A Python integer. Used to create random seeds. See\n\nset_random_seed\n for behavior.\n\n\ndtype\n: The data type. Only floating point types are supported.\n\n\nname\n: name of the op.\n\n\n\n\n\n\n\n\nReturns\n:\n    An initializer that generates tensors with unit variance.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if \ndtype\n is not a floating point type.\n\n\nTypeError\n: if \nmode\n is not in ['FAN_IN', 'FAN_OUT', 'FAN_AVG'].", 
            "title": "Initializations"
        }, 
        {
            "location": "/initializations/#zeros", 
            "text": "zeros ( shape = None ,   dtype = dtype :   float32 ,   name = zeros )   Zeros.  Initialize a tensor with all elements set to zero.    Args :   shape : List of  int . A shape to initialize a Tensor (optional).  dtype : The tensor data type.  name : name of the op.     Returns :\n    The Initializer, or an initialized  Tensor  if a shape is specified.", 
            "title": "zeros"
        }, 
        {
            "location": "/initializations/#uniform", 
            "text": "uniform ( shape = None ,   minval = 0 ,   maxval = None ,   dtype = dtype :   float32 ,   seed = None ,   name = Uniform )   Uniform.  Initialization with random values from a uniform distribution.  The generated values follow a uniform distribution in the range [minval, maxval) . The lower bound  minval  is included in the range,\nwhile the upper bound  maxval  is excluded.  For floats, the default range is  [0, 1) .  For ints, at least  maxval \nmust be specified explicitly.  In the integer case, the random integers are slightly biased unless maxval - minval  is an exact power of two.  The bias is small for values of maxval - minval  significantly smaller than the range of the output (either 2**32  or  2**64 ).    Args :   shape : List of  int . A shape to initialize a Tensor (optional).  dtype : The tensor data type. Only float are supported.  seed :  int . Used to create a random seed for the distribution.  name : name of the op.     Returns :\n    The Initializer, or an initialized  Tensor  if shape is specified.", 
            "title": "uniform"
        }, 
        {
            "location": "/initializations/#uniform_scaling", 
            "text": "uniform_scaling ( shape = None ,   factor = 1.0 ,   dtype = dtype :   float32 ,   seed = None ,   name = UniformScaling )   Uniform Scaling.  Initialization with random values from uniform distribution without scaling\nvariance.  When initializing a deep network, it is in principle advantageous to keep\nthe scale of the input variance constant, so it does not explode or diminish\nby reaching the final layer. If the input is  x  and the operation  x * W ,\nand we want to initialize  W  uniformly at random, we need to pick  W  from  [-sqrt(3) / sqrt(dim), sqrt(3) / sqrt(dim)]  to keep the scale intact, where  dim = W.shape[0]  (the size of the input).\nA similar calculation for convolutional networks gives an analogous result\nwith  dim  equal to the product of the first 3 dimensions.  When\nnonlinearities are present, we need to multiply this by a constant  factor .\nSee  Sussillo et al., 2014 \n( pdf ) for deeper motivation, experiments\nand the calculation of constants. In section 2.3 there, the constants were\nnumerically computed: for a linear layer it's 1.0, relu: ~1.43, tanh: ~1.15.    Args :   shape : List of  int . A shape to initialize a Tensor (optional).  factor :  float . A multiplicative factor by which the values will be\nscaled.  dtype : The tensor data type. Only float are supported.  seed :  int . Used to create a random seed for the distribution.  name : name of the op.     Returns :\n    The Initializer, or an initialized  Tensor  if shape is specified.", 
            "title": "uniform_scaling"
        }, 
        {
            "location": "/initializations/#normal", 
            "text": "normal ( shape = None ,   mean = 0.0 ,   stddev = 0.02 ,   dtype = dtype :   float32 ,   seed = None ,   name = Normal )   Normal.  Initialization with random values from a normal distribution.    Args :   shape : List of  int . A shape to initialize a Tensor (optional).  mean : Same as  dtype . The mean of the truncated normal distribution.  stddev : Same as  dtype . The standard deviation of the truncated\nnormal distribution.  dtype : The tensor data type.  seed :  int . Used to create a random seed for the distribution.  scope : scope to add the op to.  name : name of the op.     Returns :\n    The Initializer, or an initialized  Tensor  if shape is specified.", 
            "title": "normal"
        }, 
        {
            "location": "/initializations/#truncated_normal", 
            "text": "truncated_normal ( shape = None ,   mean = 0.0 ,   stddev = 0.02 ,   dtype = dtype :   float32 ,   seed = None ,   name = TruncatedNormal )   Truncated Normal.  Initialization with random values from a normal truncated distribution.  The generated values follow a normal distribution with specified mean and\nstandard deviation, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.    Args :   shape : List of  int . A shape to initialize a Tensor (optional).  mean : Same as  dtype . The mean of the truncated normal distribution.  stddev : Same as  dtype . The standard deviation of the truncated\nnormal distribution.  dtype : The tensor data type.  seed :  int . Used to create a random seed for the distribution.  name : name of the op.     Returns :\n    The Initializer, or an initialized  Tensor  if shape is specified.", 
            "title": "truncated_normal"
        }, 
        {
            "location": "/initializations/#xavier", 
            "text": "xavier ( uniform = True ,   seed = None ,   dtype = dtype :   float32 ,   name = Xavier )   Xavier.  Returns an initializer performing \"Xavier\" initialization for weights.  This initializer is designed to keep the scale of the gradients roughly the\nsame in all layers. In uniform distribution this ends up being the range: x = sqrt(6. / (in + out)); [-x, x]  and for normal distribution a standard\ndeviation of  sqrt(3. / (in + out))  is used.    Args :   uniform : Whether to use uniform or normal distributed random\ninitialization.  seed : A Python integer. Used to create random seeds. See set_random_seed  for behavior.  dtype : The data type. Only floating point types are supported.  name : name of the op.     Returns :\n    An initializer for a weight matrix.    References :\n    Understanding the difficulty of training deep feedforward neural\n    networks. International conference on artificial intelligence and\n    statistics. Xavier Glorot and Yoshua Bengio (2010).    Links :   [http ://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf]\n(http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)", 
            "title": "xavier"
        }, 
        {
            "location": "/initializations/#variance_scaling", 
            "text": "variance_scaling ( factor = 2.0 ,   mode = FAN_IN ,   uniform = False ,   seed = None ,   dtype = dtype :   float32 ,   name = Xavier )   Variance Scaling.  Returns an initializer that generates tensors without scaling variance.  When initializing a deep network, it is in principle advantageous to keep\nthe scale of the input variance constant, so it does not explode or diminish\nby reaching the final layer. This initializer use the following formula:  if mode= FAN_IN : # Count only number of input connections.\n  n = fan_in\nelif mode= FAN_OUT : # Count only number of output connections.\n  n = fan_out\nelif mode= FAN_AVG : # Average number of inputs and output connections.\n  n = (fan_in + fan_out)/2.0\n\n  truncated_normal(shape, 0.0, stddev=sqrt(factor / n))  To get http://arxiv.org/pdf/1502.01852v1.pdf use (Default):\n- factor=2.0 mode='FAN_IN' uniform=False  To get http://arxiv.org/abs/1408.5093 use:\n- factor=1.0 mode='FAN_IN' uniform=True  To get http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf use:\n- factor=1.0 mode='FAN_AVG' uniform=True.  To get xavier_initializer use either:\n- factor=1.0 mode='FAN_AVG' uniform=True.\n- factor=1.0 mode='FAN_AVG' uniform=False.    Args :   factor : Float.  A multiplicative factor.  mode : String.  'FAN_IN', 'FAN_OUT', 'FAN_AVG'.  uniform : Whether to use uniform or normal distributed random\ninitialization.  seed : A Python integer. Used to create random seeds. See set_random_seed  for behavior.  dtype : The data type. Only floating point types are supported.  name : name of the op.     Returns :\n    An initializer that generates tensors with unit variance.    Raises :   ValueError : if  dtype  is not a floating point type.  TypeError : if  mode  is not in ['FAN_IN', 'FAN_OUT', 'FAN_AVG'].", 
            "title": "variance_scaling"
        }, 
        {
            "location": "/regularizations/", 
            "text": "built_regularizer\n\n\nbuilt_regularizer\n(\nfct\n,\n \ncollect\n)\n\n\n\n\n\n\nBuilds the regularizer function.\n\n\n\n\nArgs\n:\n\n\nfct\n: the metric function to build.\n\n\ncollect\n: whether to collect this metric under the metric collection.\n\n\n\n\n\n\n\n\n\n\nl2_regularizer\n\n\nl2_regularizer\n(\nscale\n=\n0.001\n,\n \nname\n=\nl2Regularizer\n,\n \ncollect\n=\nTrue\n)\n\n\n\n\n\n\nReturns a function that can be used to apply L2 regularization to a tensor.\n\n\nComputes half the L2 norm of a tensor without the \nsqrt\n:\n\n\noutput = sum(t ** 2) / 2 * wd\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nx\n: \nTensor\n. The tensor to apply regularization.\n\n\nscale\n: \nfloat\n. A scalar multiplier \nTensor\n. 0.0 disables the regularizer.\n\n\nname\n: \nstr\n name of the app.\n\n\ncollect\n: add to regularization losses\n\n\n\n\n\n\n\n\nReturns\n:\n    The regularization \nTensor\n.\n\n\n\n\n\n\n\n\nl1_regularizer\n\n\nl1_regularizer\n(\nscale\n=\n0.001\n,\n \nname\n=\nl1Regularizer\n,\n \ncollect\n=\nTrue\n)\n\n\n\n\n\n\nReturns a function that can be used to apply L1 regularization to a tensor.\n\n\nComputes the L1 norm of a tensor:\n\n\noutput = sum(|t|) * scale\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nscale\n: \nfloat\n. A scalar multiplier \nTensor\n. 0.0 disables the regularizer.\n\n\nname\n: name of the app.\n\n\ncollect\n: add to regularization losses\n\n\n\n\n\n\n\n\nReturns\n:\n    The regularization \nTensor\n.\n\n\n\n\n\n\n\n\nl2_l1_regularizer\n\n\nl2_l1_regularizer\n(\nscale_l1\n=\n0.001\n,\n \nscale_l2\n=\n0.001\n,\n \nname\n=\nl2l1Regularizer\n,\n \ncollect\n=\nTrue\n)\n\n\n\n\n\n\nReturns a function that can be used to apply L2 L1 regularization to a tensor.\n\n\nComputes the L2 and L1 norm of a tensor:\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nscale_l1\n: \nfloat\n. A scalar multiplier \nTensor\n. 0.0 disables the regularizer.\n\n\nscale_l2\n: \nfloat\n. A scalar multiplier \nTensor\n. 0.0 disables the regularizer.\n\n\nname\n: name of the app.\n\n\ncollect\n: add to regularization losses\n\n\n\n\n\n\n\n\nReturns\n:\n    The regularization \nTensor\n.", 
            "title": "Regularizations"
        }, 
        {
            "location": "/regularizations/#built_regularizer", 
            "text": "built_regularizer ( fct ,   collect )   Builds the regularizer function.   Args :  fct : the metric function to build.  collect : whether to collect this metric under the metric collection.", 
            "title": "built_regularizer"
        }, 
        {
            "location": "/regularizations/#l2_regularizer", 
            "text": "l2_regularizer ( scale = 0.001 ,   name = l2Regularizer ,   collect = True )   Returns a function that can be used to apply L2 regularization to a tensor.  Computes half the L2 norm of a tensor without the  sqrt :  output = sum(t ** 2) / 2 * wd    Args :   x :  Tensor . The tensor to apply regularization.  scale :  float . A scalar multiplier  Tensor . 0.0 disables the regularizer.  name :  str  name of the app.  collect : add to regularization losses     Returns :\n    The regularization  Tensor .", 
            "title": "l2_regularizer"
        }, 
        {
            "location": "/regularizations/#l1_regularizer", 
            "text": "l1_regularizer ( scale = 0.001 ,   name = l1Regularizer ,   collect = True )   Returns a function that can be used to apply L1 regularization to a tensor.  Computes the L1 norm of a tensor:  output = sum(|t|) * scale    Args :   scale :  float . A scalar multiplier  Tensor . 0.0 disables the regularizer.  name : name of the app.  collect : add to regularization losses     Returns :\n    The regularization  Tensor .", 
            "title": "l1_regularizer"
        }, 
        {
            "location": "/regularizations/#l2_l1_regularizer", 
            "text": "l2_l1_regularizer ( scale_l1 = 0.001 ,   scale_l2 = 0.001 ,   name = l2l1Regularizer ,   collect = True )   Returns a function that can be used to apply L2 L1 regularization to a tensor.  Computes the L2 and L1 norm of a tensor:    Args :   scale_l1 :  float . A scalar multiplier  Tensor . 0.0 disables the regularizer.  scale_l2 :  float . A scalar multiplier  Tensor . 0.0 disables the regularizer.  name : name of the app.  collect : add to regularization losses     Returns :\n    The regularization  Tensor .", 
            "title": "l2_l1_regularizer"
        }, 
        {
            "location": "/libs/configs/", 
            "text": "[source]\n\n\nRunConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nRunConfig\n(\nmaster\n=\nNone\n,\n \nnum_cores\n=\n0\n,\n \nlog_device_placement\n=\nFalse\n,\n \ngpu_memory_fraction\n=\n1.0\n,\n \ntf_random_seed\n=\nNone\n,\n \nsave_summary_steps\n=\n100\n,\n \nsave_checkpoints_secs\n=\n600\n,\n \nsave_checkpoints_steps\n=\nNone\n,\n \nkeep_checkpoint_max\n=\n5\n,\n \nkeep_checkpoint_every_n_hours\n=\n10000\n,\n \nevaluation_master\n=\n,\n \nmodel_dir\n=\nNone\n)\n\n\n\n\n\n\n\n\n[source]\n\n\nConfigurable\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nConfigurable\n()\n\n\n\n\n\n\nConfigurable\n is an abstract class for defining an configurable objects.\n\n\nA configurable class reads a configuration (YAML, Json) and create a config instance.\n\n\n\n\n[source]\n\n\nPipelineConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nPipelineConfig\n(\nmodule\n=\nNone\n,\n \nname\n=\nNone\n,\n \nsubgraph_configs_by_features\n=\nNone\n,\n \ndynamic_pad\n=\nTrue\n,\n \nbucket_boundaries\n=\nFalse\n,\n \nbatch_size\n=\n64\n,\n \nnum_epochs\n=\n4\n,\n \nmin_after_dequeue\n=\n5000\n,\n \nnum_threads\n=\n3\n,\n \nshuffle\n=\nFalse\n,\n \nallow_smaller_final_batch\n=\nTrue\n,\n \nparams\n=\nNone\n)\n\n\n\n\n\n\nThe PipelineConfig holds information needed to create a \nPipeline\n.\n\n\n\n\nArgs\n:\n\n\nmodule\n: \nstr\n, the pipeline module to use.\n\n\nname\n: \nstr\n, name to give for the pipeline.\n\n\ndynamic_pad\n: \nbool\n, If True the piple uses dynamic padding.\n\n\nbucket_boundaries\n:\n\n\nbatch_size\n: \nint\n, the batch size.\n\n\nnum_epochs\n: number of epochs to iterate over in this pipeline.\n\n\nmin_after_dequeue\n: \nint\n, number of element to have in the queue.\n\n\nnum_threads\n: \nint\n, number of threads to use in the queue.\n\n\nshuffle\n: If true, shuffle the data.\n\n\nnum_epochs\n: Number of times to iterate through the dataset. If None, iterate forever.\n\n\nparams\n: \ndict\n, extra information to pass to the pipeline.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nInputDataConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nInputDataConfig\n(\ninput_type\n=\nNone\n,\n \npipeline_config\n=\nNone\n,\n \nx\n=\nNone\n,\n \ny\n=\nNone\n)\n\n\n\n\n\n\nThe InputDataConfig holds information needed to create a \nInputData\n.\n\n\n\n\nArgs\n:\n\n\ninput_type\n: \nstr\n, the type of the input data, e.g. numpy arrays.\n\n\npipeline_config\n: The pipeline config to use.\n\n\nx\n: The x values, only used with NUMPY and PANDAS types.\n\n\ny\n: The y values, only used with NUMPY and PANDAS types.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nLossConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nLossConfig\n(\nmodule\n,\n \nparams\n=\nNone\n)\n\n\n\n\n\n\nThe LossConfig holds information needed to create a \nLoss\n.\n\n\n\n\nArgs\n:\n\n\nmodule\n: \nstr\n, module loss to use.\n\n\nparams\n: \ndict\n, extra information to pass to the loss.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nMetricConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nMetricConfig\n(\nmodule\n,\n \nparams\n=\nNone\n)\n\n\n\n\n\n\nThe MetricConfig holds information needed to create a \nMetric\n.\n\n\n\n\nArgs\n:\n\n\nmodule\n: \nstr\n, name to give for the metric.\n\n\nparams\n: \ndict\n, extra information to pass to the metric.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nBridgeConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nBridgeConfig\n(\nmodule\n,\n \nstate_size\n=\nNone\n)\n\n\n\n\n\n\nThe BridgeConfig class holds information neede to create a \nBridge\n for a generator model.\n\n\n\n\n[source]\n\n\nExperimentConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nExperimentConfig\n(\nname\n,\n \noutput_dir\n,\n \nrun_config\n,\n \ntrain_input_data_config\n,\n \neval_input_data_config\n,\n \nestimator_config\n,\n \nmodel_config\n,\n \ntrain_hooks_config\n=\nNone\n,\n \neval_hooks_config\n=\nNone\n,\n \neval_metrics_config\n=\nNone\n,\n \neval_every_n_steps\n=\n1000\n,\n \ntrain_steps\n=\n10000\n,\n \neval_steps\n=\n100\n,\n \neval_delay_secs\n=\n0\n,\n \ncontinuous_eval_throttle_secs\n=\n60\n,\n \ndelay_workers_by_global_step\n=\nFalse\n,\n \nexport_strategies\n=\nNone\n,\n \ntrain_steps_per_iteration\n=\n1000\n)\n\n\n\n\n\n\nThe ExperimentConfig holds information needed to create a \nExperiment\n.\n\n\n\n\nArgs\n:\n\n\nname\n: \nstr\n, name to give for the experiment.\n\n\noutput_dir\n: \nstr\n, where to save training and evaluation data.\n\n\nrun_config\n: Tensorflow run config.\n\n\ntrain_input_data_config\n: Train input data configuration.\n\n\neval_input_data_config\n: Eval input data configuration.\n\n\nestimator_config\n: The estimator configuration.\n\n\nmodel_config\n: The model configuration.\n\n\ntrain_hooks_config\n: The training hooks configuration.\n\n\neval_hooks_config\n: The evaluation hooks configuration.\n\n\neval_metrics_config\n: The evaluation metrics config.\n\n\neval_every_n_steps\n: \nint\n, the frequency of evaluation.\n\n\ntrain_steps\n: \nint\n, the number of steps to train the model.\n\n\neval_steps\n: \nint\n, the number of steps to eval the model.\n\n\neval_delay_secs\n: \nint\n, used to delay the evaluation.\n\n\ncontinuous_eval_throttle_secs\n: Do not re-evaluate unless the last evaluation\n    was started at least this many seconds ago for continuous_eval().\n\n\ndelay_workers_by_global_step\n: if \nTrue\n delays training workers based on global step\n    instead of time.\n\n\nexport_strategies\n: A list of \nExportStrategy\ns, or a single one, or None.\n\n\ntrain_steps_per_iteration\n: (applies only to continuous_train_and_evaluate).\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nModelConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nModelConfig\n(\nloss_config\n,\n \noptimizer_config\n,\n \nmodule\n=\nNone\n,\n \ngraph_config\n=\nNone\n,\n \nencoder_config\n=\nNone\n,\n \ndecoder_config\n=\nNone\n,\n \nbridge_config\n=\nNone\n,\n \nsummaries\n=\nall\n,\n \neval_metrics_config\n=\nNone\n,\n \nclip_gradients\n=\n5.0\n)\n\n\n\n\n\n\nThe ModelConfig holds information needed to create a \nModel\n.\n\n\n\n\nArgs\n:\n\n\nloss_config\n: The loss configuration.\n\n\noptimizer_config\n: The optimizer configuration.\n\n\ngraph_config\n: The graph configuration.\n\n\nmodel_type\n: \nstr\n, The type of the model (\nclassifier\n, 'regressor, or \ngenerator\n).\n\n\nsummaries\n: \nstr\n or \nlist\n, the summary levels.\n\n\neval_metrics_config\n: The evaluation metrics configuration.\n\n\nclip_gradients\n: \nfloat\n, The value to clip the gradients with.\n\n\nparams\n: \ndict\n, extra information to pass to the model.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nEstimatorConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nEstimatorConfig\n(\nmodule\n=\nEstimator\n,\n \noutput_dir\n=\nNone\n,\n \nparams\n=\nNone\n)\n\n\n\n\n\n\nThe EstimatorConfig holds information needed to create a \nEstimator\n.\n\n\n\n\nArgs\n:\n\n\ncls\n: \nstr\n, estimator class to use.\n\n\noutput_dir\n: \nstr\n, where to save training and evaluation data.\n\n\nparams\n: \ndict\n, extra information to pass to the estimator.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nOptimizerConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nOptimizerConfig\n(\nmodule\n,\n \nlearning_rate\n=\n0.0001\n,\n \ndecay_type\n=\n,\n \ndecay_steps\n=\n100\n,\n \ndecay_rate\n=\n0.99\n,\n \nstart_decay_at\n=\n0\n,\n \nstop_decay_at\n=\n2147483647\n,\n \nmin_learning_rate\n=\n1e-12\n,\n \nstaircase\n=\nFalse\n,\n \nsync_replicas\n=\n0\n,\n \nsync_replicas_to_aggregate\n=\n0\n,\n \nparams\n=\nNone\n)\n\n\n\n\n\n\nThe OptimizerConfig holds information needed to create a \nOptimizer\n.\n\n\n\n\nArgs\n:\n\n\nmodule\n: \nstr\n, optimizer optimizer to use.\n\n\nlearning_rate\n: A Tensor or a floating point value. The learning rate to use.\n\n\ndecay_steps\n: How often to apply decay.\n\n\ndecay_rate\n: A Python number. The decay rate.\n\n\nstart_decay_at\n: Don't decay before this step\n\n\nstop_decay_at\n: Don't decay after this step\n\n\nmin_learning_rate\n: Don't decay below this number\n\n\ndecay_type\n: A decay function name defined in \ntf.train\n\n    possible Values: exponential_decay, inverse_time_decay, natural_exp_decay,\n             piecewise_constant, polynomial_decay.\n\n\nstaircase\n: Whether to apply decay in a discrete staircase,\n    as opposed to continuous, fashion.\n\n\nsync_replicas\n:\n\n\nsync_replicas_to_aggregate\n:\n\n\nparams\n: \ndict\n, extra information to pass to the optimizer.\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nSubGraphConfig\n\n\npolyaxon\n.\nlibs\n.\nconfigs\n.\nSubGraphConfig\n(\nmodules\n,\n \nkwargs\n,\n \nfeatures\n=\nNone\n)\n\n\n\n\n\n\nThe configuration used to create subgraphs.\n\n\nHandles also nested subgraphs.\n\n\n\n\nArgs\n:\n\n\nname\n: \nstr\n. The name of this subgraph, used for creating the scope.\n\n\nmodules\n: \nlist\n.  The modules to connect inside this subgraph, e.g. layers\n\n\nkwargs\n: \nlist\n. the list key word args to call each method with.\n\n\n\n\n\n\n\n\n\n\ncreate_run_config\n\n\ncreate_run_config\n(\ntf_random_seed\n=\nNone\n,\n \nsave_checkpoints_secs\n=\nNone\n,\n \nsave_checkpoints_steps\n=\n600\n,\n \nkeep_checkpoint_max\n=\n5\n,\n \nkeep_checkpoint_every_n_hours\n=\n4\n,\n \ngpu_memory_fraction\n=\n1.0\n,\n \ngpu_allow_growth\n=\nFalse\n,\n \nlog_device_placement\n=\nFalse\n)\n\n\n\n\n\n\nCreates a \nRunConfig\n instance.", 
            "title": "Configs"
        }, 
        {
            "location": "/libs/configs/#runconfig", 
            "text": "polyaxon . libs . configs . RunConfig ( master = None ,   num_cores = 0 ,   log_device_placement = False ,   gpu_memory_fraction = 1.0 ,   tf_random_seed = None ,   save_summary_steps = 100 ,   save_checkpoints_secs = 600 ,   save_checkpoints_steps = None ,   keep_checkpoint_max = 5 ,   keep_checkpoint_every_n_hours = 10000 ,   evaluation_master = ,   model_dir = None )    [source]", 
            "title": "RunConfig"
        }, 
        {
            "location": "/libs/configs/#configurable", 
            "text": "polyaxon . libs . configs . Configurable ()   Configurable  is an abstract class for defining an configurable objects.  A configurable class reads a configuration (YAML, Json) and create a config instance.   [source]", 
            "title": "Configurable"
        }, 
        {
            "location": "/libs/configs/#pipelineconfig", 
            "text": "polyaxon . libs . configs . PipelineConfig ( module = None ,   name = None ,   subgraph_configs_by_features = None ,   dynamic_pad = True ,   bucket_boundaries = False ,   batch_size = 64 ,   num_epochs = 4 ,   min_after_dequeue = 5000 ,   num_threads = 3 ,   shuffle = False ,   allow_smaller_final_batch = True ,   params = None )   The PipelineConfig holds information needed to create a  Pipeline .   Args :  module :  str , the pipeline module to use.  name :  str , name to give for the pipeline.  dynamic_pad :  bool , If True the piple uses dynamic padding.  bucket_boundaries :  batch_size :  int , the batch size.  num_epochs : number of epochs to iterate over in this pipeline.  min_after_dequeue :  int , number of element to have in the queue.  num_threads :  int , number of threads to use in the queue.  shuffle : If true, shuffle the data.  num_epochs : Number of times to iterate through the dataset. If None, iterate forever.  params :  dict , extra information to pass to the pipeline.      [source]", 
            "title": "PipelineConfig"
        }, 
        {
            "location": "/libs/configs/#inputdataconfig", 
            "text": "polyaxon . libs . configs . InputDataConfig ( input_type = None ,   pipeline_config = None ,   x = None ,   y = None )   The InputDataConfig holds information needed to create a  InputData .   Args :  input_type :  str , the type of the input data, e.g. numpy arrays.  pipeline_config : The pipeline config to use.  x : The x values, only used with NUMPY and PANDAS types.  y : The y values, only used with NUMPY and PANDAS types.      [source]", 
            "title": "InputDataConfig"
        }, 
        {
            "location": "/libs/configs/#lossconfig", 
            "text": "polyaxon . libs . configs . LossConfig ( module ,   params = None )   The LossConfig holds information needed to create a  Loss .   Args :  module :  str , module loss to use.  params :  dict , extra information to pass to the loss.      [source]", 
            "title": "LossConfig"
        }, 
        {
            "location": "/libs/configs/#metricconfig", 
            "text": "polyaxon . libs . configs . MetricConfig ( module ,   params = None )   The MetricConfig holds information needed to create a  Metric .   Args :  module :  str , name to give for the metric.  params :  dict , extra information to pass to the metric.      [source]", 
            "title": "MetricConfig"
        }, 
        {
            "location": "/libs/configs/#bridgeconfig", 
            "text": "polyaxon . libs . configs . BridgeConfig ( module ,   state_size = None )   The BridgeConfig class holds information neede to create a  Bridge  for a generator model.   [source]", 
            "title": "BridgeConfig"
        }, 
        {
            "location": "/libs/configs/#experimentconfig", 
            "text": "polyaxon . libs . configs . ExperimentConfig ( name ,   output_dir ,   run_config ,   train_input_data_config ,   eval_input_data_config ,   estimator_config ,   model_config ,   train_hooks_config = None ,   eval_hooks_config = None ,   eval_metrics_config = None ,   eval_every_n_steps = 1000 ,   train_steps = 10000 ,   eval_steps = 100 ,   eval_delay_secs = 0 ,   continuous_eval_throttle_secs = 60 ,   delay_workers_by_global_step = False ,   export_strategies = None ,   train_steps_per_iteration = 1000 )   The ExperimentConfig holds information needed to create a  Experiment .   Args :  name :  str , name to give for the experiment.  output_dir :  str , where to save training and evaluation data.  run_config : Tensorflow run config.  train_input_data_config : Train input data configuration.  eval_input_data_config : Eval input data configuration.  estimator_config : The estimator configuration.  model_config : The model configuration.  train_hooks_config : The training hooks configuration.  eval_hooks_config : The evaluation hooks configuration.  eval_metrics_config : The evaluation metrics config.  eval_every_n_steps :  int , the frequency of evaluation.  train_steps :  int , the number of steps to train the model.  eval_steps :  int , the number of steps to eval the model.  eval_delay_secs :  int , used to delay the evaluation.  continuous_eval_throttle_secs : Do not re-evaluate unless the last evaluation\n    was started at least this many seconds ago for continuous_eval().  delay_workers_by_global_step : if  True  delays training workers based on global step\n    instead of time.  export_strategies : A list of  ExportStrategy s, or a single one, or None.  train_steps_per_iteration : (applies only to continuous_train_and_evaluate).      [source]", 
            "title": "ExperimentConfig"
        }, 
        {
            "location": "/libs/configs/#modelconfig", 
            "text": "polyaxon . libs . configs . ModelConfig ( loss_config ,   optimizer_config ,   module = None ,   graph_config = None ,   encoder_config = None ,   decoder_config = None ,   bridge_config = None ,   summaries = all ,   eval_metrics_config = None ,   clip_gradients = 5.0 )   The ModelConfig holds information needed to create a  Model .   Args :  loss_config : The loss configuration.  optimizer_config : The optimizer configuration.  graph_config : The graph configuration.  model_type :  str , The type of the model ( classifier , 'regressor, or  generator ).  summaries :  str  or  list , the summary levels.  eval_metrics_config : The evaluation metrics configuration.  clip_gradients :  float , The value to clip the gradients with.  params :  dict , extra information to pass to the model.      [source]", 
            "title": "ModelConfig"
        }, 
        {
            "location": "/libs/configs/#estimatorconfig", 
            "text": "polyaxon . libs . configs . EstimatorConfig ( module = Estimator ,   output_dir = None ,   params = None )   The EstimatorConfig holds information needed to create a  Estimator .   Args :  cls :  str , estimator class to use.  output_dir :  str , where to save training and evaluation data.  params :  dict , extra information to pass to the estimator.      [source]", 
            "title": "EstimatorConfig"
        }, 
        {
            "location": "/libs/configs/#optimizerconfig", 
            "text": "polyaxon . libs . configs . OptimizerConfig ( module ,   learning_rate = 0.0001 ,   decay_type = ,   decay_steps = 100 ,   decay_rate = 0.99 ,   start_decay_at = 0 ,   stop_decay_at = 2147483647 ,   min_learning_rate = 1e-12 ,   staircase = False ,   sync_replicas = 0 ,   sync_replicas_to_aggregate = 0 ,   params = None )   The OptimizerConfig holds information needed to create a  Optimizer .   Args :  module :  str , optimizer optimizer to use.  learning_rate : A Tensor or a floating point value. The learning rate to use.  decay_steps : How often to apply decay.  decay_rate : A Python number. The decay rate.  start_decay_at : Don't decay before this step  stop_decay_at : Don't decay after this step  min_learning_rate : Don't decay below this number  decay_type : A decay function name defined in  tf.train \n    possible Values: exponential_decay, inverse_time_decay, natural_exp_decay,\n             piecewise_constant, polynomial_decay.  staircase : Whether to apply decay in a discrete staircase,\n    as opposed to continuous, fashion.  sync_replicas :  sync_replicas_to_aggregate :  params :  dict , extra information to pass to the optimizer.      [source]", 
            "title": "OptimizerConfig"
        }, 
        {
            "location": "/libs/configs/#subgraphconfig", 
            "text": "polyaxon . libs . configs . SubGraphConfig ( modules ,   kwargs ,   features = None )   The configuration used to create subgraphs.  Handles also nested subgraphs.   Args :  name :  str . The name of this subgraph, used for creating the scope.  modules :  list .  The modules to connect inside this subgraph, e.g. layers  kwargs :  list . the list key word args to call each method with.", 
            "title": "SubGraphConfig"
        }, 
        {
            "location": "/libs/configs/#create_run_config", 
            "text": "create_run_config ( tf_random_seed = None ,   save_checkpoints_secs = None ,   save_checkpoints_steps = 600 ,   keep_checkpoint_max = 5 ,   keep_checkpoint_every_n_hours = 4 ,   gpu_memory_fraction = 1.0 ,   gpu_allow_growth = False ,   log_device_placement = False )   Creates a  RunConfig  instance.", 
            "title": "create_run_config"
        }, 
        {
            "location": "/libs/getters/", 
            "text": "get_optimizer\n\n\nget_optimizer\n(\nmodule\n)\n\n\n\n\n\n\n\n\nget_activation\n\n\nget_activation\n(\nmodule\n)\n\n\n\n\n\n\n\n\nget_initializer\n\n\nget_initializer\n(\nmodule\n)\n\n\n\n\n\n\n\n\nget_regularizer\n\n\nget_regularizer\n(\nmodule\n)\n\n\n\n\n\n\n\n\nget_metric\n\n\nget_metric\n(\nmodule\n,\n \nincoming\n,\n \noutputs\n)\n\n\n\n\n\n\n\n\nget_eval_metric\n\n\nget_eval_metric\n(\nmodule\n,\n \ny_pred\n,\n \ny_true\n)\n\n\n\n\n\n\n\n\nget_loss\n\n\nget_loss\n(\nmodule\n,\n \ny_pred\n,\n \ny_true\n)\n\n\n\n\n\n\n\n\nget_pipeline\n\n\nget_pipeline\n(\nmodule\n,\n \nmode\n,\n \nshuffle\n,\n \nnum_epochs\n,\n \nsubgraph_configs_by_features\n=\nNone\n)\n\n\n\n\n\n\n\n\nget_graph_fn\n\n\nget_graph_fn\n(\nconfig\n)\n\n\n\n\n\n\nCreates the graph operations.\n\n\n\n\nget_bridge_fn\n\n\nget_bridge_fn\n(\nconfig\n)\n\n\n\n\n\n\nCreates a bridge function. Defaults to \nNoOpBridge\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nconfig\n: \nBridgeConfig\n instance.\n\n\n\n\n\n\n\n\nReturns\n:\n    \nfunction\n.\n\n\n\n\n\n\n\n\nget_model_fn\n\n\nget_model_fn\n(\nmodel_config\n,\n \ngraph_fn\n=\nNone\n,\n \nencoder_fn\n=\nNone\n,\n \ndecoder_fn\n=\nNone\n,\n \nbridge_fn\n=\nNone\n)\n\n\n\n\n\n\n\n\nget_estimator\n\n\nget_estimator\n(\nestimator_config\n,\n \nmodel_config\n,\n \nrun_config\n)\n\n\n\n\n\n\n\n\nget_hooks\n\n\nget_hooks\n(\nhooks_config\n)", 
            "title": "Getters"
        }, 
        {
            "location": "/libs/getters/#get_optimizer", 
            "text": "get_optimizer ( module )", 
            "title": "get_optimizer"
        }, 
        {
            "location": "/libs/getters/#get_activation", 
            "text": "get_activation ( module )", 
            "title": "get_activation"
        }, 
        {
            "location": "/libs/getters/#get_initializer", 
            "text": "get_initializer ( module )", 
            "title": "get_initializer"
        }, 
        {
            "location": "/libs/getters/#get_regularizer", 
            "text": "get_regularizer ( module )", 
            "title": "get_regularizer"
        }, 
        {
            "location": "/libs/getters/#get_metric", 
            "text": "get_metric ( module ,   incoming ,   outputs )", 
            "title": "get_metric"
        }, 
        {
            "location": "/libs/getters/#get_eval_metric", 
            "text": "get_eval_metric ( module ,   y_pred ,   y_true )", 
            "title": "get_eval_metric"
        }, 
        {
            "location": "/libs/getters/#get_loss", 
            "text": "get_loss ( module ,   y_pred ,   y_true )", 
            "title": "get_loss"
        }, 
        {
            "location": "/libs/getters/#get_pipeline", 
            "text": "get_pipeline ( module ,   mode ,   shuffle ,   num_epochs ,   subgraph_configs_by_features = None )", 
            "title": "get_pipeline"
        }, 
        {
            "location": "/libs/getters/#get_graph_fn", 
            "text": "get_graph_fn ( config )   Creates the graph operations.", 
            "title": "get_graph_fn"
        }, 
        {
            "location": "/libs/getters/#get_bridge_fn", 
            "text": "get_bridge_fn ( config )   Creates a bridge function. Defaults to  NoOpBridge    Args :   config :  BridgeConfig  instance.     Returns :\n     function .", 
            "title": "get_bridge_fn"
        }, 
        {
            "location": "/libs/getters/#get_model_fn", 
            "text": "get_model_fn ( model_config ,   graph_fn = None ,   encoder_fn = None ,   decoder_fn = None ,   bridge_fn = None )", 
            "title": "get_model_fn"
        }, 
        {
            "location": "/libs/getters/#get_estimator", 
            "text": "get_estimator ( estimator_config ,   model_config ,   run_config )", 
            "title": "get_estimator"
        }, 
        {
            "location": "/libs/getters/#get_hooks", 
            "text": "get_hooks ( hooks_config )", 
            "title": "get_hooks"
        }, 
        {
            "location": "/libs/utils/", 
            "text": "track\n\n\ntrack\n(\ntensor\n,\n \ncollection\n,\n \nscope\n=\nNone\n)\n\n\n\n\n\n\nTrack tensor by adding it to the collection.\n\n\n\n\nget_tracked\n\n\nget_tracked\n(\ncollection\n,\n \nscope\n=\nNone\n)\n\n\n\n\n\n\nReturns a list of values in the collection with the given \ncollection\n.\n\n\n\n\nget_shape\n\n\nget_shape\n(\nx\n)\n\n\n\n\n\n\nGet the incoming data shape.\n\n\n\n\nArgs\n:\n\n\nx\n: incoming data.\n\n\n\n\n\n\nReturns\n:\n    the incoming data shape.\n\n\n\n\n\n\nvalidate_dtype\n\n\nvalidate_dtype\n(\nx\n)\n\n\n\n\n\n\n\n\nget_variable_scope\n\n\nget_variable_scope\n(\nname\n=\nNone\n,\n \nscope\n=\nNone\n,\n \nvalues\n=\nNone\n,\n \nreuse\n=\nNone\n)\n\n\n\n\n\n\n\n\nget_name_scope\n\n\nget_name_scope\n(\nname\n=\nNone\n,\n \nscope\n=\nNone\n,\n \nvalues\n=\nNone\n)\n\n\n\n\n\n\n\n\nclip\n\n\nclip\n(\nx\n,\n \nmin_value\n,\n \nmax_value\n)\n\n\n\n\n\n\nElement-wise value clipping.\n\n\n\n\nint_or_tuple\n\n\nint_or_tuple\n(\nvalue\n)\n\n\n\n\n\n\nConverts \nvalue\n (int or tuple) to height, width.\n\n\nThis functions normalizes the input value by always returning a tuple.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nvalue\n: A list of 2 ints, 4 ints, a single int or a tf.TensorShape.\n\n\n\n\n\n\n\n\nReturns\n:\n    A list with 4 values.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \nvalue\n it not well formed.\n\n\nTypeError\n: if the \nvalue\n type is not supported\n\n\n\n\n\n\n\n\n\n\nint_or_tuple_3d\n\n\nint_or_tuple_3d\n(\nvalue\n)\n\n\n\n\n\n\nConverts \nvalue\n (int or tuple) to height, width for 3d ops.\n\n\nThis functions normalizes the input value by always returning a tuple.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nvalue\n: A list of 3 ints, 5 ints, a single int or a tf.TensorShape.\n\n\n\n\n\n\n\n\nReturns\n:\n    A list with 5 values.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: If \nvalue\n it not well formed.\n\n\nTypeError\n: if the \nvalue\n type is not supported\n\n\n\n\n\n\n\n\n\n\nvalidate_padding\n\n\nvalidate_padding\n(\nvalue\n)\n\n\n\n\n\n\nValidates and format padding value\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nvalue\n: \nstr\n padding value to validate.\n\n\n\n\n\n\n\n\nReturns\n:\n    formatted value.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if is not valid.\n\n\n\n\n\n\n\n\n\n\nvalidate_filter_size\n\n\nvalidate_filter_size\n(\nfilter_size\n,\n \nin_depth\n,\n \nnum_filter\n)\n\n\n\n\n\n\nValidates filter size for CNN operations\n\n\n\n\nvalidate_filter_size_3d\n\n\nvalidate_filter_size_3d\n(\nfilter_size\n,\n \nin_depth\n,\n \nnum_filter\n)\n\n\n\n\n\n\nValidates filter size for 3d CNN operations\n\n\n\n\ncheck_restore_tensor\n\n\ncheck_restore_tensor\n(\ntensor_to_check\n,\n \nexclvars\n)\n\n\n\n\n\n\n\n\ntranspose_batch_time\n\n\ntranspose_batch_time\n(\nx\n)\n\n\n\n\n\n\nTranspose the batch and time dimensions of a Tensor.\n\n\nRetains as much of the static shape information as possible.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nx\n: A tensor of rank 2 or higher.\n\n\n\n\n\n\n\n\nReturns\n:\n    x transposed along the first two dimensions.\n\n\n\n\n\n\nRaises\n:\n\n\n\n\nValueError\n: if \nx\n is rank 1 or lower.\n\n\n\n\n\n\n\n\n\n\ngenerate_model_dir\n\n\ngenerate_model_dir\n()\n\n\n\n\n\n\n\n\nget_arguments\n\n\nget_arguments\n(\nfunc\n)\n\n\n\n\n\n\nReturns list of arguments this function has.\n\n\n\n\nextract_batch_length\n\n\nextract_batch_length\n(\nvalues\n)\n\n\n\n\n\n\nExtracts batch length of predictions.\n\n\n\n\nnew_attr_context\n\n\nnew_attr_context\n()\n\n\n\n\n\n\nCreates a new context in which an object's attribute can be changed.\n\n\nThis creates a context in which an object's attribute can be changed.\nOnce the context is exited, the attribute reverts to its original value.\n\n\n\n\n\n\nArgs\n:\n\n\n\n\nobj\n: An object whose attribute to restore at the end of the context.\n\n\nattr\n: An attribute to remember and restore at the end of the context.\n\n\n\n\n\n\n\n\nYields\n:\n    Context.\n\n\n\n\n\n\nExample\n:\n\n\n\n\n\n\n \nmy_obj\n.\nx\n \n=\n \n1\n\n\n \nwith\n \n_new_attr_context\n(\nmy_obj\n,\n \nx\n):\n\n\n \nmy_obj\n.\nx\n \n=\n \n2\n\n\n \nprint\n(\nmy_obj\n.\nx\n)\n\n\n \nprint\n(\nmy_obj\n.\nx\n)", 
            "title": "Utils"
        }, 
        {
            "location": "/libs/utils/#track", 
            "text": "track ( tensor ,   collection ,   scope = None )   Track tensor by adding it to the collection.", 
            "title": "track"
        }, 
        {
            "location": "/libs/utils/#get_tracked", 
            "text": "get_tracked ( collection ,   scope = None )   Returns a list of values in the collection with the given  collection .", 
            "title": "get_tracked"
        }, 
        {
            "location": "/libs/utils/#get_shape", 
            "text": "get_shape ( x )   Get the incoming data shape.   Args :  x : incoming data.    Returns :\n    the incoming data shape.", 
            "title": "get_shape"
        }, 
        {
            "location": "/libs/utils/#validate_dtype", 
            "text": "validate_dtype ( x )", 
            "title": "validate_dtype"
        }, 
        {
            "location": "/libs/utils/#get_variable_scope", 
            "text": "get_variable_scope ( name = None ,   scope = None ,   values = None ,   reuse = None )", 
            "title": "get_variable_scope"
        }, 
        {
            "location": "/libs/utils/#get_name_scope", 
            "text": "get_name_scope ( name = None ,   scope = None ,   values = None )", 
            "title": "get_name_scope"
        }, 
        {
            "location": "/libs/utils/#clip", 
            "text": "clip ( x ,   min_value ,   max_value )   Element-wise value clipping.", 
            "title": "clip"
        }, 
        {
            "location": "/libs/utils/#int_or_tuple", 
            "text": "int_or_tuple ( value )   Converts  value  (int or tuple) to height, width.  This functions normalizes the input value by always returning a tuple.    Args :   value : A list of 2 ints, 4 ints, a single int or a tf.TensorShape.     Returns :\n    A list with 4 values.    Raises :   ValueError : If  value  it not well formed.  TypeError : if the  value  type is not supported", 
            "title": "int_or_tuple"
        }, 
        {
            "location": "/libs/utils/#int_or_tuple_3d", 
            "text": "int_or_tuple_3d ( value )   Converts  value  (int or tuple) to height, width for 3d ops.  This functions normalizes the input value by always returning a tuple.    Args :   value : A list of 3 ints, 5 ints, a single int or a tf.TensorShape.     Returns :\n    A list with 5 values.    Raises :   ValueError : If  value  it not well formed.  TypeError : if the  value  type is not supported", 
            "title": "int_or_tuple_3d"
        }, 
        {
            "location": "/libs/utils/#validate_padding", 
            "text": "validate_padding ( value )   Validates and format padding value    Args :   value :  str  padding value to validate.     Returns :\n    formatted value.    Raises :   ValueError : if is not valid.", 
            "title": "validate_padding"
        }, 
        {
            "location": "/libs/utils/#validate_filter_size", 
            "text": "validate_filter_size ( filter_size ,   in_depth ,   num_filter )   Validates filter size for CNN operations", 
            "title": "validate_filter_size"
        }, 
        {
            "location": "/libs/utils/#validate_filter_size_3d", 
            "text": "validate_filter_size_3d ( filter_size ,   in_depth ,   num_filter )   Validates filter size for 3d CNN operations", 
            "title": "validate_filter_size_3d"
        }, 
        {
            "location": "/libs/utils/#check_restore_tensor", 
            "text": "check_restore_tensor ( tensor_to_check ,   exclvars )", 
            "title": "check_restore_tensor"
        }, 
        {
            "location": "/libs/utils/#transpose_batch_time", 
            "text": "transpose_batch_time ( x )   Transpose the batch and time dimensions of a Tensor.  Retains as much of the static shape information as possible.    Args :   x : A tensor of rank 2 or higher.     Returns :\n    x transposed along the first two dimensions.    Raises :   ValueError : if  x  is rank 1 or lower.", 
            "title": "transpose_batch_time"
        }, 
        {
            "location": "/libs/utils/#generate_model_dir", 
            "text": "generate_model_dir ()", 
            "title": "generate_model_dir"
        }, 
        {
            "location": "/libs/utils/#get_arguments", 
            "text": "get_arguments ( func )   Returns list of arguments this function has.", 
            "title": "get_arguments"
        }, 
        {
            "location": "/libs/utils/#extract_batch_length", 
            "text": "extract_batch_length ( values )   Extracts batch length of predictions.", 
            "title": "extract_batch_length"
        }, 
        {
            "location": "/libs/utils/#new_attr_context", 
            "text": "new_attr_context ()   Creates a new context in which an object's attribute can be changed.  This creates a context in which an object's attribute can be changed.\nOnce the context is exited, the attribute reverts to its original value.    Args :   obj : An object whose attribute to restore at the end of the context.  attr : An attribute to remember and restore at the end of the context.     Yields :\n    Context.    Example :      my_obj . x   =   1    with   _new_attr_context ( my_obj ,   x ):    my_obj . x   =   2    print ( my_obj . x )    print ( my_obj . x )", 
            "title": "new_attr_context"
        }, 
        {
            "location": "/datasets/converters/", 
            "text": "[source]\n\n\nImageReader\n\n\npolyaxon\n.\ndatasets\n.\nconverters\n.\nImageReader\n(\nchannels\n=\n3\n)\n\n\n\n\n\n\nBase ImageReader class that provides an operation to read/encode/decode an image.\n\n\n\n\n[source]\n\n\nPNGImageReader\n\n\npolyaxon\n.\ndatasets\n.\nconverters\n.\nPNGImageReader\n(\nchannels\n=\n3\n)\n\n\n\n\n\n\nA png image class reader\n\n\n\n\n[source]\n\n\nPNGNumpyImageReader\n\n\npolyaxon\n.\ndatasets\n.\nconverters\n.\nPNGNumpyImageReader\n(\nshape\n=\nNone\n)\n\n\n\n\n\n\nA numpy png image class reader\n\n\n\n\n[source]\n\n\nJPGNumpyImageReader\n\n\npolyaxon\n.\ndatasets\n.\nconverters\n.\nJPGNumpyImageReader\n(\nshape\n=\nNone\n)\n\n\n\n\n\n\nA jpeg numpy image class reader\n\n\n\n\n[source]\n\n\nJPEGImageReader\n\n\npolyaxon\n.\ndatasets\n.\nconverters\n.\nJPEGImageReader\n(\nchannels\n=\n3\n)\n\n\n\n\n\n\nA jpeg image class reader\n\n\n\n\n[source]\n\n\nImagesToTFExampleConverter\n\n\npolyaxon\n.\ndatasets\n.\nconverters\n.\nImagesToTFExampleConverter\n(\nclasses\n,\n \ncolorspace\n,\n \nimage_format\n,\n \nchannels\n,\n \nimage_reader\n=\nNone\n,\n \nheight\n=\nNone\n,\n \nwidth\n=\nNone\n,\n \nstore_filenames\n=\nFalse\n)\n\n\n\n\n\n\nConverts images to a TFRecords of TF-Example protos.\n\n\nEach record is TF-Example protocol buffer which contain a single image and label.\n\n\n\n\nArgs\n:\n\n\nclasses\n: \ndict\n or \nlist\n. The data classes.\n    e.g. ['zero', 'one', 'two', ...] or {0: 'cats', 1: 'dogs'}\n\n\ncolorspace\n: \nstr\n. The color space of the images, 'rgb', 'grayscale' ...\n\n\nimage_format\n: \nstr\n. The format of the images, 'png', 'jpeg'.\n\n\nchannels\n: \nint\n. The number of channels of the images, e.g. 1, 3.\n\n\nimage_reader\n: \nImageReader\n instance. If \nNone\n an image reader is created automatically\n    based on the \nimage_format\n.\n\n\nheight\n: \nint\n. If provided the the height per image will not be stored in the TFRecord,\n    only in the meta_data file.\n\n\nwidth\n: \nint\n. If provided the the width per image will not be stored in the TFRecord,\n    only in the meta_data file.\n\n\nstore_filenames\n: \nbool\n. If \nTrue\n the filename of the image will be stored as a feature.", 
            "title": "Converters"
        }, 
        {
            "location": "/datasets/converters/#imagereader", 
            "text": "polyaxon . datasets . converters . ImageReader ( channels = 3 )   Base ImageReader class that provides an operation to read/encode/decode an image.   [source]", 
            "title": "ImageReader"
        }, 
        {
            "location": "/datasets/converters/#pngimagereader", 
            "text": "polyaxon . datasets . converters . PNGImageReader ( channels = 3 )   A png image class reader   [source]", 
            "title": "PNGImageReader"
        }, 
        {
            "location": "/datasets/converters/#pngnumpyimagereader", 
            "text": "polyaxon . datasets . converters . PNGNumpyImageReader ( shape = None )   A numpy png image class reader   [source]", 
            "title": "PNGNumpyImageReader"
        }, 
        {
            "location": "/datasets/converters/#jpgnumpyimagereader", 
            "text": "polyaxon . datasets . converters . JPGNumpyImageReader ( shape = None )   A jpeg numpy image class reader   [source]", 
            "title": "JPGNumpyImageReader"
        }, 
        {
            "location": "/datasets/converters/#jpegimagereader", 
            "text": "polyaxon . datasets . converters . JPEGImageReader ( channels = 3 )   A jpeg image class reader   [source]", 
            "title": "JPEGImageReader"
        }, 
        {
            "location": "/datasets/converters/#imagestotfexampleconverter", 
            "text": "polyaxon . datasets . converters . ImagesToTFExampleConverter ( classes ,   colorspace ,   image_format ,   channels ,   image_reader = None ,   height = None ,   width = None ,   store_filenames = False )   Converts images to a TFRecords of TF-Example protos.  Each record is TF-Example protocol buffer which contain a single image and label.   Args :  classes :  dict  or  list . The data classes.\n    e.g. ['zero', 'one', 'two', ...] or {0: 'cats', 1: 'dogs'}  colorspace :  str . The color space of the images, 'rgb', 'grayscale' ...  image_format :  str . The format of the images, 'png', 'jpeg'.  channels :  int . The number of channels of the images, e.g. 1, 3.  image_reader :  ImageReader  instance. If  None  an image reader is created automatically\n    based on the  image_format .  height :  int . If provided the the height per image will not be stored in the TFRecord,\n    only in the meta_data file.  width :  int . If provided the the width per image will not be stored in the TFRecord,\n    only in the meta_data file.  store_filenames :  bool . If  True  the filename of the image will be stored as a feature.", 
            "title": "ImagesToTFExampleConverter"
        }, 
        {
            "location": "/datasets/cifar10/", 
            "text": "prepare_dataset\n\n\nprepare_dataset\n(\nconverter\n,\n \ndataset_dir\n,\n \ndata_name\n,\n \nfilenames\n)\n\n\n\n\n\n\n\n\nprepare\n\n\nprepare\n(\ndataset_dir\n)\n\n\n\n\n\n\nRuns download and conversion operation.\n\n\n\n\nArgs\n:\n\n\ndataset_dir\n: The dataset directory where the dataset is stored.\n\n\n\n\n\n\n\n\n\n\ncreate_input_fn\n\n\ncreate_input_fn\n(\ndataset_dir\n)", 
            "title": "Cifar10"
        }, 
        {
            "location": "/datasets/cifar10/#prepare_dataset", 
            "text": "prepare_dataset ( converter ,   dataset_dir ,   data_name ,   filenames )", 
            "title": "prepare_dataset"
        }, 
        {
            "location": "/datasets/cifar10/#prepare", 
            "text": "prepare ( dataset_dir )   Runs download and conversion operation.   Args :  dataset_dir : The dataset directory where the dataset is stored.", 
            "title": "prepare"
        }, 
        {
            "location": "/datasets/cifar10/#create_input_fn", 
            "text": "create_input_fn ( dataset_dir )", 
            "title": "create_input_fn"
        }, 
        {
            "location": "/datasets/flowers17/", 
            "text": "filenames_by_classes\n\n\nfilenames_by_classes\n(\ndataset_dir\n,\n \nnum_images\n,\n \nfolds\n)\n\n\n\n\n\n\n\n\nconvert_images\n\n\nconvert_images\n(\nsession\n,\n \nwriter\n,\n \nconverter\n,\n \nfilesnames_by_classes\n)\n\n\n\n\n\n\n\n\nprepare_dataset\n\n\nprepare_dataset\n(\nconverter\n,\n \ndataset_dir\n,\n \nnum_images\n,\n \nfolds\n)\n\n\n\n\n\n\n\n\nprepare\n\n\nprepare\n(\ndataset_dir\n)\n\n\n\n\n\n\nRuns download and conversion operation.\n\n\n\n\nArgs\n:\n\n\ndataset_dir\n: The dataset directory where the dataset is stored.\n\n\n\n\n\n\n\n\n\n\ncreate_input_fn\n\n\ncreate_input_fn\n(\ndataset_dir\n)", 
            "title": "Flowers17"
        }, 
        {
            "location": "/datasets/flowers17/#filenames_by_classes", 
            "text": "filenames_by_classes ( dataset_dir ,   num_images ,   folds )", 
            "title": "filenames_by_classes"
        }, 
        {
            "location": "/datasets/flowers17/#convert_images", 
            "text": "convert_images ( session ,   writer ,   converter ,   filesnames_by_classes )", 
            "title": "convert_images"
        }, 
        {
            "location": "/datasets/flowers17/#prepare_dataset", 
            "text": "prepare_dataset ( converter ,   dataset_dir ,   num_images ,   folds )", 
            "title": "prepare_dataset"
        }, 
        {
            "location": "/datasets/flowers17/#prepare", 
            "text": "prepare ( dataset_dir )   Runs download and conversion operation.   Args :  dataset_dir : The dataset directory where the dataset is stored.", 
            "title": "prepare"
        }, 
        {
            "location": "/datasets/flowers17/#create_input_fn", 
            "text": "create_input_fn ( dataset_dir )", 
            "title": "create_input_fn"
        }, 
        {
            "location": "/datasets/mnist/", 
            "text": "prepare_dataset\n\n\nprepare_dataset\n(\nconverter\n,\n \ndataset_dir\n,\n \ndata_name\n,\n \nnum_images\n,\n \nnum_eval\n=\n0\n)\n\n\n\n\n\n\n\n\nprepare\n\n\nprepare\n(\ndataset_dir\n)\n\n\n\n\n\n\nRuns download and conversion operation.\n\n\n\n\nArgs\n:\n\n\ndataset_dir\n: The dataset directory where the dataset is stored.\n\n\n\n\n\n\n\n\n\n\ncreate_input_fn\n\n\ncreate_input_fn\n(\ndataset_dir\n)", 
            "title": "Mnist"
        }, 
        {
            "location": "/datasets/mnist/#prepare_dataset", 
            "text": "prepare_dataset ( converter ,   dataset_dir ,   data_name ,   num_images ,   num_eval = 0 )", 
            "title": "prepare_dataset"
        }, 
        {
            "location": "/datasets/mnist/#prepare", 
            "text": "prepare ( dataset_dir )   Runs download and conversion operation.   Args :  dataset_dir : The dataset directory where the dataset is stored.", 
            "title": "prepare"
        }, 
        {
            "location": "/datasets/mnist/#create_input_fn", 
            "text": "create_input_fn ( dataset_dir )", 
            "title": "create_input_fn"
        }, 
        {
            "location": "/CONTRIBUTING/", 
            "text": "How to contribute\n\n\nPolyaxon project would love to welcome your contributions. There are several ways to help out:\n\n\n\n\nCreate an \nissue\n on GitHub, if you have found a bug\n\n\nWrite test cases for open bug issues\n\n\nWrite patches for open bug/feature issues, preferably with test cases included\n\n\nContribute to the documentation\n\n\nBlog about different ways you are using Polyaxon\n\n\n\n\nThere are a few guidelines that we need contributors to follow so that we have a chance of keeping on top of things.\n\n\nReporting issues\n\n\nPolyaxon has probably many issues and bugs, a great way to contribute to the project is to send a detailed report when you encounter an issue. We always appreciate a well-written, thorough bug report, and will thank you for it!\n\n\nSometimes  Polyaxon is missing a feature you need, and we encourage our users to create and contribute such features.\n\n\nCheck the current \nissues\n if doesn't already include that problem or suggestion before submitting an issue.\nIf you find a match, add a quick \"+1\", Doing this helps prioritize the most common problems and requests.\n\n\nWhen reporting issues, please include your host OS (Ubuntu 14.04, CentOS 7, etc), and the version of the libraries you are using.\n\n\nPlease also include the steps required to reproduce the problem if possible and applicable. This information will help us review and fix your issue faster.\n\n\nContributing\n\n\nBefore you contribute to Polyaxon, there are a few things that you'll need to do\n\n\n\n\nMake sure you have a \nGitHub account\n.\n\n\nSubmit an \nissue\n, assuming one does not already exist.\n\n\nClearly describe the issue including steps to reproduce when it is a bug.\n\n\nMake sure you fill in the earliest version that you know has the issue.\n\n\nFork the repository on GitHub.\n\n\n\n\nMaking Changes\n\n\n\n\nCreate a topic branch from where you want to base your work.\n\n\nThis is usually the master branch.\n\n\nOnly target an existing branch if you are certain your fix must be on that branch.\n\n\nTo quickly create a topic branch based on master; \ngit checkout -b my_contribution origin/master\n.\n    It is best to avoid working directly on the \nmaster\n branch. Doing so will help avoid conflicts if you pull in updates from origin.\n\n\nMake commits of logical units. Implementing a new function and calling it in\n  another file constitute a single logical unit of work.\n\n\nA majority of submissions should have a single commit, so if in doubt, squash your commits down to one commit.\n\n\nUse descriptive commit messages and reference the #issue number.\n\n\nCore test cases should continue to pass. (Test are in progress)\n\n\nPull requests must be cleanly rebased on top of master without multiple branches mixed into the PR.\n\n\n\n\nWhich branch to base the work\n\n\nAll changes should be be based on the latest master commit.\n\n\nQuestions\n\n\nIf you need help with how to use this library, please check the list of examples, if it is still unclear you can also open an issue.", 
            "title": "Contributing"
        }, 
        {
            "location": "/CONTRIBUTING/#how-to-contribute", 
            "text": "Polyaxon project would love to welcome your contributions. There are several ways to help out:   Create an  issue  on GitHub, if you have found a bug  Write test cases for open bug issues  Write patches for open bug/feature issues, preferably with test cases included  Contribute to the documentation  Blog about different ways you are using Polyaxon   There are a few guidelines that we need contributors to follow so that we have a chance of keeping on top of things.", 
            "title": "How to contribute"
        }, 
        {
            "location": "/CONTRIBUTING/#reporting-issues", 
            "text": "Polyaxon has probably many issues and bugs, a great way to contribute to the project is to send a detailed report when you encounter an issue. We always appreciate a well-written, thorough bug report, and will thank you for it!  Sometimes  Polyaxon is missing a feature you need, and we encourage our users to create and contribute such features.  Check the current  issues  if doesn't already include that problem or suggestion before submitting an issue.\nIf you find a match, add a quick \"+1\", Doing this helps prioritize the most common problems and requests.  When reporting issues, please include your host OS (Ubuntu 14.04, CentOS 7, etc), and the version of the libraries you are using.  Please also include the steps required to reproduce the problem if possible and applicable. This information will help us review and fix your issue faster.", 
            "title": "Reporting issues"
        }, 
        {
            "location": "/CONTRIBUTING/#contributing", 
            "text": "Before you contribute to Polyaxon, there are a few things that you'll need to do   Make sure you have a  GitHub account .  Submit an  issue , assuming one does not already exist.  Clearly describe the issue including steps to reproduce when it is a bug.  Make sure you fill in the earliest version that you know has the issue.  Fork the repository on GitHub.", 
            "title": "Contributing"
        }, 
        {
            "location": "/CONTRIBUTING/#making-changes", 
            "text": "Create a topic branch from where you want to base your work.  This is usually the master branch.  Only target an existing branch if you are certain your fix must be on that branch.  To quickly create a topic branch based on master;  git checkout -b my_contribution origin/master .\n    It is best to avoid working directly on the  master  branch. Doing so will help avoid conflicts if you pull in updates from origin.  Make commits of logical units. Implementing a new function and calling it in\n  another file constitute a single logical unit of work.  A majority of submissions should have a single commit, so if in doubt, squash your commits down to one commit.  Use descriptive commit messages and reference the #issue number.  Core test cases should continue to pass. (Test are in progress)  Pull requests must be cleanly rebased on top of master without multiple branches mixed into the PR.", 
            "title": "Making Changes"
        }, 
        {
            "location": "/CONTRIBUTING/#which-branch-to-base-the-work", 
            "text": "All changes should be be based on the latest master commit.", 
            "title": "Which branch to base the work"
        }, 
        {
            "location": "/CONTRIBUTING/#questions", 
            "text": "If you need help with how to use this library, please check the list of examples, if it is still unclear you can also open an issue.", 
            "title": "Questions"
        }
    ]
}